{"cells":[{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[],"source":["from transformers import AutoTokenizer, TrainingArguments, Trainer, AutoModelForTokenClassification\n","from pathlib import Path\n","import numpy as np\n","import torch\n","from tokenizers import AddedToken\n","from tqdm.notebook import tqdm\n","from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n","import pandas as pd\n","from datasets import Dataset\n","\n","kaggle=False\n","\n","path=\"/kaggle/input/pii-detection-removal-from-educational-data\" if kaggle else \"data\"\n","train_path = path + \"/train.json\"\n","test_path = path + \"/test.json\"\n","\n","mixtral_path=\"data/mpware_mixtral8x7b_v1.1.json\" if not kaggle else \"/kaggle/input/mixtral-8x7b-v11/mixtral8x7b_v1.1.json\"\n","\n","model_path = \"/kaggle/input/huggingfacedebertav3variants/deberta-v3-base\" if kaggle else \"microsoft/deberta-v3-base\"\n","\n","if not kaggle: import neptune\n","if not kaggle: from seqeval.metrics import recall_score, precision_score, f1_score, accuracy_score"]},{"cell_type":"markdown","metadata":{},"source":["https://www.kaggle.com/datasets/mpware/pii-mixtral8x7b-generated-essays"]},{"cell_type":"code","execution_count":40,"metadata":{"execution":{"iopub.execute_input":"2024-01-25T21:37:35.578114Z","iopub.status.busy":"2024-01-25T21:37:35.577723Z","iopub.status.idle":"2024-01-25T21:37:35.587366Z","shell.execute_reply":"2024-01-25T21:37:35.5858Z","shell.execute_reply.started":"2024-01-25T21:37:35.578083Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["5.000000000000001e-07\n"]}],"source":["cross_entropy_weight_multi = 400\n","\n","CROSS_ENTROPY_WEIGHTS = [cross_entropy_weight_multi]*12\n","CROSS_ENTROPY_WEIGHTS.append(1)\n","\n","\n","# best PII-265\n","\n","parameter= {\n","    \"model\": model_path,\n","    \"max_length\": 512,\n","    \"stride\": 128,\n","    \"inference_max_length\": 2500,\n","    \"batch_size\": 8,\n","    \"inference_batch_size\": 1,\n","    \"lr\": 5e-05,\n","    \"lr_scale_unfreeze\": 0.01,\n","    \"filter_no_pii_percent_allow\": 0.2,\n","    \"notebook\": \"20_deberta base_1024len.ipynb\",\n","    \"CROSS_ENTROPY_WEIGHT_MULTI\": cross_entropy_weight_multi,\n","    \"epochs_before_unfreeze\": 1,\n","    \"epochs_after_unfreeze\": 2,\n","    \"train_test_split\": 0.2,\n","    \"num_proc\": 16 if not kaggle else 2, \n","    \"freeze_embeddings\": False,\n","    \"freeze_layers\": 6,\n","    \"warumup_steps\": 500,\n","    \"weight_decay\": 0.01,\n","    \"logging_dir\": './logs',\n","    \"logging_steps\": 10,\n","    \"evaluation_strategy\": \"steps\",\n","    \"eval_steps\": 400,\n","    \"save_steps\": 400,\n","    \"save_total_limit\": 3,\n","    \"load_best_model_at_end\": False,\n","    \"metric_for_best_model\": \"f1\",\n","    \"greater_is_better\": True,\n","    \"overwrite_output_dir\": True,\n","    \"report_to\": \"none\",\n","}\n","\n","print(parameter[\"lr\"]*parameter[\"lr_scale_unfreeze\"])"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[],"source":["target = [\n","    'B-EMAIL', 'B-ID_NUM', 'B-NAME_STUDENT', 'B-PHONE_NUM', \n","    'B-STREET_ADDRESS', 'B-URL_PERSONAL', 'B-USERNAME', 'I-ID_NUM', \n","    'I-NAME_STUDENT', 'I-PHONE_NUM', 'I-STREET_ADDRESS', 'I-URL_PERSONAL'\n","]"]},{"cell_type":"code","execution_count":42,"metadata":{"execution":{"iopub.execute_input":"2024-01-25T21:37:35.606208Z","iopub.status.busy":"2024-01-25T21:37:35.605889Z","iopub.status.idle":"2024-01-25T21:37:35.62164Z","shell.execute_reply":"2024-01-25T21:37:35.620746Z","shell.execute_reply.started":"2024-01-25T21:37:35.606175Z"},"trusted":true},"outputs":[],"source":["from itertools import chain\n","import json\n","\n","data = json.load(open(train_path))\n","all_labels = sorted(list(set(chain(*[x[\"labels\"] for x in data]))))\n","label2id = {l: i for i,l in enumerate(all_labels)}\n","id2label = {v:k for k,v in label2id.items()}"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[],"source":["import random\n","\n","def tokenize(example, tokenizer, label2id, max_length, all_labels_list, stride):\n","    text = []\n","    import numpy as np\n","\n","    # these are at the character level\n","    labels = []\n","    targets = []\n","\n","    for t, l, ws in zip(example[\"tokens\"], example[\"labels\"], example[\"trailing_whitespace\"]):\n","\n","        text.append(t)\n","        labels.extend([l]*len(t))\n","        \n","        if l in all_labels_list:\n","            targets.append(1)\n","        else:\n","            targets.append(0)\n","        # if there is trailing whitespace\n","        if ws:\n","            text.append(\" \")\n","            labels.append(\"O\")\n","\n","    tokenized = tokenizer(\n","        \"\".join(text),              \n","        stride=stride,\n","        return_overflowing_tokens=True,\n","        return_offsets_mapping=True, \n","        truncation=True, \n","        max_length=max_length, padding=\"max_length\")\n","    \n","    \n","    target_num = sum(targets)\n","    labels = np.array(labels)\n","\n","    text = \"\".join(text)\n","\n","    for i in range(len(tokenized.input_ids)):\n","        single_tokenized = {\n","            \"input_ids\":tokenized[\"input_ids\"][i],\n","            \"token_type_ids\":tokenized[\"token_type_ids\"][i],\n","            \"attention_mask\":tokenized[\"attention_mask\"][i],\n","            \"offset_mapping\":tokenized[\"offset_mapping\"][i],\n","            \"overflow_to_sample_mapping\":tokenized[\"overflow_to_sample_mapping\"][i],\n","        }\n","        yield get_token_data_for_chunk(single_tokenized, text, label2id, labels, target_num)\n","   \n","            \n","\n","def get_token_data_for_chunk(tokenized, text, label2id, labels, target_num):\n","    token_labels = []\n","\n","    for start_idx, end_idx in tokenized[\"offset_mapping\"]:\n","\n","        # CLS token\n","        if start_idx == 0 and end_idx == 0: \n","            token_labels.append(label2id[\"O\"])\n","            continue\n","\n","        # case when token starts with whitespace\n","        if text[start_idx].isspace():\n","            start_idx += 1\n","\n","        try:\n","            token_labels.append(label2id[labels[start_idx]])\n","        except:\n","            token_labels.append(label2id[\"O\"])\n","\n","    length = len(tokenized[\"input_ids\"])\n","\n","    return {\n","        **tokenized,\n","        \"labels\": token_labels,\n","        \"length\": length,\n","        \"target_num\": target_num,\n","        \"group\": 1 if target_num>0 else 0\n","    }\n","\n","# https://www.kaggle.com/competitions/pii-detection-removal-from-educational-data/discussion/468844\n","def filter_no_pii(example, percent_allow=parameter[\"filter_no_pii_percent_allow\"]):\n","    # Return True if there is PII\n","    # Or 20% of the time if there isn't\n","    has_pii = set(\"O\") != set(example[\"labels\"])\n","    return has_pii or (random.random() < percent_allow)"]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","c:\\Users\\Bernd\\anaconda3\\envs\\mytorch\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:470: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n","  warnings.warn(\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"name":"stdout","output_type":"stream","text":["{'input_ids': [1, 2169, 12103, 270, 3513, 28310, 4593, 271, 57498, 24360, 16789, 271, 1609, 30065, 12287, 662, 86260, 128001, 128001, 6738, 429, 1857, 128001, 128001, 279, 1637, 273, 380, 264, 408, 305, 6998, 1879, 308, 384, 390, 262, 6870, 265, 266, 663, 269, 262, 791, 2269, 260, 128001, 128001, 458, 1444, 269, 266, 791, 2269, 302, 1663, 264, 262, 3742, 265, 72791, 1398, 897, 260, 263, 72791, 1398, 736, 260, 287, 15724, 261, 10040, 268, 5152, 271, 92671, 2531, 280, 51388, 260, 3045, 294, 9110, 25247, 42255, 268, 1931, 280, 65426, 7933, 260, 285, 261, 262, 791, 2269, 287, 698, 59729, 6000, 285, 269, 266, 4981, 5190, 3395, 272, 3832, 262, 1008, 7392, 265, 262, 791, 263, 1279, 262, 1959, 280, 268, 1068, 264, 282, 1315, 260, 45110, 30097, 435, 128001, 128001, 329, 1637, 303, 386, 5228, 294, 128001, 128001, 1795, 325, 269, 3469, 264, 305, 263, 490, 298, 1449, 1318, 1146, 1578, 263, 295, 282, 619, 1126, 128001, 128001, 1795, 325, 269, 18440, 128001, 128001, 1795, 325, 1279, 51669, 263, 9563, 265, 439, 128001, 128001, 1795, 325, 295, 282, 2312, 264, 356, 810, 265, 1364, 294, 1521, 14850, 261, 735, 7273, 261, 1423, 261, 2820, 265, 353, 1000, 128001, 128001, 1795, 325, 269, 2714, 270, 305, 355, 263, 269, 639, 264, 799, 128001, 128001, 1795, 325, 269, 785, 263, 8331, 9963, 128001, 128001, 1795, 325, 682, 3979, 262, 7337, 265, 1205, 261, 1604, 261, 88147, 128001, 128001, 1795, 325, 40478, 268, 128001, 128001, 1795, 325, 682, 262, 663, 14022, 128001, 128001, 1795, 325, 1279, 274, 264, 2314, 1000, 128001, 128001, 279, 2820, 265, 266, 791, 2269, 2351, 275, 299, 781, 320, 40843, 1137, 288, 359, 1386, 260, 329, 1392, 582, 11243, 1000, 320, 6045, 893, 261, 109134, 441, 291, 1386, 267, 266, 25531, 1730, 261, 319, 267, 930, 269, 1797, 275, 283, 386, 6357, 283, 353, 1000, 260, 128001, 128001, 329, 1637, 4292, 5023, 263, 5506, 264, 282, 45769, 261, 278, 269, 266, 2269, 265, 262, 2230, 260, 128001, 128001, 28303, 269, 5469, 401, 2754, 551, 1800, 275, 262, 1459, 260, 128001, 128001, 5736, 429, 22459, 128001, 128001, 273, 564, 262, 568, 265, 262, 791, 2269, 2820, 275, 262, 6998, 2566, 441, 266, 614, 1247, 287, 7551, 289, 1089, 1247, 285, 260, 344, 262, 1386, 265, 262, 1247, 261, 273, 1183, 263, 4738, 262, 2270, 264, 587, 260, 128001, 128001, 3492, 266, 813, 265, 841, 261, 273, 1653, 262, 6998, 267, 14956, 262, 791, 2269, 260, 273, 7052, 262, 813, 265, 841, 970, 264, 262, 2270, 264, 282, 5007, 260, 344, 262, 810, 265, 841, 261, 301, 295, 380, 294, 328, 261, 339, 261, 335, 261, 399, 261, 579, 261, 361, 261, 361, 400, 260, 128001, 128001, 279, 380, 265, 262, 317, 14916, 318, 269, 379, 1257, 264, 796, 262, 5392, 260, 927, 291, 384, 261, 262, 8921, 604, 484, 268, 1161, 292, 41582, 263, 2090, 52521, 264, 9803, 353, 1000, 840, 1029, 265, 7392, 260, 273, 741, 375, 743, 270, 266, 3956, 260, 128001, 128001, 2169, 12103, 270, 3513, 28310, 4593, 271, 57498, 24360, 16789, 271, 1609, 30065, 12287, 662, 86260, 128001, 128001, 643, 14956, 262, 791, 2269, 277, 1089, 261, 273, 9803, 264, 262, 2754, 266, 1412, 15895, 265, 308, 374, 275, 262, 908, 265, 1163, 4321, 261, 1426, 263, 88147, 260, 329, 567, 3956, 327, 437, 268, 375, 743, 263, 1279, 262, 791, 2269, 264, 10358, 260, 1414, 103648, 275, 278, 261, 262, 6998, 2607, 262, 617, 265, 262, 1637, 260, 1060, 261, 262, 567, 3956, 2449, 321, 402, 310, 1000, 263, 14350, 9963, 457, 262, 6998, 260, 8132, 291, 353, 791, 2269, 261, 306, 286, 1859, 264, 374, 603, 263, 409, 264, 365, 3979, 262, 37316, 1000, 260, 128001, 128001, 273, 394, 910, 305, 262, 1205, 273, 2059, 267, 291, 810, 265, 2475, 267, 556, 264, 3468, 4817, 1507, 270, 1129, 271, 12197, 260, 606, 6991, 281, 262, 2233, 265, 312, 460, 1836, 260, 279, 1129, 271, 12197, 281, 2090, 526, 264, 2313, 262, 1604, 265, 262, 1205, 263, 295, 413, 1538, 2359, 264, 15757, 349, 260, 450, 433, 2995, 264, 308, 841, 2022, 264, 266, 24664, 5190, 260, 128001, 128001, 17798, 128001, 128001, 458, 273, 433, 1201, 275, 262, 34554, 265, 291, 810, 265, 3956, 269, 262, 2754, 2897, 270, 262, 663, 260, 329, 1637, 1530, 264, 527, 2094, 260, 279, 2754, 1825, 262, 697, 263, 409, 264, 548, 898, 278, 260, 1060, 261, 306, 1166, 638, 5421, 289, 9340, 265, 262, 663, 260, 336, 1903, 1328, 269, 1119, 261, 2090, 15742, 262, 3450, 265, 1144, 2255, 260, 128001, 128001, 2169, 12103, 270, 3513, 28310, 4593, 271, 57498, 24360, 16789, 271, 1609, 30065, 12287, 662, 86260, 128001, 128001, 30097, 376, 294, 7826, 7035, 22229, 2196, 663, 128001, 128001, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'offset_mapping': [(0, 0), (0, 6), (6, 15), (15, 19), (19, 30), (30, 37), (37, 40), (40, 41), (41, 43), (43, 46), (46, 51), (51, 52), (52, 53), (53, 57), (57, 60), (60, 62), (62, 66), (66, 67), (67, 68), (68, 77), (77, 79), (79, 89), (89, 90), (90, 91), (91, 94), (94, 99), (99, 101), (101, 105), (105, 108), (108, 113), (113, 117), (117, 130), (130, 138), (138, 144), (144, 148), (148, 156), (156, 160), (160, 171), (171, 174), (174, 176), (176, 184), (184, 187), (187, 191), (192, 197), (197, 201), (201, 202), (202, 203), (203, 204), (204, 208), (208, 216), (216, 219), (219, 221), (221, 226), (226, 230), (230, 231), (231, 241), (241, 244), (244, 248), (248, 259), (259, 262), (262, 266), (266, 268), (268, 270), (270, 271), (271, 275), (275, 279), (279, 281), (281, 283), (283, 284), (284, 286), (286, 290), (290, 291), (291, 295), (295, 296), (296, 299), (299, 300), (300, 303), (304, 306), (306, 307), (307, 319), (319, 320), (320, 326), (326, 327), (327, 331), (331, 333), (333, 339), (339, 340), (340, 342), (342, 343), (343, 348), (348, 355), (355, 356), (356, 357), (357, 358), (358, 362), (362, 367), (367, 371), (371, 373), (373, 375), (375, 385), (385, 393), (393, 394), (394, 397), (397, 399), (399, 407), (408, 423), (423, 433), (433, 438), (438, 446), (446, 450), (450, 458), (458, 470), (470, 473), (473, 477), (477, 482), (482, 486), (486, 493), (493, 497), (497, 503), (503, 504), (504, 505), (506, 516), (516, 519), (519, 522), (522, 531), (531, 532), (532, 535), (535, 541), (541, 542), (542, 543), (543, 544), (544, 548), (548, 553), (553, 557), (557, 562), (562, 573), (573, 574), (574, 575), (575, 576), (576, 577), (578, 581), (581, 584), (584, 595), (595, 598), (598, 602), (602, 606), (606, 611), (611, 615), (615, 623), (623, 635), (635, 644), (644, 655), (655, 659), (659, 663), (663, 666), (666, 671), (672, 680), (680, 681), (681, 682), (682, 683), (684, 687), (687, 690), (690, 699), (699, 700), (700, 701), (701, 702), (703, 706), (706, 713), (713, 728), (728, 732), (732, 740), (740, 743), (743, 755), (755, 756), (756, 757), (757, 758), (759, 762), (762, 766), (766, 769), (769, 777), (777, 780), (780, 784), (784, 789), (789, 792), (792, 802), (802, 803), (803, 808), (808, 814), (814, 815), (815, 823), (823, 831), (831, 832), (832, 841), (841, 842), (842, 851), (851, 854), (855, 859), (859, 865), (865, 866), (866, 867), (867, 868), (869, 872), (872, 875), (875, 884), (884, 888), (888, 892), (892, 899), (899, 903), (903, 906), (906, 911), (911, 914), (914, 920), (920, 921), (921, 922), (922, 923), (924, 927), (927, 930), (930, 934), (934, 938), (938, 949), (949, 959), (959, 960), (960, 961), (961, 962), (963, 966), (966, 972), (972, 980), (980, 984), (984, 994), (994, 997), (997, 1006), (1006, 1007), (1007, 1021), (1021, 1022), (1022, 1039), (1039, 1040), (1040, 1041), (1041, 1042), (1043, 1046), (1046, 1057), (1057, 1058), (1058, 1059), (1059, 1060), (1060, 1061), (1062, 1065), (1065, 1071), (1071, 1075), (1075, 1083), (1083, 1098), (1098, 1099), (1099, 1100), (1100, 1101), (1102, 1105), (1105, 1112), (1112, 1116), (1116, 1119), (1119, 1127), (1127, 1133), (1133, 1134), (1134, 1135), (1135, 1138), (1138, 1147), (1147, 1150), (1150, 1152), (1152, 1157), (1157, 1161), (1161, 1168), (1168, 1173), (1173, 1176), (1176, 1181), (1181, 1182), (1182, 1189), (1189, 1197), (1197, 1200), (1200, 1204), (1204, 1211), (1211, 1212), (1212, 1217), (1217, 1226), (1226, 1232), (1233, 1243), (1243, 1249), (1249, 1250), (1250, 1254), (1254, 1260), (1260, 1261), (1261, 1273), (1273, 1280), (1280, 1285), (1285, 1292), (1292, 1295), (1295, 1297), (1297, 1304), (1304, 1314), (1314, 1315), (1315, 1321), (1321, 1324), (1324, 1329), (1329, 1332), (1333, 1343), (1343, 1348), (1348, 1351), (1351, 1356), (1356, 1365), (1365, 1368), (1368, 1372), (1372, 1378), (1378, 1379), (1379, 1380), (1380, 1381), (1381, 1385), (1385, 1390), (1390, 1398), (1398, 1409), (1409, 1413), (1413, 1419), (1419, 1422), (1422, 1425), (1425, 1435), (1435, 1436), (1436, 1439), (1439, 1442), (1442, 1444), (1444, 1448), (1448, 1451), (1451, 1455), (1455, 1464), (1464, 1465), (1465, 1466), (1466, 1467), (1467, 1477), (1477, 1480), (1480, 1489), (1489, 1497), (1497, 1510), (1510, 1515), (1515, 1527), (1527, 1532), (1532, 1536), (1536, 1543), (1543, 1544), (1544, 1545), (1545, 1546), (1546, 1557), (1557, 1559), (1559, 1567), (1567, 1568), (1568, 1569), (1569, 1570), (1570, 1576), (1576, 1580), (1580, 1588), (1588, 1591), (1591, 1595), (1595, 1600), (1600, 1604), (1604, 1613), (1613, 1618), (1618, 1622), (1622, 1635), (1635, 1644), (1644, 1651), (1651, 1653), (1653, 1659), (1659, 1665), (1666, 1668), (1668, 1673), (1673, 1676), (1676, 1682), (1682, 1688), (1688, 1689), (1689, 1690), (1690, 1693), (1693, 1697), (1697, 1704), (1704, 1707), (1707, 1711), (1711, 1717), (1717, 1718), (1718, 1720), (1720, 1726), (1726, 1730), (1730, 1740), (1740, 1744), (1744, 1750), (1750, 1753), (1753, 1760), (1760, 1761), (1761, 1762), (1762, 1763), (1763, 1770), (1770, 1772), (1772, 1779), (1779, 1782), (1782, 1792), (1792, 1793), (1793, 1795), (1795, 1801), (1801, 1805), (1805, 1818), (1818, 1821), (1821, 1831), (1831, 1835), (1835, 1840), (1840, 1844), (1844, 1845), (1845, 1847), (1847, 1853), (1853, 1857), (1857, 1864), (1865, 1868), (1868, 1878), (1878, 1888), (1888, 1891), (1891, 1895), (1895, 1901), (1901, 1904), (1904, 1907), (1907, 1917), (1917, 1918), (1918, 1921), (1921, 1925), (1925, 1930), (1930, 1933), (1933, 1943), (1943, 1944), (1944, 1947), (1947, 1951), (1951, 1955), (1955, 1956), (1956, 1960), (1960, 1961), (1961, 1966), (1966, 1967), (1968, 1973), (1973, 1974), (1974, 1980), (1980, 1981), (1981, 1985), (1985, 1986), (1986, 1990), (1990, 1991), (1991, 1995), (1995, 2000), (2000, 2001), (2001, 2002), (2002, 2003), (2003, 2006), (2006, 2010), (2010, 2013), (2013, 2017), (2017, 2019), (2019, 2022), (2022, 2023), (2023, 2026), (2026, 2031), (2031, 2043), (2043, 2046), (2046, 2057), (2057, 2061), (2061, 2068), (2068, 2069), (2069, 2072), (2072, 2077), (2077, 2081), (2081, 2082), (2082, 2086), (2086, 2098), (2098, 2105), (2106, 2111), (2111, 2112), (2112, 2119), (2119, 2124), (2124, 2134), (2134, 2138), (2138, 2143), (2143, 2149), (2149, 2152), (2152, 2160), (2160, 2164), (2164, 2170), (2170, 2172), (2172, 2177), (2177, 2180), (2180, 2192), (2192, 2193), (2193, 2195), (2195, 2200), (2200, 2204), (2205, 2211), (2211, 2215), (2215, 2217), (2217, 2226), (2226, 2227), (2227, 2228), (2228, 2229), (2229, 2235), (2235, 2244), (2244, 2248), (2248, 2259), (2259, 2266), (2266, 2269), (2269, 2270), (2270, 2272), (2272, 2275), (2275, 2280), (2280, 2281), (2281, 2282), (2282, 2286), (2286, 2289), (2289, 2291), (2291, 2295), (2295, 2296), (2296, 2297), (2297, 2302), (2302, 2312), (2312, 2316), (2316, 2321), (2321, 2325), (2325, 2328), (2328, 2334), (2334, 2335), (2335, 2337), (2337, 2345), (2345, 2348), (2348, 2352), (2352, 2365), (2365, 2367), (2367, 2375), (2375, 2389), (2389, 2392), (2392, 2398), (2399, 2404), (2404, 2409), (2409, 2413), (2413, 2422), (2422, 2425), (2425, 2431), (2431, 2437), (2437, 2438), (2438, 2445), (2445, 2449), (2449, 2466), (2466, 2467), (2467, 2472), (2472, 2479), (2479, 2488), (2488, 2493), (2493, 2498), (2498, 2499), (2500, 2504), (2504, 2510), (2510, 2514), (2514, 2521), (2521, 2525), (2525, 2530), (2530, 2534), (2534, 2537), (2537, 2544), (2544, 2545), (2545, 2550), (2550, 2563), (2563, 2568), (2568, 2571), (2571, 2572), (2572, 2576), (2576, 2589), (2589, 2598), (2599, 2603), (2603, 2609), (2609, 2612), (2612, 2616), (2616, 2621), (2621, 2622), (2622, 2627), (2627, 2628), (2628, 2632), (2632, 2639), (2639, 2648), (2648, 2655), (2655, 2659), (2659, 2664), (2664, 2669), (2669, 2675), (2675, 2679), (2679, 2692), (2693, 2703), (2703, 2711), (2711, 2715), (2715, 2728), (2728, 2729), (2729, 2736), (2736, 2741), (2741, 2745), (2745, 2750), (2750, 2754), (2754, 2755), (2755, 2760), (2760, 2765), (2765, 2773), (2773, 2776), (2776, 2781), (2782, 2791), (2791, 2795), (2795, 2800), (2800, 2803), (2803, 2808), (2808, 2816), (2816, 2820), (2820, 2827), (2827, 2833), (2833, 2834), (2834, 2835), (2835, 2836), (2836, 2837), (2837, 2841), (2841, 2849), (2849, 2853), (2853, 2857), (2857, 2866), (2866, 2868), (2868, 2875), (2875, 2878), (2878, 2883), (2883, 2888), (2888, 2891), (2891, 2898), (2898, 2901), (2901, 2907), (2907, 2910), (2910, 2915), (2915, 2921), (2921, 2935), (2935, 2939), (2940, 2949), (2949, 2950), (2950, 2956), (2956, 2957), (2957, 2963), (2963, 2977), (2977, 2981), (2981, 2985), (2985, 2990), (2990, 2993), (2993, 2996), (2996, 3005), (3005, 3012), (3012, 3013), (3013, 3017), (3017, 3026), (3026, 3027), (3027, 3033), (3033, 3037), (3038, 3043), (3043, 3048), (3048, 3051), (3051, 3060), (3060, 3064), (3064, 3078), (3078, 3081), (3081, 3085), (3085, 3094), (3094, 3098), (3098, 3102), (3102, 3107), (3107, 3113), (3113, 3123), (3123, 3126), (3126, 3135), (3135, 3140), (3140, 3141), (3142, 3147), (3147, 3152), (3152, 3160), (3160, 3163), (3163, 3169), (3169, 3179), (3179, 3185), (3185, 3188), (3188, 3190), (3190, 3200), (3200, 3215), (3215, 3216), (3216, 3217), (3217, 3218), (3218, 3226), (3226, 3227), (3227, 3228), (3228, 3232), (3232, 3234), (3234, 3239), (3239, 3247), (3247, 3252), (3252, 3256), (3256, 3269), (3269, 3272), (3272, 3277), (3277, 3282), (3282, 3285), (3285, 3294), (3294, 3297), (3297, 3301), (3301, 3314), (3314, 3325), (3325, 3329), (3330, 3334), (3334, 3342), (3342, 3343), (3343, 3348), (3348, 3353), (3353, 3359), (3359, 3362), (3362, 3367), (3367, 3375), (3375, 3376), (3376, 3380), (3380, 3393), (3393, 3405), (3405, 3409), (3409, 3415), (3415, 3419), (3419, 3424), (3424, 3427), (3427, 3432), (3433, 3441), (3441, 3444), (3444, 3445), (3445, 3450), (3450, 3451), (3451, 3456), (3456, 3463), (3463, 3470), (3470, 3477), (3477, 3480), (3480, 3489), (3489, 3492), (3492, 3496), (3496, 3504), (3504, 3505), (3505, 3507), (3507, 3513), (3513, 3526), (3526, 3529), (3529, 3535), (3535, 3536), (3537, 3542), (3542, 3555), (3555, 3559), (3559, 3574), (3574, 3577), (3577, 3585), (3585, 3593), (3593, 3594), (3594, 3595), (3595, 3596), (3596, 3602), (3602, 3611), (3611, 3615), (3615, 3626), (3626, 3633), (3633, 3636), (3636, 3637), (3637, 3639), (3639, 3642), (3642, 3647), (3647, 3648), (3648, 3649), (3649, 3653), (3653, 3656), (3656, 3658), (3658, 3662), (3662, 3663), (3663, 3664), (3664, 3669), (3669, 3671), (3671, 3672), (3672, 3677), (3677, 3681), (3681, 3688), (3688, 3699), (3699, 3707), (3707, 3708), (3708, 3709), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0)], 'overflow_to_sample_mapping': 0, 'labels': [12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 2, 2, 2, 8, 8, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 2, 2, 2, 8, 8, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 2, 2, 2, 8, 8, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12], 'length': 2000, 'target_num': 753, 'group': 1}\n"]}],"source":["tokenizer = AutoTokenizer.from_pretrained(parameter[\"model\"])\n","tokenizer.add_tokens(AddedToken(\"\\n\", normalized=False)) \n","\n","data = json.load(open(train_path))\n","for tokenized in tokenize(data[0], tokenizer, label2id, 2000, all_labels, 128):\n","    print(tokenized)\n","    \n"]},{"cell_type":"code","execution_count":45,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5db618cfe93043ad8780e4f6ff415f62","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/10 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["23\n"]}],"source":["def get_ds_dict_from_data(data):\n","    tokenized_list=[]\n","    for i, example in tqdm(enumerate(data), total=len(data)):\n","        for tokenized in tokenize(example, tokenizer, label2id, parameter[\"max_length\"], all_labels, parameter[\"stride\"]):\n","            tokenized_list.append(tokenized)\n","    ds_dict={}\n","    for k in tokenized_list[0].keys():\n","        ds_dict[k] = [x[k] for x in tokenized_list]\n","    return ds_dict\n","\n","ds=get_ds_dict_from_data(data[:10])\n","print(len(ds[\"input_ids\"]))"]},{"cell_type":"code","execution_count":46,"metadata":{},"outputs":[],"source":["len_data=len(data)\n","valid_idx = random.sample(range(len_data), int(len_data*parameter[\"train_test_split\"]))\n","train_idx = list(set(range(len_data)) - set(valid_idx))\n","\n","train_data = [data[i] for i in train_idx]\n","valid_data = [data[i] for i in valid_idx]\n","\n","\n","\n","\n","mixtral_data=json.load(open(mixtral_path))\n","\n","train_data=train_data+mixtral_data\n"]},{"cell_type":"code","execution_count":47,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"532a1242e50445c4b6d9c7a427d7f865","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/8138 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"91da1d6f22ab46b6a5947da708e3188a","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1361 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["train_data_tokenized = get_ds_dict_from_data(train_data)\n","valid_data_tokenized = get_ds_dict_from_data(valid_data)\n"]},{"cell_type":"code","execution_count":48,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"631e74d400824ee0a4260d6727d54c1b","version_major":2,"version_minor":0},"text/plain":["Filter (num_proc=16):   0%|          | 0/17528 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["17528 2829\n"]}],"source":["train_ds = Dataset.from_dict(train_data_tokenized)\n","train_ds=train_ds.filter(filter_no_pii, num_proc=parameter[\"num_proc\"])\n","valid_ds = Dataset.from_dict(valid_data_tokenized)\n","\n","print(len(train_ds), len(valid_ds))\n"]},{"cell_type":"code","execution_count":49,"metadata":{},"outputs":[],"source":["def tokenize_inference(example, tokenizer, max_length):\n","        text = []\n","        for t,  ws in zip(example[\"tokens\"], example[\"trailing_whitespace\"]):\n","            text.append(t)\n","            if ws:\n","                text.append(\" \")\n","        tokenized = tokenizer(\"\".join(text), return_offsets_mapping=True, truncation=True, max_length=max_length, padding=\"max_length\")\n","        text = \"\".join(text)\n","        length = len(tokenized.input_ids)\n","        return {\n","            **tokenized,\n","            \"length\": length,\n","        }\n","        \n","class TestTokenizer():\n","    def __init__(self, tokenizer):\n","        self.tokenizer = tokenizer\n","    \n","    def preprocess(self, example):\n","        # Preprocess the tokens and labels by adding trailing whitespace and labels\n","        tokens = []\n","        tokens_without_ws = []\n","        token_map = [] # Use the index as labels\n","        index = 0\n","        for token, t_ws in zip(example[\"tokens\"], example[\"trailing_whitespace\"]):\n","            tokens_without_ws.append(token)\n","            tokens.append(token)\n","            token_map.extend([index] * len(token))\n","            # Added trailing whitespace and label if true and \n","            if t_ws:\n","                tokens.append(\" \")\n","                token_map.append(-1)\n","            index += 1\n","        return tokens, token_map, tokens_without_ws\n","    \n","    def tokenize(self, example):\n","        tokens, token_map, tokens_without_ws = self.preprocess(example)\n","        text = \"\".join(tokens)\n","        tokenized = self.tokenizer(text, return_offsets_mapping=True, padding=\"max_length\",\n","                                   truncation=True, max_length=parameter[\"inference_max_length\"])\n","        return {**tokenized, \"token_map\": token_map, \"tokens\": tokens, \"tokens_without_ws\": tokens_without_ws} \n","\n","class PiiDatasetInference(torch.utils.data.Dataset):\n","        def __init__(self, dataset, tokenizer):\n","            self.dataset = dataset\n","            self.tokenizer=TestTokenizer(tokenizer)\n","            \n","        def __getitem__(self, idx):\n","            vals=self.tokenizer.tokenize(self.dataset[idx])\n","            input_ids = torch.tensor(vals[\"input_ids\"])\n","            attention_mask = torch.tensor(vals[\"attention_mask\"])\n","            document_id = self.dataset[idx][\"document\"]\n","            return input_ids, attention_mask, document_id, vals\n","        \n","        def __len__(self):\n","            return len(self.dataset)\n","\n","# Convert preds to a list of dictionaries\n","def to_test_submission(preds=None, dataset=None, document_ids=None, id2label=None):\n","    pairs = []\n","    row_id = 0\n","    results = []\n","    \n","    for i in range(len(preds)):\n","        input_ids, attention_mask, document_id, vals = dataset[i]\n","        token_map=vals[\"token_map\"]\n","        offsets=vals[\"offset_mapping\"]\n","        tokens=vals[\"tokens_without_ws\"]\n","        #print(\"tokens\", tokens)\n","        pred=preds[i]\n","        #print(\"original_text\", original_text)\n","        #print(\"token_map\", token_map)\n","        #print(\"offsets\", offsets)   \n","        #print(\"pred\", pred)\n","\n","\n","        for token_pred, input_id, (start_idx, end_idx) in zip(pred, input_ids, offsets):\n","            #print(\"\\nnow doing \", start_idx,  end_idx, token_pred)\n","            if start_idx == 0 and end_idx == 0: # Skip 0 offset\n","                continue\n","            # Skip spaces \n","            while start_idx < len(token_map):\n","                #print(\"loop, start_idx now\", start_idx) \n","                #print(\" tokens[token_map[start_idx]]: \", tokens[token_map[start_idx]] if not tokens[token_map[start_idx]].isspace() else \"WHITESPACE\")          \n","                if token_map[start_idx] == -1: # Skip unknown tokens               \n","                    start_idx += 1\n","                elif tokens[token_map[start_idx]].isspace(): # Skip white space\n","                    start_idx += 1\n","                else:\n","                    break\n","            # Ensure start index < length\n","            if start_idx < len(token_map):\n","                token_id = token_map[start_idx]\n","                #print(\"token_id\", token_id)\n","                #token_id= input_id.item()\n","                label_pred = id2label[token_pred.item()]\n","                #print(\"label_pred\", label_pred)\n","                # ignore \"O\" and whitespace preds\n","                if label_pred != \"O\" and token_id != -1:\n","                    #print(\"is PII\", token_id, label_pred)\n","                    token_str = tokens[token_id]\n","                    pair=(document_id, token_id)\n","                    if pair not in pairs:\n","                        results.append({\n","                            \"row_id\": row_id, \n","                            \"document\": document_id,\n","                            \"token\": token_id, \n","                            \"label\": label_pred,\n","                            \"token_str\": token_str\n","                        })\n","                        pairs.append(pair)\n","                        row_id += 1\n","\n","    # Create a dataframe \n","    return results\n","\n","def create_submission(model, filename=\"submission.csv\"):\n","    data = json.load(open(train_path))\n","    from itertools import chain\n","    all_labels = sorted(list(set(chain(*[x[\"labels\"] for x in data]))))\n","    label2id = {l: i for i,l in enumerate(all_labels)}\n","    id2label = {v:k for k,v in label2id.items()}\n","\n","    data=json.load(open(test_path))\n","    tokenizer = AutoTokenizer.from_pretrained(parameter[\"model\"])\n","    my_dataset=PiiDatasetInference(data, tokenizer)\n","    loader=torch.utils.data.DataLoader(my_dataset, batch_size=1, shuffle=False)\n","\n","    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model.eval()\n","    \n","    # stack all predictions into tensor\n","    all_preds = []\n","\n","    for id, attention_mask, document_ids, vals in loader:\n","        id=id.to(device)\n","        attention_mask=attention_mask.to(device)\n","        preds=model(id, attention_mask).get('logits').argmax(dim=2)\n","        all_preds.append(preds)\n","        #for pred, id in zip(preds.flatten(), id.flatten()):\n","        #    if pred != 12:\n","                #print(f\"Document: {document_id.item()} TOKEN:{tokenizer.decode(id)}  --- pred:{id2label[pred.item()]}\")\n","        #        output[row_id]={\"document\":document_id.item(), \"token\":id.item(), \"label\":id2label[pred.item()]}\n","        #        row_id+=1\n","        #for pred, id in zip(preds.flatten(), id.flatten()):\n","        #    if pred != 12:\n","        #        print(f\"TOKEN:{tokenizer.decode(id)}  --- pred:{id2label[pred.item()]}\")\n","    \n","   \n","    all_preds = torch.cat(all_preds, dim=0)\n","    \n","    results = to_test_submission(preds=all_preds, dataset=my_dataset, document_ids=document_ids, id2label=id2label)\n","    if len(results) == 0:\n","        print(\"Error in create_submission(): No predictions made, probably because the model is not learning. Check the model and the data.\")\n","        return\n","    df = pd.DataFrame(results)\n","    df=df[[\"row_id\", \"document\", \"token\", \"label\"]]\n","    print(df)\n","    df.to_csv(filename, index=False)\n","\n","#create_submission(MyModel(parameter['model'], len(label2id)).to(device), \"submission_just_dumb.csv\")\n","# create_submission(model, \"submission.csv\")\n","    \n","\n","\n"]},{"cell_type":"code","execution_count":50,"metadata":{},"outputs":[],"source":["from transformers import DataCollatorForTokenClassification\n","\n","collator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=16)"]},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[],"source":["# using Trainer and TrainingArguments from transformers\n","\n","\n","def compute_metrics(p, all_labels):\n","    predictions, labels = p\n","    predictions = np.argmax(predictions, axis=2)\n","\n","    # Remove ignored index (special tokens)\n","    true_predictions = [\n","        [all_labels[p] for (p, l) in zip(prediction, label) if l != -100]\n","        for prediction, label in zip(predictions, labels)\n","    ]\n","    true_labels = [\n","        [all_labels[l] for (p, l) in zip(prediction, label) if l != -100]\n","        for prediction, label in zip(predictions, labels)\n","    ]\n","    \n","    recall = recall_score(true_labels, true_predictions)\n","    precision = precision_score(true_labels, true_predictions)\n","    f1_score = (1 + 5*5) * recall * precision / (5*5*precision + recall)\n","    \n","    results = {\n","        'recall': recall,\n","        'precision': precision,\n","        'f1': f1_score\n","    }\n","    return results\n","\n","from functools import partial\n","from transformers import TrainerCallback\n","from transformers.trainer_callback import TrainerControl, TrainerState\n","from transformers.training_args import TrainingArguments\n","\n","def get_trainer(model, train_dataloader, valid_dataloader, learnrate_multiplier=1.0):\n","\n","    if not kaggle:\n","        from transformers.integrations import NeptuneCallback\n","\n","        run = neptune.init_run(\n","            project=\"bernd.heidemann/PII\",\n","            api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIzNjBlYzVkNi0zZTUwLTQ1ODYtODhlNC02NDUxNDg0MDdjNzUifQ==\",\n","        )  # your credentials\n","        run[\"parameters\"] = {\n","        **parameter\n","        }\n","\n","        neptune_callback = NeptuneCallback(run=run, log_model_weights=False, log_parameters=False)\n","\n","    training_args = TrainingArguments(\n","        output_dir='./results',          # output directory\n","        num_train_epochs=parameter[\"epochs_before_unfreeze\"]+parameter[\"epochs_after_unfreeze\"],\n","        per_device_train_batch_size=parameter[\"batch_size\"],  # batch size per device during training\n","        per_device_eval_batch_size=parameter[\"inference_batch_size\"],   # batch size for evaluation\n","        warmup_steps=parameter[\"warumup_steps\"],                # number of warmup steps for learning rate scheduler\n","        weight_decay=parameter[\"weight_decay\"],               # strength of weight decay\n","        logging_dir=parameter[\"logging_dir\"],            # directory for storing logs\n","        logging_steps=parameter[\"logging_steps\"],\n","        evaluation_strategy=parameter[\"evaluation_strategy\"],\n","        eval_steps=parameter[\"eval_steps\"],\n","        save_steps=parameter[\"save_steps\"],\n","        save_total_limit=parameter[\"save_total_limit\"],\n","        load_best_model_at_end=parameter[\"load_best_model_at_end\"],\n","        metric_for_best_model=\"f1\" if not kaggle else \"eval_loss\",\n","        greater_is_better=True if not kaggle else False,\n","        overwrite_output_dir=parameter[\"overwrite_output_dir\"],\n","        report_to=parameter[\"report_to\"],\n","        learning_rate=parameter[\"lr\"]\n","    )\n","\n","    class FreezingCallback(TrainerCallback):\n","        def on_epoch_begin(self, args, state, control, model, **kwargs):\n","            if state.epoch == parameter[\"epochs_before_unfreeze\"]:\n","                # change learning rate\n","                optimizer= kwargs[\"optimizer\"]\n","\n","                for param_group in optimizer.param_groups:\n","                    param_group['lr'] = parameter[\"lr\"]*parameter[\"lr_scale_unfreeze\"]\n","                for param in model.base_model.parameters():\n","                    param.requires_grad = True\n","                \n","    class MyTrainer(Trainer):\n","        def __init__(self, model=None, args=None, train_dataset=None, eval_dataset=None, compute_metrics=None, callbacks=None):\n","            super().__init__(model=model, args=args, train_dataset=train_dataset, eval_dataset=eval_dataset, compute_metrics=compute_metrics, callbacks=callbacks)\n","            # Definieren Sie hier Ihre Gewichte fr die Klassen, z.B. torch.tensor([1.0, 2.0, 0.5])\n","            self.weight = torch.tensor(CROSS_ENTROPY_WEIGHTS).to(device)\n","            self.loss_func=torch.nn.CrossEntropyLoss(ignore_index=-100, weight=torch.tensor(CROSS_ENTROPY_WEIGHTS, dtype=torch.float32).to(device))\n","\n","        def compute_loss(self, model, inputs, return_outputs=False):\n","            labels = inputs.get(\"labels\")\n","            outputs = model(**inputs)\n","            logits = outputs.get('logits')\n","            loss = self.loss_func(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n","            return (loss, outputs) if return_outputs else loss\n","        \n","    trainer = MyTrainer(\n","        model=model,                         # the instantiated  Transformers model to be trained\n","        args=training_args,                  # training arguments, defined above\n","        train_dataset=train_dataloader,         # training dataset\n","        eval_dataset=valid_dataloader,             # evaluation dataset\n","        compute_metrics=partial(compute_metrics, all_labels=all_labels) if not kaggle else None,\n","        callbacks=[neptune_callback, FreezingCallback()] if not kaggle else [FreezingCallback()]\n","    )\n","    return trainer"]},{"cell_type":"code","execution_count":52,"metadata":{},"outputs":[],"source":["\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":53,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["https://app.neptune.ai/bernd.heidemann/PII/e/PII-271\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"250fcef931c54fbdbbdd4526dc8bd0cf","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/6573 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'loss': 2.9511, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.0}\n","{'loss': 3.0291, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.01}\n","{'loss': 2.8849, 'learning_rate': 3e-06, 'epoch': 0.01}\n","{'loss': 2.7755, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.02}\n","{'loss': 2.328, 'learning_rate': 5e-06, 'epoch': 0.02}\n","{'loss': 2.1189, 'learning_rate': 6e-06, 'epoch': 0.03}\n","{'loss': 2.0533, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.03}\n","{'loss': 1.6546, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.04}\n","{'loss': 1.6616, 'learning_rate': 9e-06, 'epoch': 0.04}\n","{'loss': 1.0699, 'learning_rate': 1e-05, 'epoch': 0.05}\n","{'loss': 1.0664, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.05}\n","{'loss': 0.5222, 'learning_rate': 1.2e-05, 'epoch': 0.05}\n","{'loss': 1.1713, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.06}\n","{'loss': 0.7269, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.06}\n","{'loss': 0.7546, 'learning_rate': 1.5e-05, 'epoch': 0.07}\n","{'loss': 0.4735, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.07}\n","{'loss': 0.5376, 'learning_rate': 1.7000000000000003e-05, 'epoch': 0.08}\n","{'loss': 0.4432, 'learning_rate': 1.8e-05, 'epoch': 0.08}\n","{'loss': 0.3929, 'learning_rate': 1.9e-05, 'epoch': 0.09}\n","{'loss': 0.4442, 'learning_rate': 2e-05, 'epoch': 0.09}\n","{'loss': 0.2596, 'learning_rate': 2.1e-05, 'epoch': 0.1}\n","{'loss': 0.4704, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.1}\n","{'loss': 0.14, 'learning_rate': 2.3000000000000003e-05, 'epoch': 0.1}\n","{'loss': 0.6158, 'learning_rate': 2.4e-05, 'epoch': 0.11}\n","{'loss': 0.1801, 'learning_rate': 2.5e-05, 'epoch': 0.11}\n","{'loss': 0.3032, 'learning_rate': 2.6000000000000002e-05, 'epoch': 0.12}\n","{'loss': 0.4617, 'learning_rate': 2.7000000000000002e-05, 'epoch': 0.12}\n","{'loss': 0.2977, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.13}\n","{'loss': 0.0779, 'learning_rate': 2.9e-05, 'epoch': 0.13}\n","{'loss': 0.1532, 'learning_rate': 3e-05, 'epoch': 0.14}\n","{'loss': 0.1537, 'learning_rate': 3.1e-05, 'epoch': 0.14}\n","{'loss': 0.2413, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.15}\n","{'loss': 0.1116, 'learning_rate': 3.3e-05, 'epoch': 0.15}\n","{'loss': 0.2389, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.16}\n","{'loss': 0.1686, 'learning_rate': 3.5e-05, 'epoch': 0.16}\n","{'loss': 0.1707, 'learning_rate': 3.6e-05, 'epoch': 0.16}\n","{'loss': 0.1217, 'learning_rate': 3.7e-05, 'epoch': 0.17}\n","{'loss': 0.0975, 'learning_rate': 3.8e-05, 'epoch': 0.17}\n","{'loss': 0.1437, 'learning_rate': 3.9000000000000006e-05, 'epoch': 0.18}\n","{'loss': 0.0957, 'learning_rate': 4e-05, 'epoch': 0.18}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1b5ef02b3451439cb3fa09c716430f06","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2829 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.02020096406340599, 'eval_recall': 0.8482328482328483, 'eval_precision': 0.29715950473415875, 'eval_f1': 0.7917599641737573, 'eval_runtime': 38.5999, 'eval_samples_per_second': 73.29, 'eval_steps_per_second': 73.29, 'epoch': 0.18}\n","{'loss': 0.2963, 'learning_rate': 4.1e-05, 'epoch': 0.19}\n","{'loss': 0.2102, 'learning_rate': 4.2e-05, 'epoch': 0.19}\n","{'loss': 0.1455, 'learning_rate': 4.3e-05, 'epoch': 0.2}\n","{'loss': 0.0479, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.2}\n","{'loss': 0.1071, 'learning_rate': 4.5e-05, 'epoch': 0.21}\n","{'loss': 0.055, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.21}\n","{'loss': 0.1106, 'learning_rate': 4.7e-05, 'epoch': 0.21}\n","{'loss': 0.0882, 'learning_rate': 4.8e-05, 'epoch': 0.22}\n","{'loss': 0.1225, 'learning_rate': 4.9e-05, 'epoch': 0.22}\n","{'loss': 0.1186, 'learning_rate': 5e-05, 'epoch': 0.23}\n","{'loss': 0.1582, 'learning_rate': 4.991766836818706e-05, 'epoch': 0.23}\n","{'loss': 0.0885, 'learning_rate': 4.9835336736374116e-05, 'epoch': 0.24}\n","{'loss': 0.1967, 'learning_rate': 4.975300510456117e-05, 'epoch': 0.24}\n","{'loss': 0.12, 'learning_rate': 4.967067347274823e-05, 'epoch': 0.25}\n","{'loss': 0.1033, 'learning_rate': 4.9588341840935286e-05, 'epoch': 0.25}\n","{'loss': 0.1187, 'learning_rate': 4.950601020912235e-05, 'epoch': 0.26}\n","{'loss': 0.0608, 'learning_rate': 4.9423678577309406e-05, 'epoch': 0.26}\n","{'loss': 0.0742, 'learning_rate': 4.934134694549646e-05, 'epoch': 0.26}\n","{'loss': 0.5567, 'learning_rate': 4.925901531368352e-05, 'epoch': 0.27}\n","{'loss': 0.047, 'learning_rate': 4.9176683681870575e-05, 'epoch': 0.27}\n","{'loss': 0.1516, 'learning_rate': 4.909435205005763e-05, 'epoch': 0.28}\n","{'loss': 0.0316, 'learning_rate': 4.901202041824469e-05, 'epoch': 0.28}\n","{'loss': 0.0416, 'learning_rate': 4.892968878643175e-05, 'epoch': 0.29}\n","{'loss': 0.1451, 'learning_rate': 4.884735715461881e-05, 'epoch': 0.29}\n","{'loss': 0.2332, 'learning_rate': 4.8765025522805865e-05, 'epoch': 0.3}\n","{'loss': 0.0594, 'learning_rate': 4.868269389099292e-05, 'epoch': 0.3}\n","{'loss': 0.1052, 'learning_rate': 4.860036225917998e-05, 'epoch': 0.31}\n","{'loss': 0.0586, 'learning_rate': 4.851803062736704e-05, 'epoch': 0.31}\n","{'loss': 0.0835, 'learning_rate': 4.84356989955541e-05, 'epoch': 0.31}\n","{'loss': 0.0296, 'learning_rate': 4.8353367363741155e-05, 'epoch': 0.32}\n","{'loss': 0.023, 'learning_rate': 4.827103573192821e-05, 'epoch': 0.32}\n","{'loss': 0.0397, 'learning_rate': 4.818870410011527e-05, 'epoch': 0.33}\n","{'loss': 0.1843, 'learning_rate': 4.8106372468302325e-05, 'epoch': 0.33}\n","{'loss': 0.1439, 'learning_rate': 4.802404083648938e-05, 'epoch': 0.34}\n","{'loss': 0.1114, 'learning_rate': 4.794170920467644e-05, 'epoch': 0.34}\n","{'loss': 0.0175, 'learning_rate': 4.7859377572863495e-05, 'epoch': 0.35}\n","{'loss': 0.1249, 'learning_rate': 4.777704594105055e-05, 'epoch': 0.35}\n","{'loss': 0.042, 'learning_rate': 4.769471430923761e-05, 'epoch': 0.36}\n","{'loss': 0.0795, 'learning_rate': 4.7612382677424665e-05, 'epoch': 0.36}\n","{'loss': 0.0256, 'learning_rate': 4.753005104561173e-05, 'epoch': 0.37}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0d4d141eb17e47adb7e7a53248dbffdc","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2829 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.013400131836533546, 'eval_recall': 0.9272349272349273, 'eval_precision': 0.5357357357357357, 'eval_f1': 0.901886058720591, 'eval_runtime': 38.2334, 'eval_samples_per_second': 73.993, 'eval_steps_per_second': 73.993, 'epoch': 0.37}\n","{'loss': 0.056, 'learning_rate': 4.7447719413798785e-05, 'epoch': 0.37}\n","{'loss': 0.0172, 'learning_rate': 4.736538778198584e-05, 'epoch': 0.37}\n","{'loss': 0.0608, 'learning_rate': 4.72830561501729e-05, 'epoch': 0.38}\n","{'loss': 0.0919, 'learning_rate': 4.7200724518359955e-05, 'epoch': 0.38}\n","{'loss': 0.0205, 'learning_rate': 4.711839288654701e-05, 'epoch': 0.39}\n","{'loss': 0.0385, 'learning_rate': 4.703606125473407e-05, 'epoch': 0.39}\n","{'loss': 0.0023, 'learning_rate': 4.6953729622921125e-05, 'epoch': 0.4}\n","{'loss': 0.0567, 'learning_rate': 4.687139799110819e-05, 'epoch': 0.4}\n","{'loss': 0.056, 'learning_rate': 4.6789066359295245e-05, 'epoch': 0.41}\n","{'loss': 0.0112, 'learning_rate': 4.67067347274823e-05, 'epoch': 0.41}\n","{'loss': 0.0068, 'learning_rate': 4.662440309566936e-05, 'epoch': 0.42}\n","{'loss': 0.0108, 'learning_rate': 4.654207146385642e-05, 'epoch': 0.42}\n","{'loss': 0.0428, 'learning_rate': 4.645973983204348e-05, 'epoch': 0.42}\n","{'loss': 0.0334, 'learning_rate': 4.6377408200230535e-05, 'epoch': 0.43}\n","{'loss': 0.0221, 'learning_rate': 4.629507656841759e-05, 'epoch': 0.43}\n","{'loss': 0.0559, 'learning_rate': 4.621274493660465e-05, 'epoch': 0.44}\n","{'loss': 0.0129, 'learning_rate': 4.6130413304791704e-05, 'epoch': 0.44}\n","{'loss': 0.0824, 'learning_rate': 4.604808167297876e-05, 'epoch': 0.45}\n","{'loss': 0.0356, 'learning_rate': 4.596575004116582e-05, 'epoch': 0.45}\n","{'loss': 0.0747, 'learning_rate': 4.5883418409352874e-05, 'epoch': 0.46}\n","{'loss': 0.0338, 'learning_rate': 4.580108677753993e-05, 'epoch': 0.46}\n","{'loss': 0.0409, 'learning_rate': 4.571875514572699e-05, 'epoch': 0.47}\n","{'loss': 0.0106, 'learning_rate': 4.563642351391405e-05, 'epoch': 0.47}\n","{'loss': 0.0547, 'learning_rate': 4.555409188210111e-05, 'epoch': 0.47}\n","{'loss': 0.0424, 'learning_rate': 4.5471760250288164e-05, 'epoch': 0.48}\n","{'loss': 0.0151, 'learning_rate': 4.538942861847522e-05, 'epoch': 0.48}\n","{'loss': 0.2808, 'learning_rate': 4.530709698666228e-05, 'epoch': 0.49}\n","{'loss': 0.11, 'learning_rate': 4.5224765354849334e-05, 'epoch': 0.49}\n","{'loss': 0.0065, 'learning_rate': 4.514243372303639e-05, 'epoch': 0.5}\n","{'loss': 0.0439, 'learning_rate': 4.506010209122345e-05, 'epoch': 0.5}\n","{'loss': 0.0236, 'learning_rate': 4.4977770459410504e-05, 'epoch': 0.51}\n","{'loss': 0.081, 'learning_rate': 4.489543882759756e-05, 'epoch': 0.51}\n","{'loss': 0.01, 'learning_rate': 4.481310719578462e-05, 'epoch': 0.52}\n","{'loss': 0.0082, 'learning_rate': 4.473077556397168e-05, 'epoch': 0.52}\n","{'loss': 0.0037, 'learning_rate': 4.464844393215874e-05, 'epoch': 0.52}\n","{'loss': 0.0367, 'learning_rate': 4.4566112300345794e-05, 'epoch': 0.53}\n","{'loss': 0.0681, 'learning_rate': 4.448378066853286e-05, 'epoch': 0.53}\n","{'loss': 0.0227, 'learning_rate': 4.4401449036719914e-05, 'epoch': 0.54}\n","{'loss': 0.016, 'learning_rate': 4.431911740490697e-05, 'epoch': 0.54}\n","{'loss': 0.0514, 'learning_rate': 4.423678577309403e-05, 'epoch': 0.55}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"161c576ea5e5407d8e161939606ef237","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2829 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.02234867587685585, 'eval_recall': 0.9074844074844075, 'eval_precision': 0.5262206148282098, 'eval_f1': 0.8828814811933564, 'eval_runtime': 38.0555, 'eval_samples_per_second': 74.339, 'eval_steps_per_second': 74.339, 'epoch': 0.55}\n","{'loss': 0.2566, 'learning_rate': 4.4154454141281084e-05, 'epoch': 0.55}\n","{'loss': 0.0224, 'learning_rate': 4.407212250946814e-05, 'epoch': 0.56}\n","{'loss': 0.0163, 'learning_rate': 4.39897908776552e-05, 'epoch': 0.56}\n","{'loss': 0.0137, 'learning_rate': 4.3907459245842254e-05, 'epoch': 0.57}\n","{'loss': 0.0077, 'learning_rate': 4.382512761402931e-05, 'epoch': 0.57}\n","{'loss': 0.0853, 'learning_rate': 4.374279598221637e-05, 'epoch': 0.58}\n","{'loss': 0.5094, 'learning_rate': 4.366046435040343e-05, 'epoch': 0.58}\n","{'loss': 0.0103, 'learning_rate': 4.357813271859049e-05, 'epoch': 0.58}\n","{'loss': 0.0139, 'learning_rate': 4.3495801086777543e-05, 'epoch': 0.59}\n","{'loss': 0.0901, 'learning_rate': 4.34134694549646e-05, 'epoch': 0.59}\n","{'loss': 0.0699, 'learning_rate': 4.333113782315166e-05, 'epoch': 0.6}\n","{'loss': 0.0066, 'learning_rate': 4.324880619133871e-05, 'epoch': 0.6}\n","{'loss': 0.0057, 'learning_rate': 4.316647455952577e-05, 'epoch': 0.61}\n","{'loss': 0.0136, 'learning_rate': 4.308414292771283e-05, 'epoch': 0.61}\n","{'loss': 0.0015, 'learning_rate': 4.300181129589988e-05, 'epoch': 0.62}\n","{'loss': 0.0246, 'learning_rate': 4.291947966408694e-05, 'epoch': 0.62}\n","{'loss': 0.0162, 'learning_rate': 4.2837148032273996e-05, 'epoch': 0.63}\n","{'loss': 0.0324, 'learning_rate': 4.275481640046106e-05, 'epoch': 0.63}\n","{'loss': 0.1985, 'learning_rate': 4.2672484768648117e-05, 'epoch': 0.63}\n","{'loss': 0.0302, 'learning_rate': 4.259015313683517e-05, 'epoch': 0.64}\n","{'loss': 0.0136, 'learning_rate': 4.250782150502223e-05, 'epoch': 0.64}\n","{'loss': 0.0044, 'learning_rate': 4.242548987320929e-05, 'epoch': 0.65}\n","{'loss': 0.0335, 'learning_rate': 4.234315824139635e-05, 'epoch': 0.65}\n","{'loss': 0.052, 'learning_rate': 4.2260826609583406e-05, 'epoch': 0.66}\n","{'loss': 0.0213, 'learning_rate': 4.217849497777046e-05, 'epoch': 0.66}\n","{'loss': 0.0025, 'learning_rate': 4.209616334595752e-05, 'epoch': 0.67}\n","{'loss': 0.0025, 'learning_rate': 4.2013831714144576e-05, 'epoch': 0.67}\n","{'loss': 0.0099, 'learning_rate': 4.193150008233163e-05, 'epoch': 0.68}\n","{'loss': 0.0019, 'learning_rate': 4.184916845051869e-05, 'epoch': 0.68}\n","{'loss': 0.0039, 'learning_rate': 4.176683681870575e-05, 'epoch': 0.68}\n","{'loss': 0.0021, 'learning_rate': 4.168450518689281e-05, 'epoch': 0.69}\n","{'loss': 0.0014, 'learning_rate': 4.1602173555079866e-05, 'epoch': 0.69}\n","{'loss': 0.0164, 'learning_rate': 4.151984192326692e-05, 'epoch': 0.7}\n","{'loss': 0.0504, 'learning_rate': 4.143751029145398e-05, 'epoch': 0.7}\n","{'loss': 0.0366, 'learning_rate': 4.1355178659641036e-05, 'epoch': 0.71}\n","{'loss': 0.0037, 'learning_rate': 4.127284702782809e-05, 'epoch': 0.71}\n","{'loss': 0.0236, 'learning_rate': 4.119051539601515e-05, 'epoch': 0.72}\n","{'loss': 0.035, 'learning_rate': 4.1108183764202206e-05, 'epoch': 0.72}\n","{'loss': 0.0224, 'learning_rate': 4.102585213238926e-05, 'epoch': 0.73}\n","{'loss': 0.0027, 'learning_rate': 4.094352050057632e-05, 'epoch': 0.73}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"07166f855d19419ebe833835ddbcbcbd","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2829 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.012712503783404827, 'eval_recall': 0.8814968814968815, 'eval_precision': 0.47032723239046037, 'eval_f1': 0.852821722817468, 'eval_runtime': 37.8275, 'eval_samples_per_second': 74.787, 'eval_steps_per_second': 74.787, 'epoch': 0.73}\n","{'loss': 0.0168, 'learning_rate': 4.0861188868763376e-05, 'epoch': 0.73}\n","{'loss': 0.0039, 'learning_rate': 4.077885723695044e-05, 'epoch': 0.74}\n","{'loss': 0.0031, 'learning_rate': 4.0696525605137496e-05, 'epoch': 0.74}\n","{'loss': 0.0149, 'learning_rate': 4.061419397332455e-05, 'epoch': 0.75}\n","{'loss': 0.0482, 'learning_rate': 4.053186234151161e-05, 'epoch': 0.75}\n","{'loss': 0.0049, 'learning_rate': 4.0449530709698666e-05, 'epoch': 0.76}\n","{'loss': 0.0094, 'learning_rate': 4.036719907788572e-05, 'epoch': 0.76}\n","{'loss': 0.0082, 'learning_rate': 4.0284867446072786e-05, 'epoch': 0.77}\n","{'loss': 0.0218, 'learning_rate': 4.020253581425984e-05, 'epoch': 0.77}\n","{'loss': 0.0106, 'learning_rate': 4.01202041824469e-05, 'epoch': 0.78}\n","{'loss': 0.0539, 'learning_rate': 4.0037872550633956e-05, 'epoch': 0.78}\n","{'loss': 0.0021, 'learning_rate': 3.995554091882101e-05, 'epoch': 0.79}\n","{'loss': 0.0067, 'learning_rate': 3.9873209287008076e-05, 'epoch': 0.79}\n","{'loss': 0.0276, 'learning_rate': 3.979087765519513e-05, 'epoch': 0.79}\n","{'loss': 0.0106, 'learning_rate': 3.970854602338219e-05, 'epoch': 0.8}\n","{'loss': 0.1394, 'learning_rate': 3.9626214391569245e-05, 'epoch': 0.8}\n","{'loss': 0.0893, 'learning_rate': 3.95438827597563e-05, 'epoch': 0.81}\n","{'loss': 0.004, 'learning_rate': 3.946155112794336e-05, 'epoch': 0.81}\n","{'loss': 0.0128, 'learning_rate': 3.9379219496130415e-05, 'epoch': 0.82}\n","{'loss': 0.0185, 'learning_rate': 3.929688786431747e-05, 'epoch': 0.82}\n","{'loss': 0.0126, 'learning_rate': 3.921455623250453e-05, 'epoch': 0.83}\n","{'loss': 0.0015, 'learning_rate': 3.9132224600691585e-05, 'epoch': 0.83}\n","{'loss': 0.0024, 'learning_rate': 3.904989296887864e-05, 'epoch': 0.84}\n","{'loss': 0.0116, 'learning_rate': 3.89675613370657e-05, 'epoch': 0.84}\n","{'loss': 0.045, 'learning_rate': 3.888522970525276e-05, 'epoch': 0.84}\n","{'loss': 0.0312, 'learning_rate': 3.880289807343982e-05, 'epoch': 0.85}\n","{'loss': 0.1582, 'learning_rate': 3.8720566441626875e-05, 'epoch': 0.85}\n","{'loss': 0.186, 'learning_rate': 3.863823480981393e-05, 'epoch': 0.86}\n","{'loss': 0.0981, 'learning_rate': 3.855590317800099e-05, 'epoch': 0.86}\n","{'loss': 0.0042, 'learning_rate': 3.8473571546188045e-05, 'epoch': 0.87}\n","{'loss': 0.1959, 'learning_rate': 3.83912399143751e-05, 'epoch': 0.87}\n","{'loss': 0.0043, 'learning_rate': 3.830890828256216e-05, 'epoch': 0.88}\n","{'loss': 0.0234, 'learning_rate': 3.822657665074922e-05, 'epoch': 0.88}\n","{'loss': 0.041, 'learning_rate': 3.814424501893628e-05, 'epoch': 0.89}\n","{'loss': 0.0034, 'learning_rate': 3.8061913387123335e-05, 'epoch': 0.89}\n","{'loss': 0.0087, 'learning_rate': 3.797958175531039e-05, 'epoch': 0.89}\n","{'loss': 0.0955, 'learning_rate': 3.7897250123497455e-05, 'epoch': 0.9}\n","{'loss': 0.0243, 'learning_rate': 3.781491849168451e-05, 'epoch': 0.9}\n","{'loss': 0.0061, 'learning_rate': 3.773258685987157e-05, 'epoch': 0.91}\n","{'loss': 0.0008, 'learning_rate': 3.7650255228058625e-05, 'epoch': 0.91}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"818eb74826c745e88f5b4fa197190655","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2829 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.013211013749241829, 'eval_recall': 0.8076923076923077, 'eval_precision': 0.5886363636363636, 'eval_f1': 0.7962948364209697, 'eval_runtime': 37.9864, 'eval_samples_per_second': 74.474, 'eval_steps_per_second': 74.474, 'epoch': 0.91}\n","{'loss': 0.0327, 'learning_rate': 3.756792359624568e-05, 'epoch': 0.92}\n","{'loss': 0.0029, 'learning_rate': 3.748559196443274e-05, 'epoch': 0.92}\n","{'loss': 0.07, 'learning_rate': 3.7403260332619795e-05, 'epoch': 0.93}\n","{'loss': 0.0109, 'learning_rate': 3.732092870080685e-05, 'epoch': 0.93}\n","{'loss': 0.022, 'learning_rate': 3.723859706899391e-05, 'epoch': 0.94}\n","{'loss': 0.0096, 'learning_rate': 3.7156265437180965e-05, 'epoch': 0.94}\n","{'loss': 0.0016, 'learning_rate': 3.707393380536802e-05, 'epoch': 0.94}\n","{'loss': 0.0232, 'learning_rate': 3.6991602173555085e-05, 'epoch': 0.95}\n","{'loss': 0.0046, 'learning_rate': 3.690927054174214e-05, 'epoch': 0.95}\n","{'loss': 0.0021, 'learning_rate': 3.68269389099292e-05, 'epoch': 0.96}\n","{'loss': 0.003, 'learning_rate': 3.6744607278116254e-05, 'epoch': 0.96}\n","{'loss': 0.1755, 'learning_rate': 3.666227564630331e-05, 'epoch': 0.97}\n","{'loss': 0.0017, 'learning_rate': 3.657994401449037e-05, 'epoch': 0.97}\n","{'loss': 0.0026, 'learning_rate': 3.6497612382677424e-05, 'epoch': 0.98}\n","{'loss': 0.013, 'learning_rate': 3.641528075086448e-05, 'epoch': 0.98}\n","{'loss': 0.0022, 'learning_rate': 3.633294911905154e-05, 'epoch': 0.99}\n","{'loss': 0.0072, 'learning_rate': 3.6250617487238594e-05, 'epoch': 0.99}\n","{'loss': 0.3541, 'learning_rate': 3.616828585542566e-05, 'epoch': 0.99}\n","{'loss': 0.0195, 'learning_rate': 3.6085954223612714e-05, 'epoch': 1.0}\n","{'loss': 0.018, 'learning_rate': 3.600362259179977e-05, 'epoch': 1.0}\n","{'loss': 0.0034, 'learning_rate': 3.592129095998683e-05, 'epoch': 1.01}\n","{'loss': 0.0013, 'learning_rate': 3.583895932817389e-05, 'epoch': 1.01}\n","{'loss': 0.0017, 'learning_rate': 3.575662769636095e-05, 'epoch': 1.02}\n","{'loss': 0.0081, 'learning_rate': 3.5674296064548004e-05, 'epoch': 1.02}\n","{'loss': 0.0015, 'learning_rate': 3.559196443273506e-05, 'epoch': 1.03}\n","{'loss': 0.0029, 'learning_rate': 3.550963280092212e-05, 'epoch': 1.03}\n","{'loss': 0.0007, 'learning_rate': 3.5427301169109174e-05, 'epoch': 1.04}\n","{'loss': 0.002, 'learning_rate': 3.534496953729623e-05, 'epoch': 1.04}\n","{'loss': 0.0047, 'learning_rate': 3.526263790548329e-05, 'epoch': 1.05}\n","{'loss': 0.0019, 'learning_rate': 3.5180306273670344e-05, 'epoch': 1.05}\n","{'loss': 0.0017, 'learning_rate': 3.50979746418574e-05, 'epoch': 1.05}\n","{'loss': 0.0013, 'learning_rate': 3.5015643010044464e-05, 'epoch': 1.06}\n","{'loss': 0.0056, 'learning_rate': 3.493331137823152e-05, 'epoch': 1.06}\n","{'loss': 0.0053, 'learning_rate': 3.485097974641858e-05, 'epoch': 1.07}\n","{'loss': 0.0385, 'learning_rate': 3.4768648114605634e-05, 'epoch': 1.07}\n","{'loss': 0.0059, 'learning_rate': 3.468631648279269e-05, 'epoch': 1.08}\n","{'loss': 0.0011, 'learning_rate': 3.460398485097975e-05, 'epoch': 1.08}\n","{'loss': 0.0314, 'learning_rate': 3.4521653219166804e-05, 'epoch': 1.09}\n","{'loss': 0.0008, 'learning_rate': 3.443932158735386e-05, 'epoch': 1.09}\n","{'loss': 0.0006, 'learning_rate': 3.435698995554092e-05, 'epoch': 1.1}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7a5bacdec2434328ab15e9e9bedfbdfc","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2829 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.019407890737056732, 'eval_recall': 0.7286902286902287, 'eval_precision': 0.6332429990966576, 'eval_f1': 0.7244902015343643, 'eval_runtime': 38.152, 'eval_samples_per_second': 74.151, 'eval_steps_per_second': 74.151, 'epoch': 1.1}\n","{'loss': 0.0215, 'learning_rate': 3.4274658323727974e-05, 'epoch': 1.1}\n","{'loss': 0.0037, 'learning_rate': 3.419232669191503e-05, 'epoch': 1.1}\n","{'loss': 0.0103, 'learning_rate': 3.410999506010209e-05, 'epoch': 1.11}\n","{'loss': 0.0013, 'learning_rate': 3.402766342828915e-05, 'epoch': 1.11}\n","{'loss': 0.1877, 'learning_rate': 3.394533179647621e-05, 'epoch': 1.12}\n","{'loss': 0.0008, 'learning_rate': 3.3863000164663263e-05, 'epoch': 1.12}\n","{'loss': 0.0221, 'learning_rate': 3.378066853285033e-05, 'epoch': 1.13}\n","{'loss': 0.004, 'learning_rate': 3.3698336901037383e-05, 'epoch': 1.13}\n","{'loss': 0.0017, 'learning_rate': 3.361600526922444e-05, 'epoch': 1.14}\n","{'loss': 0.0104, 'learning_rate': 3.35336736374115e-05, 'epoch': 1.14}\n","{'loss': 0.0701, 'learning_rate': 3.345134200559855e-05, 'epoch': 1.15}\n","{'loss': 0.0021, 'learning_rate': 3.336901037378561e-05, 'epoch': 1.15}\n","{'loss': 0.0008, 'learning_rate': 3.3286678741972667e-05, 'epoch': 1.15}\n","{'loss': 0.0012, 'learning_rate': 3.320434711015972e-05, 'epoch': 1.16}\n","{'loss': 0.0007, 'learning_rate': 3.3122015478346787e-05, 'epoch': 1.16}\n","{'loss': 0.0378, 'learning_rate': 3.303968384653384e-05, 'epoch': 1.17}\n","{'loss': 0.0056, 'learning_rate': 3.29573522147209e-05, 'epoch': 1.17}\n","{'loss': 0.0019, 'learning_rate': 3.2875020582907956e-05, 'epoch': 1.18}\n","{'loss': 0.0008, 'learning_rate': 3.279268895109501e-05, 'epoch': 1.18}\n","{'loss': 0.0051, 'learning_rate': 3.271035731928207e-05, 'epoch': 1.19}\n","{'loss': 0.0179, 'learning_rate': 3.2628025687469126e-05, 'epoch': 1.19}\n","{'loss': 0.0019, 'learning_rate': 3.254569405565618e-05, 'epoch': 1.2}\n","{'loss': 0.1996, 'learning_rate': 3.246336242384324e-05, 'epoch': 1.2}\n","{'loss': 0.0022, 'learning_rate': 3.2381030792030296e-05, 'epoch': 1.2}\n","{'loss': 0.0231, 'learning_rate': 3.229869916021735e-05, 'epoch': 1.21}\n","{'loss': 0.0026, 'learning_rate': 3.221636752840441e-05, 'epoch': 1.21}\n","{'loss': 0.0049, 'learning_rate': 3.213403589659147e-05, 'epoch': 1.22}\n","{'loss': 0.0017, 'learning_rate': 3.205170426477853e-05, 'epoch': 1.22}\n","{'loss': 0.0033, 'learning_rate': 3.1969372632965586e-05, 'epoch': 1.23}\n","{'loss': 0.0194, 'learning_rate': 3.188704100115264e-05, 'epoch': 1.23}\n","{'loss': 0.0097, 'learning_rate': 3.18047093693397e-05, 'epoch': 1.24}\n","{'loss': 0.0016, 'learning_rate': 3.172237773752676e-05, 'epoch': 1.24}\n","{'loss': 0.0044, 'learning_rate': 3.164004610571382e-05, 'epoch': 1.25}\n","{'loss': 0.0004, 'learning_rate': 3.1557714473900876e-05, 'epoch': 1.25}\n","{'loss': 0.0004, 'learning_rate': 3.147538284208793e-05, 'epoch': 1.26}\n","{'loss': 0.0074, 'learning_rate': 3.139305121027499e-05, 'epoch': 1.26}\n","{'loss': 0.007, 'learning_rate': 3.1310719578462046e-05, 'epoch': 1.26}\n","{'loss': 0.0555, 'learning_rate': 3.12283879466491e-05, 'epoch': 1.27}\n","{'loss': 0.0005, 'learning_rate': 3.1146056314836166e-05, 'epoch': 1.27}\n","{'loss': 0.0048, 'learning_rate': 3.106372468302322e-05, 'epoch': 1.28}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f0dd53bda6484f42801f8bd10d81a967","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2829 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.010665029287338257, 'eval_recall': 0.9282744282744283, 'eval_precision': 0.6297602256699577, 'eval_f1': 0.911653840113083, 'eval_runtime': 37.5572, 'eval_samples_per_second': 75.325, 'eval_steps_per_second': 75.325, 'epoch': 1.28}\n","{'loss': 0.0008, 'learning_rate': 3.098139305121028e-05, 'epoch': 1.28}\n","{'loss': 0.0016, 'learning_rate': 3.0899061419397336e-05, 'epoch': 1.29}\n","{'loss': 0.0015, 'learning_rate': 3.081672978758439e-05, 'epoch': 1.29}\n","{'loss': 0.0008, 'learning_rate': 3.073439815577145e-05, 'epoch': 1.3}\n","{'loss': 0.0647, 'learning_rate': 3.0652066523958506e-05, 'epoch': 1.3}\n","{'loss': 0.0007, 'learning_rate': 3.056973489214556e-05, 'epoch': 1.31}\n","{'loss': 0.0005, 'learning_rate': 3.048740326033262e-05, 'epoch': 1.31}\n","{'loss': 0.0063, 'learning_rate': 3.0405071628519676e-05, 'epoch': 1.31}\n","{'loss': 0.0466, 'learning_rate': 3.0322739996706732e-05, 'epoch': 1.32}\n","{'loss': 0.0324, 'learning_rate': 3.0240408364893796e-05, 'epoch': 1.32}\n","{'loss': 0.0013, 'learning_rate': 3.0158076733080852e-05, 'epoch': 1.33}\n","{'loss': 0.006, 'learning_rate': 3.007574510126791e-05, 'epoch': 1.33}\n","{'loss': 0.0044, 'learning_rate': 2.999341346945497e-05, 'epoch': 1.34}\n","{'loss': 0.0142, 'learning_rate': 2.9911081837642025e-05, 'epoch': 1.34}\n","{'loss': 0.0135, 'learning_rate': 2.9828750205829082e-05, 'epoch': 1.35}\n","{'loss': 0.0129, 'learning_rate': 2.974641857401614e-05, 'epoch': 1.35}\n","{'loss': 0.0016, 'learning_rate': 2.9664086942203195e-05, 'epoch': 1.36}\n","{'loss': 0.0016, 'learning_rate': 2.9581755310390252e-05, 'epoch': 1.36}\n","{'loss': 0.1122, 'learning_rate': 2.949942367857731e-05, 'epoch': 1.36}\n","{'loss': 0.0047, 'learning_rate': 2.9417092046764365e-05, 'epoch': 1.37}\n","{'loss': 0.0021, 'learning_rate': 2.9334760414951422e-05, 'epoch': 1.37}\n","{'loss': 0.0132, 'learning_rate': 2.9252428783138485e-05, 'epoch': 1.38}\n","{'loss': 0.0004, 'learning_rate': 2.9170097151325542e-05, 'epoch': 1.38}\n","{'loss': 0.0012, 'learning_rate': 2.90877655195126e-05, 'epoch': 1.39}\n","{'loss': 0.0052, 'learning_rate': 2.9005433887699655e-05, 'epoch': 1.39}\n","{'loss': 0.0009, 'learning_rate': 2.8923102255886715e-05, 'epoch': 1.4}\n","{'loss': 0.0075, 'learning_rate': 2.8840770624073772e-05, 'epoch': 1.4}\n","{'loss': 0.0015, 'learning_rate': 2.875843899226083e-05, 'epoch': 1.41}\n","{'loss': 0.0003, 'learning_rate': 2.8676107360447885e-05, 'epoch': 1.41}\n","{'loss': 0.0173, 'learning_rate': 2.859377572863494e-05, 'epoch': 1.41}\n","{'loss': 0.0044, 'learning_rate': 2.8511444096821998e-05, 'epoch': 1.42}\n","{'loss': 0.0649, 'learning_rate': 2.8429112465009055e-05, 'epoch': 1.42}\n","{'loss': 0.0022, 'learning_rate': 2.834678083319611e-05, 'epoch': 1.43}\n","{'loss': 0.0007, 'learning_rate': 2.8264449201383175e-05, 'epoch': 1.43}\n","{'loss': 0.0063, 'learning_rate': 2.818211756957023e-05, 'epoch': 1.44}\n","{'loss': 0.0236, 'learning_rate': 2.8099785937757288e-05, 'epoch': 1.44}\n","{'loss': 0.0009, 'learning_rate': 2.8017454305944345e-05, 'epoch': 1.45}\n","{'loss': 0.0015, 'learning_rate': 2.7935122674131405e-05, 'epoch': 1.45}\n","{'loss': 0.0349, 'learning_rate': 2.785279104231846e-05, 'epoch': 1.46}\n","{'loss': 0.0085, 'learning_rate': 2.7770459410505518e-05, 'epoch': 1.46}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"629824a5baca46e3a394e0708be28e3d","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2829 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.010004960000514984, 'eval_recall': 0.9760914760914761, 'eval_precision': 0.6361788617886179, 'eval_f1': 0.9564365744730863, 'eval_runtime': 37.577, 'eval_samples_per_second': 75.285, 'eval_steps_per_second': 75.285, 'epoch': 1.46}\n","{'loss': 0.001, 'learning_rate': 2.7688127778692575e-05, 'epoch': 1.47}\n","{'loss': 0.0007, 'learning_rate': 2.760579614687963e-05, 'epoch': 1.47}\n","{'loss': 0.0009, 'learning_rate': 2.7523464515066688e-05, 'epoch': 1.47}\n","{'loss': 0.0005, 'learning_rate': 2.7441132883253745e-05, 'epoch': 1.48}\n","{'loss': 0.0015, 'learning_rate': 2.7358801251440808e-05, 'epoch': 1.48}\n","{'loss': 0.0159, 'learning_rate': 2.7276469619627865e-05, 'epoch': 1.49}\n","{'loss': 0.0007, 'learning_rate': 2.719413798781492e-05, 'epoch': 1.49}\n","{'loss': 0.0052, 'learning_rate': 2.7111806356001978e-05, 'epoch': 1.5}\n","{'loss': 0.0604, 'learning_rate': 2.7029474724189034e-05, 'epoch': 1.5}\n","{'loss': 0.0029, 'learning_rate': 2.694714309237609e-05, 'epoch': 1.51}\n","{'loss': 0.0018, 'learning_rate': 2.686481146056315e-05, 'epoch': 1.51}\n","{'loss': 0.0009, 'learning_rate': 2.6782479828750208e-05, 'epoch': 1.52}\n","{'loss': 0.0643, 'learning_rate': 2.6700148196937264e-05, 'epoch': 1.52}\n","{'loss': 0.0025, 'learning_rate': 2.661781656512432e-05, 'epoch': 1.52}\n","{'loss': 0.002, 'learning_rate': 2.6535484933311378e-05, 'epoch': 1.53}\n","{'loss': 0.0159, 'learning_rate': 2.6453153301498434e-05, 'epoch': 1.53}\n","{'loss': 0.0021, 'learning_rate': 2.6370821669685498e-05, 'epoch': 1.54}\n","{'loss': 0.0026, 'learning_rate': 2.6288490037872554e-05, 'epoch': 1.54}\n","{'loss': 0.0021, 'learning_rate': 2.620615840605961e-05, 'epoch': 1.55}\n","{'loss': 0.007, 'learning_rate': 2.6123826774246667e-05, 'epoch': 1.55}\n","{'loss': 0.052, 'learning_rate': 2.6041495142433724e-05, 'epoch': 1.56}\n","{'loss': 0.0093, 'learning_rate': 2.595916351062078e-05, 'epoch': 1.56}\n","{'loss': 0.0022, 'learning_rate': 2.5876831878807837e-05, 'epoch': 1.57}\n","{'loss': 0.0066, 'learning_rate': 2.5794500246994897e-05, 'epoch': 1.57}\n","{'loss': 0.0006, 'learning_rate': 2.5712168615181954e-05, 'epoch': 1.57}\n","{'loss': 0.0081, 'learning_rate': 2.562983698336901e-05, 'epoch': 1.58}\n","{'loss': 0.0005, 'learning_rate': 2.5547505351556067e-05, 'epoch': 1.58}\n","{'loss': 0.0006, 'learning_rate': 2.5465173719743124e-05, 'epoch': 1.59}\n","{'loss': 0.0012, 'learning_rate': 2.5382842087930187e-05, 'epoch': 1.59}\n","{'loss': 0.0022, 'learning_rate': 2.5300510456117244e-05, 'epoch': 1.6}\n","{'loss': 0.0062, 'learning_rate': 2.52181788243043e-05, 'epoch': 1.6}\n","{'loss': 0.002, 'learning_rate': 2.5135847192491357e-05, 'epoch': 1.61}\n","{'loss': 0.0045, 'learning_rate': 2.5053515560678414e-05, 'epoch': 1.61}\n","{'loss': 0.0006, 'learning_rate': 2.497118392886547e-05, 'epoch': 1.62}\n","{'loss': 0.0216, 'learning_rate': 2.4888852297052527e-05, 'epoch': 1.62}\n","{'loss': 0.0218, 'learning_rate': 2.4806520665239587e-05, 'epoch': 1.62}\n","{'loss': 0.0032, 'learning_rate': 2.4724189033426644e-05, 'epoch': 1.63}\n","{'loss': 0.0035, 'learning_rate': 2.4641857401613704e-05, 'epoch': 1.63}\n","{'loss': 0.0007, 'learning_rate': 2.455952576980076e-05, 'epoch': 1.64}\n","{'loss': 0.0003, 'learning_rate': 2.4477194137987817e-05, 'epoch': 1.64}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2c6b09cfd1e74a09af8cfa182daf127f","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2829 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.008598806336522102, 'eval_recall': 0.9781704781704782, 'eval_precision': 0.5940656565656566, 'eval_f1': 0.9544355153312006, 'eval_runtime': 37.9449, 'eval_samples_per_second': 74.555, 'eval_steps_per_second': 74.555, 'epoch': 1.64}\n","{'loss': 0.0025, 'learning_rate': 2.4394862506174873e-05, 'epoch': 1.65}\n","{'loss': 0.0529, 'learning_rate': 2.431253087436193e-05, 'epoch': 1.65}\n","{'loss': 0.0255, 'learning_rate': 2.4230199242548987e-05, 'epoch': 1.66}\n","{'loss': 0.002, 'learning_rate': 2.4147867610736047e-05, 'epoch': 1.66}\n","{'loss': 0.0025, 'learning_rate': 2.4065535978923103e-05, 'epoch': 1.67}\n","{'loss': 0.0026, 'learning_rate': 2.398320434711016e-05, 'epoch': 1.67}\n","{'loss': 0.0022, 'learning_rate': 2.3900872715297217e-05, 'epoch': 1.68}\n","{'loss': 0.0091, 'learning_rate': 2.3818541083484273e-05, 'epoch': 1.68}\n","{'loss': 0.0034, 'learning_rate': 2.3736209451671333e-05, 'epoch': 1.68}\n","{'loss': 0.0009, 'learning_rate': 2.365387781985839e-05, 'epoch': 1.69}\n","{'loss': 0.0021, 'learning_rate': 2.357154618804545e-05, 'epoch': 1.69}\n","{'loss': 0.0003, 'learning_rate': 2.3489214556232507e-05, 'epoch': 1.7}\n","{'loss': 0.0005, 'learning_rate': 2.3406882924419563e-05, 'epoch': 1.7}\n","{'loss': 0.0008, 'learning_rate': 2.332455129260662e-05, 'epoch': 1.71}\n","{'loss': 0.0026, 'learning_rate': 2.324221966079368e-05, 'epoch': 1.71}\n","{'loss': 0.0016, 'learning_rate': 2.3159888028980736e-05, 'epoch': 1.72}\n","{'loss': 0.0061, 'learning_rate': 2.3077556397167793e-05, 'epoch': 1.72}\n","{'loss': 0.002, 'learning_rate': 2.299522476535485e-05, 'epoch': 1.73}\n","{'loss': 0.0033, 'learning_rate': 2.2912893133541906e-05, 'epoch': 1.73}\n","{'loss': 0.0267, 'learning_rate': 2.2830561501728963e-05, 'epoch': 1.73}\n","{'loss': 0.0316, 'learning_rate': 2.2748229869916023e-05, 'epoch': 1.74}\n","{'loss': 0.0002, 'learning_rate': 2.266589823810308e-05, 'epoch': 1.74}\n","{'loss': 0.0003, 'learning_rate': 2.258356660629014e-05, 'epoch': 1.75}\n","{'loss': 0.0039, 'learning_rate': 2.2501234974477196e-05, 'epoch': 1.75}\n","{'loss': 0.0006, 'learning_rate': 2.2418903342664253e-05, 'epoch': 1.76}\n","{'loss': 0.0164, 'learning_rate': 2.233657171085131e-05, 'epoch': 1.76}\n","{'loss': 0.0004, 'learning_rate': 2.225424007903837e-05, 'epoch': 1.77}\n","{'loss': 0.0028, 'learning_rate': 2.2171908447225426e-05, 'epoch': 1.77}\n","{'loss': 0.001, 'learning_rate': 2.2089576815412483e-05, 'epoch': 1.78}\n","{'loss': 0.002, 'learning_rate': 2.200724518359954e-05, 'epoch': 1.78}\n","{'loss': 0.0109, 'learning_rate': 2.1924913551786596e-05, 'epoch': 1.78}\n","{'loss': 0.0008, 'learning_rate': 2.1842581919973653e-05, 'epoch': 1.79}\n","{'loss': 0.069, 'learning_rate': 2.1760250288160713e-05, 'epoch': 1.79}\n","{'loss': 0.0011, 'learning_rate': 2.167791865634777e-05, 'epoch': 1.8}\n","{'loss': 0.0009, 'learning_rate': 2.1595587024534826e-05, 'epoch': 1.8}\n","{'loss': 0.0001, 'learning_rate': 2.1513255392721886e-05, 'epoch': 1.81}\n","{'loss': 0.0004, 'learning_rate': 2.1430923760908942e-05, 'epoch': 1.81}\n","{'loss': 0.0005, 'learning_rate': 2.1348592129096e-05, 'epoch': 1.82}\n","{'loss': 0.1265, 'learning_rate': 2.126626049728306e-05, 'epoch': 1.82}\n","{'loss': 0.001, 'learning_rate': 2.1183928865470116e-05, 'epoch': 1.83}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5582753d031c4774bf7ccbc3b75f9307","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2829 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.013387829065322876, 'eval_recall': 0.9636174636174636, 'eval_precision': 0.5904458598726114, 'eval_f1': 0.9407494145199062, 'eval_runtime': 37.8364, 'eval_samples_per_second': 74.769, 'eval_steps_per_second': 74.769, 'epoch': 1.83}\n","{'loss': 0.003, 'learning_rate': 2.1101597233657172e-05, 'epoch': 1.83}\n","{'loss': 0.0108, 'learning_rate': 2.101926560184423e-05, 'epoch': 1.83}\n","{'loss': 0.0009, 'learning_rate': 2.0936933970031286e-05, 'epoch': 1.84}\n","{'loss': 0.0006, 'learning_rate': 2.0854602338218346e-05, 'epoch': 1.84}\n","{'loss': 0.0035, 'learning_rate': 2.0772270706405402e-05, 'epoch': 1.85}\n","{'loss': 0.0016, 'learning_rate': 2.068993907459246e-05, 'epoch': 1.85}\n","{'loss': 0.0074, 'learning_rate': 2.0607607442779515e-05, 'epoch': 1.86}\n","{'loss': 0.0012, 'learning_rate': 2.0525275810966575e-05, 'epoch': 1.86}\n","{'loss': 0.0018, 'learning_rate': 2.0442944179153632e-05, 'epoch': 1.87}\n","{'loss': 0.0006, 'learning_rate': 2.0360612547340692e-05, 'epoch': 1.87}\n","{'loss': 0.0005, 'learning_rate': 2.027828091552775e-05, 'epoch': 1.88}\n","{'loss': 0.0021, 'learning_rate': 2.0195949283714805e-05, 'epoch': 1.88}\n","{'loss': 0.0034, 'learning_rate': 2.0113617651901862e-05, 'epoch': 1.88}\n","{'loss': 0.0011, 'learning_rate': 2.003128602008892e-05, 'epoch': 1.89}\n","{'loss': 0.0003, 'learning_rate': 1.9948954388275975e-05, 'epoch': 1.89}\n","{'loss': 0.0006, 'learning_rate': 1.9866622756463035e-05, 'epoch': 1.9}\n","{'loss': 0.1084, 'learning_rate': 1.9784291124650092e-05, 'epoch': 1.9}\n","{'loss': 0.0004, 'learning_rate': 1.970195949283715e-05, 'epoch': 1.91}\n","{'loss': 0.0018, 'learning_rate': 1.9619627861024205e-05, 'epoch': 1.91}\n","{'loss': 0.0003, 'learning_rate': 1.9537296229211262e-05, 'epoch': 1.92}\n","{'loss': 0.0006, 'learning_rate': 1.9454964597398322e-05, 'epoch': 1.92}\n","{'loss': 0.0005, 'learning_rate': 1.937263296558538e-05, 'epoch': 1.93}\n","{'loss': 0.035, 'learning_rate': 1.929030133377244e-05, 'epoch': 1.93}\n","{'loss': 0.0061, 'learning_rate': 1.9207969701959495e-05, 'epoch': 1.94}\n","{'loss': 0.0031, 'learning_rate': 1.912563807014655e-05, 'epoch': 1.94}\n","{'loss': 0.1067, 'learning_rate': 1.9043306438333608e-05, 'epoch': 1.94}\n","{'loss': 0.001, 'learning_rate': 1.8960974806520665e-05, 'epoch': 1.95}\n","{'loss': 0.0037, 'learning_rate': 1.8878643174707725e-05, 'epoch': 1.95}\n","{'loss': 0.001, 'learning_rate': 1.879631154289478e-05, 'epoch': 1.96}\n","{'loss': 0.0001, 'learning_rate': 1.8713979911081838e-05, 'epoch': 1.96}\n","{'loss': 0.0003, 'learning_rate': 1.8631648279268895e-05, 'epoch': 1.97}\n","{'loss': 0.0179, 'learning_rate': 1.854931664745595e-05, 'epoch': 1.97}\n","{'loss': 0.0001, 'learning_rate': 1.8466985015643008e-05, 'epoch': 1.98}\n","{'loss': 0.0013, 'learning_rate': 1.8384653383830068e-05, 'epoch': 1.98}\n","{'loss': 0.0007, 'learning_rate': 1.8302321752017128e-05, 'epoch': 1.99}\n","{'loss': 0.0778, 'learning_rate': 1.8219990120204185e-05, 'epoch': 1.99}\n","{'loss': 0.0442, 'learning_rate': 1.813765848839124e-05, 'epoch': 1.99}\n","{'loss': 0.002, 'learning_rate': 1.8055326856578298e-05, 'epoch': 2.0}\n","{'loss': 0.0007, 'learning_rate': 1.7972995224765355e-05, 'epoch': 2.0}\n","{'loss': 0.0004, 'learning_rate': 1.7890663592952415e-05, 'epoch': 2.01}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a51a03e887ba40ceaad61ac0c47e442a","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2829 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.019537225365638733, 'eval_recall': 0.9386694386694386, 'eval_precision': 0.7043681747269891, 'eval_f1': 0.9268119374703933, 'eval_runtime': 37.6017, 'eval_samples_per_second': 75.236, 'eval_steps_per_second': 75.236, 'epoch': 2.01}\n","{'loss': 0.0013, 'learning_rate': 1.780833196113947e-05, 'epoch': 2.01}\n","{'loss': 0.0011, 'learning_rate': 1.7726000329326528e-05, 'epoch': 2.02}\n","{'loss': 0.0001, 'learning_rate': 1.7643668697513584e-05, 'epoch': 2.02}\n","{'loss': 0.0003, 'learning_rate': 1.756133706570064e-05, 'epoch': 2.03}\n","{'loss': 0.0007, 'learning_rate': 1.74790054338877e-05, 'epoch': 2.03}\n","{'loss': 0.0006, 'learning_rate': 1.7396673802074758e-05, 'epoch': 2.04}\n","{'loss': 0.0008, 'learning_rate': 1.7314342170261814e-05, 'epoch': 2.04}\n","{'loss': 0.0005, 'learning_rate': 1.7232010538448874e-05, 'epoch': 2.04}\n","{'loss': 0.0107, 'learning_rate': 1.714967890663593e-05, 'epoch': 2.05}\n","{'loss': 0.0024, 'learning_rate': 1.7067347274822988e-05, 'epoch': 2.05}\n","{'loss': 0.0025, 'learning_rate': 1.6985015643010048e-05, 'epoch': 2.06}\n","{'loss': 0.0005, 'learning_rate': 1.6902684011197104e-05, 'epoch': 2.06}\n","{'loss': 0.0005, 'learning_rate': 1.682035237938416e-05, 'epoch': 2.07}\n","{'loss': 0.0005, 'learning_rate': 1.6738020747571217e-05, 'epoch': 2.07}\n","{'loss': 0.0007, 'learning_rate': 1.6655689115758274e-05, 'epoch': 2.08}\n","{'loss': 0.0008, 'learning_rate': 1.657335748394533e-05, 'epoch': 2.08}\n","{'loss': 0.0056, 'learning_rate': 1.649102585213239e-05, 'epoch': 2.09}\n","{'loss': 0.0003, 'learning_rate': 1.6408694220319447e-05, 'epoch': 2.09}\n","{'loss': 0.0015, 'learning_rate': 1.6326362588506504e-05, 'epoch': 2.09}\n","{'loss': 0.0002, 'learning_rate': 1.624403095669356e-05, 'epoch': 2.1}\n","{'loss': 0.0002, 'learning_rate': 1.616169932488062e-05, 'epoch': 2.1}\n","{'loss': 0.0084, 'learning_rate': 1.6079367693067677e-05, 'epoch': 2.11}\n","{'loss': 0.0005, 'learning_rate': 1.5997036061254737e-05, 'epoch': 2.11}\n","{'loss': 0.001, 'learning_rate': 1.5914704429441794e-05, 'epoch': 2.12}\n","{'loss': 0.0005, 'learning_rate': 1.583237279762885e-05, 'epoch': 2.12}\n","{'loss': 0.0124, 'learning_rate': 1.5750041165815907e-05, 'epoch': 2.13}\n","{'loss': 0.0009, 'learning_rate': 1.5667709534002964e-05, 'epoch': 2.13}\n","{'loss': 0.0591, 'learning_rate': 1.558537790219002e-05, 'epoch': 2.14}\n","{'loss': 0.0006, 'learning_rate': 1.550304627037708e-05, 'epoch': 2.14}\n","{'loss': 0.0007, 'learning_rate': 1.5420714638564137e-05, 'epoch': 2.15}\n","{'loss': 0.0003, 'learning_rate': 1.5338383006751194e-05, 'epoch': 2.15}\n","{'loss': 0.0073, 'learning_rate': 1.5256051374938252e-05, 'epoch': 2.15}\n","{'loss': 0.002, 'learning_rate': 1.5173719743125309e-05, 'epoch': 2.16}\n","{'loss': 0.0018, 'learning_rate': 1.5091388111312365e-05, 'epoch': 2.16}\n","{'loss': 0.0008, 'learning_rate': 1.5009056479499425e-05, 'epoch': 2.17}\n","{'loss': 0.0002, 'learning_rate': 1.4926724847686482e-05, 'epoch': 2.17}\n","{'loss': 0.0009, 'learning_rate': 1.484439321587354e-05, 'epoch': 2.18}\n","{'loss': 0.0058, 'learning_rate': 1.4762061584060597e-05, 'epoch': 2.18}\n","{'loss': 0.0005, 'learning_rate': 1.4679729952247653e-05, 'epoch': 2.19}\n","{'loss': 0.0007, 'learning_rate': 1.459739832043471e-05, 'epoch': 2.19}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cdd14b8e91284fa996a5145991d8a7de","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2829 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.02663557231426239, 'eval_recall': 0.761954261954262, 'eval_precision': 0.7207472959685349, 'eval_f1': 0.760282443052619, 'eval_runtime': 37.7457, 'eval_samples_per_second': 74.949, 'eval_steps_per_second': 74.949, 'epoch': 2.19}\n","{'loss': 0.0003, 'learning_rate': 1.451506668862177e-05, 'epoch': 2.2}\n","{'loss': 0.3553, 'learning_rate': 1.4432735056808827e-05, 'epoch': 2.2}\n","{'loss': 0.0009, 'learning_rate': 1.4350403424995883e-05, 'epoch': 2.2}\n","{'loss': 0.0064, 'learning_rate': 1.4268071793182942e-05, 'epoch': 2.21}\n","{'loss': 0.0116, 'learning_rate': 1.4185740161369998e-05, 'epoch': 2.21}\n","{'loss': 0.0008, 'learning_rate': 1.4103408529557058e-05, 'epoch': 2.22}\n","{'loss': 0.0004, 'learning_rate': 1.4021076897744115e-05, 'epoch': 2.22}\n","{'loss': 0.0011, 'learning_rate': 1.3938745265931172e-05, 'epoch': 2.23}\n","{'loss': 0.0002, 'learning_rate': 1.3856413634118228e-05, 'epoch': 2.23}\n","{'loss': 0.0161, 'learning_rate': 1.3774082002305286e-05, 'epoch': 2.24}\n","{'loss': 0.0005, 'learning_rate': 1.3691750370492343e-05, 'epoch': 2.24}\n","{'loss': 0.0006, 'learning_rate': 1.3609418738679403e-05, 'epoch': 2.25}\n","{'loss': 0.0006, 'learning_rate': 1.352708710686646e-05, 'epoch': 2.25}\n","{'loss': 0.0008, 'learning_rate': 1.3444755475053516e-05, 'epoch': 2.25}\n","{'loss': 0.0009, 'learning_rate': 1.3362423843240573e-05, 'epoch': 2.26}\n","{'loss': 0.0005, 'learning_rate': 1.3280092211427631e-05, 'epoch': 2.26}\n","{'loss': 0.0269, 'learning_rate': 1.3197760579614688e-05, 'epoch': 2.27}\n","{'loss': 0.0004, 'learning_rate': 1.3115428947801748e-05, 'epoch': 2.27}\n","{'loss': 0.0003, 'learning_rate': 1.3033097315988805e-05, 'epoch': 2.28}\n","{'loss': 0.0035, 'learning_rate': 1.2950765684175861e-05, 'epoch': 2.28}\n","{'loss': 0.0011, 'learning_rate': 1.2868434052362918e-05, 'epoch': 2.29}\n","{'loss': 0.007, 'learning_rate': 1.2786102420549974e-05, 'epoch': 2.29}\n","{'loss': 0.0013, 'learning_rate': 1.2703770788737033e-05, 'epoch': 2.3}\n","{'loss': 0.0006, 'learning_rate': 1.2621439156924093e-05, 'epoch': 2.3}\n","{'loss': 0.0057, 'learning_rate': 1.253910752511115e-05, 'epoch': 2.3}\n","{'loss': 0.0003, 'learning_rate': 1.2456775893298206e-05, 'epoch': 2.31}\n","{'loss': 0.0004, 'learning_rate': 1.2374444261485263e-05, 'epoch': 2.31}\n","{'loss': 0.0007, 'learning_rate': 1.2292112629672321e-05, 'epoch': 2.32}\n","{'loss': 0.0003, 'learning_rate': 1.2209780997859378e-05, 'epoch': 2.32}\n","{'loss': 0.001, 'learning_rate': 1.2127449366046436e-05, 'epoch': 2.33}\n","{'loss': 0.002, 'learning_rate': 1.2045117734233494e-05, 'epoch': 2.33}\n","{'loss': 0.0002, 'learning_rate': 1.196278610242055e-05, 'epoch': 2.34}\n","{'loss': 0.0002, 'learning_rate': 1.1880454470607607e-05, 'epoch': 2.34}\n","{'loss': 0.0026, 'learning_rate': 1.1798122838794666e-05, 'epoch': 2.35}\n","{'loss': 0.0003, 'learning_rate': 1.1715791206981722e-05, 'epoch': 2.35}\n","{'loss': 0.0002, 'learning_rate': 1.163345957516878e-05, 'epoch': 2.36}\n","{'loss': 0.0002, 'learning_rate': 1.1551127943355839e-05, 'epoch': 2.36}\n","{'loss': 0.0007, 'learning_rate': 1.1468796311542896e-05, 'epoch': 2.36}\n","{'loss': 0.0004, 'learning_rate': 1.1386464679729952e-05, 'epoch': 2.37}\n","{'loss': 0.0008, 'learning_rate': 1.130413304791701e-05, 'epoch': 2.37}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"678026da78174580847fa24d114cb1cb","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2829 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.01577676460146904, 'eval_recall': 0.8367983367983368, 'eval_precision': 0.6874466268146883, 'eval_f1': 0.8298640022203719, 'eval_runtime': 37.6374, 'eval_samples_per_second': 75.165, 'eval_steps_per_second': 75.165, 'epoch': 2.37}\n","{'loss': 0.0005, 'learning_rate': 1.1221801416104067e-05, 'epoch': 2.38}\n","{'loss': 0.0003, 'learning_rate': 1.1139469784291124e-05, 'epoch': 2.38}\n","{'loss': 0.0003, 'learning_rate': 1.1057138152478184e-05, 'epoch': 2.39}\n","{'loss': 0.0002, 'learning_rate': 1.097480652066524e-05, 'epoch': 2.39}\n","{'loss': 0.0003, 'learning_rate': 1.0892474888852297e-05, 'epoch': 2.4}\n","{'loss': 0.0676, 'learning_rate': 1.0810143257039355e-05, 'epoch': 2.4}\n","{'loss': 0.0002, 'learning_rate': 1.0727811625226412e-05, 'epoch': 2.41}\n","{'loss': 0.0513, 'learning_rate': 1.0645479993413469e-05, 'epoch': 2.41}\n","{'loss': 0.0009, 'learning_rate': 1.0563148361600527e-05, 'epoch': 2.41}\n","{'loss': 0.0025, 'learning_rate': 1.0480816729787585e-05, 'epoch': 2.42}\n","{'loss': 0.0003, 'learning_rate': 1.0398485097974642e-05, 'epoch': 2.42}\n","{'loss': 0.0004, 'learning_rate': 1.03161534661617e-05, 'epoch': 2.43}\n","{'loss': 0.0593, 'learning_rate': 1.0233821834348757e-05, 'epoch': 2.43}\n","{'loss': 0.0002, 'learning_rate': 1.0151490202535814e-05, 'epoch': 2.44}\n","{'loss': 0.0001, 'learning_rate': 1.0069158570722872e-05, 'epoch': 2.44}\n","{'loss': 0.0043, 'learning_rate': 9.98682693890993e-06, 'epoch': 2.45}\n","{'loss': 0.0002, 'learning_rate': 9.904495307096987e-06, 'epoch': 2.45}\n","{'loss': 0.0003, 'learning_rate': 9.822163675284045e-06, 'epoch': 2.46}\n","{'loss': 0.0004, 'learning_rate': 9.739832043471102e-06, 'epoch': 2.46}\n","{'loss': 0.001, 'learning_rate': 9.65750041165816e-06, 'epoch': 2.46}\n","{'loss': 0.0002, 'learning_rate': 9.575168779845217e-06, 'epoch': 2.47}\n","{'loss': 0.0054, 'learning_rate': 9.492837148032275e-06, 'epoch': 2.47}\n","{'loss': 0.001, 'learning_rate': 9.410505516219333e-06, 'epoch': 2.48}\n","{'loss': 0.0008, 'learning_rate': 9.32817388440639e-06, 'epoch': 2.48}\n","{'loss': 0.0025, 'learning_rate': 9.245842252593447e-06, 'epoch': 2.49}\n","{'loss': 0.0309, 'learning_rate': 9.163510620780505e-06, 'epoch': 2.49}\n","{'loss': 0.0006, 'learning_rate': 9.081178988967561e-06, 'epoch': 2.5}\n","{'loss': 0.0078, 'learning_rate': 8.998847357154618e-06, 'epoch': 2.5}\n","{'loss': 0.0132, 'learning_rate': 8.916515725341678e-06, 'epoch': 2.51}\n","{'loss': 0.0037, 'learning_rate': 8.834184093528735e-06, 'epoch': 2.51}\n","{'loss': 0.0013, 'learning_rate': 8.751852461715791e-06, 'epoch': 2.51}\n","{'loss': 0.0014, 'learning_rate': 8.66952082990285e-06, 'epoch': 2.52}\n","{'loss': 0.0012, 'learning_rate': 8.587189198089906e-06, 'epoch': 2.52}\n","{'loss': 0.0002, 'learning_rate': 8.504857566276963e-06, 'epoch': 2.53}\n","{'loss': 0.0312, 'learning_rate': 8.422525934464021e-06, 'epoch': 2.53}\n","{'loss': 0.001, 'learning_rate': 8.34019430265108e-06, 'epoch': 2.54}\n","{'loss': 0.0011, 'learning_rate': 8.257862670838136e-06, 'epoch': 2.54}\n","{'loss': 0.0015, 'learning_rate': 8.175531039025195e-06, 'epoch': 2.55}\n","{'loss': 0.001, 'learning_rate': 8.093199407212251e-06, 'epoch': 2.55}\n","{'loss': 0.0005, 'learning_rate': 8.010867775399308e-06, 'epoch': 2.56}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"89b1224246ba443092c41285add6960b","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2829 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.011466246098279953, 'eval_recall': 0.9781704781704782, 'eval_precision': 0.6353814989871708, 'eval_f1': 0.9582860052485213, 'eval_runtime': 37.4538, 'eval_samples_per_second': 75.533, 'eval_steps_per_second': 75.533, 'epoch': 2.56}\n","{'loss': 0.0013, 'learning_rate': 7.928536143586366e-06, 'epoch': 2.56}\n","{'loss': 0.0005, 'learning_rate': 7.846204511773424e-06, 'epoch': 2.57}\n","{'loss': 0.0019, 'learning_rate': 7.763872879960481e-06, 'epoch': 2.57}\n","{'loss': 0.0006, 'learning_rate': 7.68154124814754e-06, 'epoch': 2.57}\n","{'loss': 0.0366, 'learning_rate': 7.599209616334596e-06, 'epoch': 2.58}\n","{'loss': 0.0005, 'learning_rate': 7.5168779845216534e-06, 'epoch': 2.58}\n","{'loss': 0.0001, 'learning_rate': 7.434546352708712e-06, 'epoch': 2.59}\n","{'loss': 0.0006, 'learning_rate': 7.352214720895768e-06, 'epoch': 2.59}\n","{'loss': 0.0061, 'learning_rate': 7.269883089082825e-06, 'epoch': 2.6}\n","{'loss': 0.02, 'learning_rate': 7.187551457269884e-06, 'epoch': 2.6}\n","{'loss': 0.0003, 'learning_rate': 7.105219825456941e-06, 'epoch': 2.61}\n","{'loss': 0.0016, 'learning_rate': 7.0228881936439974e-06, 'epoch': 2.61}\n","{'loss': 0.0005, 'learning_rate': 6.940556561831056e-06, 'epoch': 2.62}\n","{'loss': 0.001, 'learning_rate': 6.858224930018113e-06, 'epoch': 2.62}\n","{'loss': 0.0004, 'learning_rate': 6.77589329820517e-06, 'epoch': 2.62}\n","{'loss': 0.0001, 'learning_rate': 6.693561666392228e-06, 'epoch': 2.63}\n","{'loss': 0.0002, 'learning_rate': 6.611230034579286e-06, 'epoch': 2.63}\n","{'loss': 0.0007, 'learning_rate': 6.528898402766342e-06, 'epoch': 2.64}\n","{'loss': 0.0006, 'learning_rate': 6.4465667709534006e-06, 'epoch': 2.64}\n","{'loss': 0.0005, 'learning_rate': 6.364235139140458e-06, 'epoch': 2.65}\n","{'loss': 0.0005, 'learning_rate': 6.281903507327516e-06, 'epoch': 2.65}\n","{'loss': 0.0051, 'learning_rate': 6.199571875514573e-06, 'epoch': 2.66}\n","{'loss': 0.0036, 'learning_rate': 6.1172402437016305e-06, 'epoch': 2.66}\n","{'loss': 0.0007, 'learning_rate': 6.034908611888688e-06, 'epoch': 2.67}\n","{'loss': 0.0001, 'learning_rate': 5.952576980075745e-06, 'epoch': 2.67}\n","{'loss': 0.0003, 'learning_rate': 5.870245348262803e-06, 'epoch': 2.67}\n","{'loss': 0.0012, 'learning_rate': 5.78791371644986e-06, 'epoch': 2.68}\n","{'loss': 0.0002, 'learning_rate': 5.705582084636918e-06, 'epoch': 2.68}\n","{'loss': 0.0012, 'learning_rate': 5.623250452823975e-06, 'epoch': 2.69}\n","{'loss': 0.0002, 'learning_rate': 5.540918821011033e-06, 'epoch': 2.69}\n","{'loss': 0.0007, 'learning_rate': 5.45858718919809e-06, 'epoch': 2.7}\n","{'loss': 0.0028, 'learning_rate': 5.376255557385148e-06, 'epoch': 2.7}\n","{'loss': 0.045, 'learning_rate': 5.293923925572205e-06, 'epoch': 2.71}\n","{'loss': 0.0305, 'learning_rate': 5.211592293759263e-06, 'epoch': 2.71}\n","{'loss': 0.0002, 'learning_rate': 5.12926066194632e-06, 'epoch': 2.72}\n","{'loss': 0.0004, 'learning_rate': 5.046929030133378e-06, 'epoch': 2.72}\n","{'loss': 0.0008, 'learning_rate': 4.964597398320435e-06, 'epoch': 2.72}\n","{'loss': 0.0024, 'learning_rate': 4.8822657665074925e-06, 'epoch': 2.73}\n","{'loss': 0.0003, 'learning_rate': 4.79993413469455e-06, 'epoch': 2.73}\n","{'loss': 0.0019, 'learning_rate': 4.7176025028816075e-06, 'epoch': 2.74}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6e370e4000c24ebcbf0ef969cf82a948","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2829 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.010693891905248165, 'eval_recall': 0.9656964656964657, 'eval_precision': 0.6785975164353543, 'eval_f1': 0.9502340768716315, 'eval_runtime': 37.5196, 'eval_samples_per_second': 75.401, 'eval_steps_per_second': 75.401, 'epoch': 2.74}\n","{'loss': 0.0013, 'learning_rate': 4.635270871068665e-06, 'epoch': 2.74}\n","{'loss': 0.0001, 'learning_rate': 4.5529392392557216e-06, 'epoch': 2.75}\n","{'loss': 0.0035, 'learning_rate': 4.47060760744278e-06, 'epoch': 2.75}\n","{'loss': 0.0014, 'learning_rate': 4.388275975629837e-06, 'epoch': 2.76}\n","{'loss': 0.0012, 'learning_rate': 4.305944343816894e-06, 'epoch': 2.76}\n","{'loss': 0.0001, 'learning_rate': 4.223612712003952e-06, 'epoch': 2.77}\n","{'loss': 0.0012, 'learning_rate': 4.14128108019101e-06, 'epoch': 2.77}\n","{'loss': 0.0094, 'learning_rate': 4.058949448378066e-06, 'epoch': 2.77}\n","{'loss': 0.0004, 'learning_rate': 3.976617816565125e-06, 'epoch': 2.78}\n","{'loss': 0.0007, 'learning_rate': 3.894286184752182e-06, 'epoch': 2.78}\n","{'loss': 0.0013, 'learning_rate': 3.8119545529392397e-06, 'epoch': 2.79}\n","{'loss': 0.0003, 'learning_rate': 3.7296229211262967e-06, 'epoch': 2.79}\n","{'loss': 0.0008, 'learning_rate': 3.6472912893133546e-06, 'epoch': 2.8}\n","{'loss': 0.0001, 'learning_rate': 3.564959657500412e-06, 'epoch': 2.8}\n","{'loss': 0.0005, 'learning_rate': 3.482628025687469e-06, 'epoch': 2.81}\n","{'loss': 0.0004, 'learning_rate': 3.4002963938745266e-06, 'epoch': 2.81}\n","{'loss': 0.0014, 'learning_rate': 3.3179647620615845e-06, 'epoch': 2.82}\n","{'loss': 0.0001, 'learning_rate': 3.2356331302486415e-06, 'epoch': 2.82}\n","{'loss': 0.0029, 'learning_rate': 3.153301498435699e-06, 'epoch': 2.83}\n","{'loss': 0.0002, 'learning_rate': 3.0709698666227565e-06, 'epoch': 2.83}\n","{'loss': 0.0003, 'learning_rate': 2.988638234809814e-06, 'epoch': 2.83}\n","{'loss': 0.0001, 'learning_rate': 2.9063066029968714e-06, 'epoch': 2.84}\n","{'loss': 0.0021, 'learning_rate': 2.823974971183929e-06, 'epoch': 2.84}\n","{'loss': 0.0002, 'learning_rate': 2.7416433393709864e-06, 'epoch': 2.85}\n","{'loss': 0.0007, 'learning_rate': 2.659311707558044e-06, 'epoch': 2.85}\n","{'loss': 0.0001, 'learning_rate': 2.5769800757451017e-06, 'epoch': 2.86}\n","{'loss': 0.0004, 'learning_rate': 2.4946484439321588e-06, 'epoch': 2.86}\n","{'loss': 0.0003, 'learning_rate': 2.4123168121192163e-06, 'epoch': 2.87}\n","{'loss': 0.0003, 'learning_rate': 2.3299851803062737e-06, 'epoch': 2.87}\n","{'loss': 0.0004, 'learning_rate': 2.247653548493331e-06, 'epoch': 2.88}\n","{'loss': 0.0011, 'learning_rate': 2.1653219166803887e-06, 'epoch': 2.88}\n","{'loss': 0.0004, 'learning_rate': 2.082990284867446e-06, 'epoch': 2.88}\n","{'loss': 0.0002, 'learning_rate': 2.0006586530545036e-06, 'epoch': 2.89}\n","{'loss': 0.0031, 'learning_rate': 1.918327021241561e-06, 'epoch': 2.89}\n","{'loss': 0.0001, 'learning_rate': 1.8359953894286186e-06, 'epoch': 2.9}\n","{'loss': 0.0005, 'learning_rate': 1.753663757615676e-06, 'epoch': 2.9}\n","{'loss': 0.0002, 'learning_rate': 1.6713321258027337e-06, 'epoch': 2.91}\n","{'loss': 0.002, 'learning_rate': 1.589000493989791e-06, 'epoch': 2.91}\n","{'loss': 0.0027, 'learning_rate': 1.5066688621768484e-06, 'epoch': 2.92}\n","{'loss': 0.0018, 'learning_rate': 1.4243372303639057e-06, 'epoch': 2.92}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3939f14429094ab09883f5fdb2ced0ee","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2829 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.011813093908131123, 'eval_recall': 0.9604989604989606, 'eval_precision': 0.6978851963746223, 'eval_f1': 0.9467959328446443, 'eval_runtime': 35.783, 'eval_samples_per_second': 79.06, 'eval_steps_per_second': 79.06, 'epoch': 2.92}\n","{'loss': 0.0002, 'learning_rate': 1.3420055985509634e-06, 'epoch': 2.93}\n","{'loss': 0.0003, 'learning_rate': 1.2596739667380209e-06, 'epoch': 2.93}\n","{'loss': 0.0002, 'learning_rate': 1.1773423349250783e-06, 'epoch': 2.93}\n","{'loss': 0.0004, 'learning_rate': 1.0950107031121356e-06, 'epoch': 2.94}\n","{'loss': 0.0057, 'learning_rate': 1.0126790712991933e-06, 'epoch': 2.94}\n","{'loss': 0.0002, 'learning_rate': 9.303474394862506e-07, 'epoch': 2.95}\n","{'loss': 0.0015, 'learning_rate': 8.480158076733082e-07, 'epoch': 2.95}\n","{'loss': 0.0004, 'learning_rate': 7.656841758603656e-07, 'epoch': 2.96}\n","{'loss': 0.0003, 'learning_rate': 6.83352544047423e-07, 'epoch': 2.96}\n","{'loss': 0.0001, 'learning_rate': 6.010209122344805e-07, 'epoch': 2.97}\n","{'loss': 0.0009, 'learning_rate': 5.18689280421538e-07, 'epoch': 2.97}\n","{'loss': 0.0018, 'learning_rate': 4.3635764860859546e-07, 'epoch': 2.98}\n","{'loss': 0.0001, 'learning_rate': 3.540260167956529e-07, 'epoch': 2.98}\n","{'loss': 0.0001, 'learning_rate': 2.7169438498271035e-07, 'epoch': 2.98}\n","{'loss': 0.0003, 'learning_rate': 1.8936275316976783e-07, 'epoch': 2.99}\n","{'loss': 0.0046, 'learning_rate': 1.070311213568253e-07, 'epoch': 2.99}\n","{'loss': 0.0009, 'learning_rate': 2.469948954388276e-08, 'epoch': 3.0}\n","{'train_runtime': 2096.6867, 'train_samples_per_second': 25.08, 'train_steps_per_second': 3.135, 'train_loss': 0.07242961128891215, 'epoch': 3.0}\n","Shutting down background jobs, please wait a moment...\n","Done!\n","Waiting for the remaining 12 operations to synchronize with Neptune. Do not kill this process.\n","All 12 operations synced, thanks for waiting!\n","Explore the metadata in the Neptune app:\n","https://app.neptune.ai/bernd.heidemann/PII/e/PII-271/metadata\n"]},{"data":{"text/plain":["TrainOutput(global_step=6573, training_loss=0.07242961128891215, metrics={'train_runtime': 2096.6867, 'train_samples_per_second': 25.08, 'train_steps_per_second': 3.135, 'train_loss': 0.07242961128891215, 'epoch': 3.0})"]},"execution_count":53,"metadata":{},"output_type":"execute_result"}],"source":["import os\n","os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n","\n","\n","\n","model = AutoModelForTokenClassification.from_pretrained(\n","    parameter[\"model\"],\n","    num_labels=len(all_labels),\n","    id2label=id2label,\n","    label2id=label2id,\n","    ignore_mismatched_sizes=True\n",")\n","\n","if parameter['freeze_embeddings']:\n","    for param in model.deberta.embeddings.parameters():\n","        param.requires_grad = False\n","        \n","if parameter['freeze_layers'] > 0:\n","    for layer in model.deberta.encoder.layer[:parameter['freeze_layers']]:\n","        for param in layer.parameters():\n","            param.requires_grad = False\n","\n","#my_model=MyModel(parameter['model'], len(label2id))\n","# set torch seed\n","torch.manual_seed(189237)\n","trainer=get_trainer(model, train_ds, valid_ds)\n","trainer.train()"]},{"cell_type":"code","execution_count":54,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","c:\\Users\\Bernd\\anaconda3\\envs\\mytorch\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:470: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n","  warnings.warn(\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"name":"stdout","output_type":"stream","text":["    row_id  document  token           label\n","0        0         7      9  B-NAME_STUDENT\n","1        1         7     10  I-NAME_STUDENT\n","2        2         7    482  B-NAME_STUDENT\n","3        3         7    483  I-NAME_STUDENT\n","4        4         7    741  B-NAME_STUDENT\n","5        5         7    742  I-NAME_STUDENT\n","6        6        10      0  B-NAME_STUDENT\n","7        7        10      1  I-NAME_STUDENT\n","8        8        10    464  B-NAME_STUDENT\n","9        9        10    465  I-NAME_STUDENT\n","10      10        16      4  B-NAME_STUDENT\n","11      11        16      5  I-NAME_STUDENT\n","12      12        86      6  B-NAME_STUDENT\n","13      13        86      7  I-NAME_STUDENT\n","14      14        93      0  B-NAME_STUDENT\n","15      15        93      1  I-NAME_STUDENT\n","16      16       104      7  B-NAME_STUDENT\n","17      17       104      8  B-NAME_STUDENT\n","18      18       104      9  I-NAME_STUDENT\n","19      19       112      5  B-NAME_STUDENT\n","20      20       112      6  I-NAME_STUDENT\n"]}],"source":["create_submission(model, f\"submission.csv\")"]},{"cell_type":"code","execution_count":55,"metadata":{},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n","\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n","\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n","\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["# save trainer\n","trainer.save_model(\"final_model\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":7500999,"sourceId":66653,"sourceType":"competition"},{"datasetId":4319117,"sourceId":7429898,"sourceType":"datasetVersion"}],"dockerImageVersionId":30636,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
