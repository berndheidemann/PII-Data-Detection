{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TrainingArguments, Trainer, AutoModelForTokenClassification\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "from tokenizers import AddedToken\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "kaggle=False\n",
    "\n",
    "path=\"/kaggle/input/pii-detection-removal-from-educational-data\" if kaggle else \"data\"\n",
    "test_path = path + \"/test.json\"\n",
    "train_path = path + \"/train.json\"\n",
    "model_path = \"/kaggle/input/huggingfacedebertav3variants/deberta-v3-base\" if kaggle else \"results\\checkpoint-3600\"\n",
    "\n",
    "\n",
    "parameter= {\n",
    "    \"model\": model_path,\n",
    "    \"inference_max_length\": 2000,\n",
    "    \"inference_batch_size\": 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_inference(example, tokenizer, max_length):\n",
    "        text = []\n",
    "        for t,  ws in zip(example[\"tokens\"], example[\"trailing_whitespace\"]):\n",
    "            text.append(t)\n",
    "            if ws:\n",
    "                text.append(\" \")\n",
    "        tokenized = tokenizer(\"\".join(text), return_offsets_mapping=True, truncation=True, max_length=max_length, padding=\"max_length\")\n",
    "        text = \"\".join(text)\n",
    "        length = len(tokenized.input_ids)\n",
    "        return {\n",
    "            **tokenized,\n",
    "            \"length\": length,\n",
    "        }\n",
    "        \n",
    "class TestTokenizer():\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def preprocess(self, example):\n",
    "        # Preprocess the tokens and labels by adding trailing whitespace and labels\n",
    "        tokens = []\n",
    "        tokens_without_ws = []\n",
    "        token_map = [] # Use the index as labels\n",
    "        index = 0\n",
    "        for token, t_ws in zip(example[\"tokens\"], example[\"trailing_whitespace\"]):\n",
    "            tokens_without_ws.append(token)\n",
    "            tokens.append(token)\n",
    "            token_map.extend([index] * len(token))\n",
    "            # Added trailing whitespace and label if true and \n",
    "            if t_ws:\n",
    "                tokens.append(\" \")\n",
    "                token_map.append(-1)\n",
    "            index += 1\n",
    "        return tokens, token_map, tokens_without_ws\n",
    "    \n",
    "    def tokenize(self, example):\n",
    "        tokens, token_map, tokens_without_ws = self.preprocess(example)\n",
    "        text = \"\".join(tokens)\n",
    "        tokenized = self.tokenizer(text, return_offsets_mapping=True, padding=\"max_length\",\n",
    "                                   truncation=True, max_length=parameter[\"inference_max_length\"])\n",
    "        return {**tokenized, \"token_map\": token_map, \"tokens\": tokens, \"tokens_without_ws\": tokens_without_ws} \n",
    "\n",
    "class PiiDatasetInference(torch.utils.data.Dataset):\n",
    "        def __init__(self, dataset, tokenizer):\n",
    "            self.dataset = dataset\n",
    "            self.tokenizer=TestTokenizer(tokenizer)\n",
    "            \n",
    "        def __getitem__(self, idx):\n",
    "            vals=self.tokenizer.tokenize(self.dataset[idx])\n",
    "            input_ids = torch.tensor(vals[\"input_ids\"])\n",
    "            attention_mask = torch.tensor(vals[\"attention_mask\"])\n",
    "            document_id = self.dataset[idx][\"document\"]\n",
    "            return input_ids, attention_mask, document_id, vals\n",
    "        \n",
    "        def __len__(self):\n",
    "            return len(self.dataset)\n",
    "\n",
    "# Convert preds to a list of dictionaries\n",
    "def to_test_submission(preds=None, dataset=None, document_ids=None, id2label=None):\n",
    "    pairs = []\n",
    "    row_id = 0\n",
    "    results = []\n",
    "    \n",
    "    for i in range(len(preds)):\n",
    "        input_ids, attention_mask, document_id, vals = dataset[i]\n",
    "        token_map=vals[\"token_map\"]\n",
    "        offsets=vals[\"offset_mapping\"]\n",
    "        tokens=vals[\"tokens_without_ws\"]\n",
    "        #print(\"tokens\", tokens)\n",
    "        pred=preds[i]\n",
    "        #print(\"original_text\", original_text)\n",
    "        #print(\"token_map\", token_map)\n",
    "        #print(\"offsets\", offsets)   \n",
    "        #print(\"pred\", pred)\n",
    "\n",
    "\n",
    "        for token_pred, input_id, (start_idx, end_idx) in zip(pred, input_ids, offsets):\n",
    "            #print(\"\\nnow doing \", start_idx,  end_idx, token_pred)\n",
    "            if start_idx == 0 and end_idx == 0: # Skip 0 offset\n",
    "                continue\n",
    "            # Skip spaces \n",
    "            while start_idx < len(token_map):\n",
    "                #print(\"loop, start_idx now\", start_idx) \n",
    "                #print(\" tokens[token_map[start_idx]]: \", tokens[token_map[start_idx]] if not tokens[token_map[start_idx]].isspace() else \"WHITESPACE\")          \n",
    "                if token_map[start_idx] == -1: # Skip unknown tokens               \n",
    "                    start_idx += 1\n",
    "                elif tokens[token_map[start_idx]].isspace(): # Skip white space\n",
    "                    start_idx += 1\n",
    "                else:\n",
    "                    break\n",
    "            # Ensure start index < length\n",
    "            if start_idx < len(token_map):\n",
    "                token_id = token_map[start_idx]\n",
    "                #print(\"token_id\", token_id)\n",
    "                #token_id= input_id.item()\n",
    "                label_pred = id2label[token_pred.item()]\n",
    "                #print(\"label_pred\", label_pred)\n",
    "                # ignore \"O\" and whitespace preds\n",
    "                if label_pred != \"O\" and token_id != -1:\n",
    "                    #print(\"is PII\", token_id, label_pred)\n",
    "                    token_str = tokens[token_id]\n",
    "                    pair=(document_id, token_id)\n",
    "                    if pair not in pairs:\n",
    "                        results.append({\n",
    "                            \"row_id\": row_id, \n",
    "                            \"document\": document_id,\n",
    "                            \"token\": token_id, \n",
    "                            \"label\": label_pred,\n",
    "                            \"token_str\": token_str\n",
    "                        })\n",
    "                        pairs.append(pair)\n",
    "                        row_id += 1\n",
    "\n",
    "    # Create a dataframe \n",
    "    return results\n",
    "\n",
    "def create_submission(model, filename=\"submission.csv\"):\n",
    "    data = json.load(open(train_path))\n",
    "    from itertools import chain\n",
    "    all_labels = sorted(list(set(chain(*[x[\"labels\"] for x in data]))))\n",
    "    label2id = {l: i for i,l in enumerate(all_labels)}\n",
    "    id2label = {v:k for k,v in label2id.items()}\n",
    "\n",
    "    data=json.load(open(test_path))\n",
    "    tokenizer = AutoTokenizer.from_pretrained(parameter[\"model\"])\n",
    "    my_dataset=PiiDatasetInference(data, tokenizer)\n",
    "    loader=torch.utils.data.DataLoader(my_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.eval()\n",
    "    \n",
    "    # stack all predictions into tensor\n",
    "    all_preds = []\n",
    "\n",
    "    for id, attention_mask, document_ids, vals in loader:\n",
    "        id=id.to(device)\n",
    "        attention_mask=attention_mask.to(device)\n",
    "        preds=model(id, attention_mask).get('logits').argmax(dim=2)\n",
    "        all_preds.append(preds)\n",
    "        #for pred, id in zip(preds.flatten(), id.flatten()):\n",
    "        #    if pred != 12:\n",
    "                #print(f\"Document: {document_id.item()} TOKEN:{tokenizer.decode(id)}  --- pred:{id2label[pred.item()]}\")\n",
    "        #        output[row_id]={\"document\":document_id.item(), \"token\":id.item(), \"label\":id2label[pred.item()]}\n",
    "        #        row_id+=1\n",
    "        #for pred, id in zip(preds.flatten(), id.flatten()):\n",
    "        #    if pred != 12:\n",
    "        #        print(f\"TOKEN:{tokenizer.decode(id)}  --- pred:{id2label[pred.item()]}\")\n",
    "    \n",
    "   \n",
    "    all_preds = torch.cat(all_preds, dim=0)\n",
    "    \n",
    "    results = to_test_submission(preds=all_preds, dataset=my_dataset, document_ids=document_ids, id2label=id2label)\n",
    "    if len(results) == 0:\n",
    "        print(\"Error in create_submission(): No predictions made, probably because the model is not learning. Check the model and the data.\")\n",
    "        return\n",
    "    df = pd.DataFrame(results)\n",
    "    df=df[[\"row_id\", \"document\", \"token\", \"label\"]]\n",
    "    print(df)\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "#create_submission(MyModel(parameter['model'], len(label2id)).to(device), \"submission_just_dumb.csv\")\n",
    "# create_submission(model, \"submission.csv\")\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "import json\n",
    "\n",
    "data = json.load(open(train_path))\n",
    "all_labels = sorted(list(set(chain(*[x[\"labels\"] for x in data]))))\n",
    "label2id = {l: i for i,l in enumerate(all_labels)}\n",
    "id2label = {v:k for k,v in label2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "collator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=16)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    parameter[\"model\"],\n",
    "    num_labels=len(all_labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
