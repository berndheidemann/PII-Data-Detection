{"cells":[{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-01-25T21:37:35.578114Z","iopub.status.busy":"2024-01-25T21:37:35.577723Z","iopub.status.idle":"2024-01-25T21:37:35.587366Z","shell.execute_reply":"2024-01-25T21:37:35.5858Z","shell.execute_reply.started":"2024-01-25T21:37:35.578083Z"},"trusted":true},"outputs":[],"source":["\n","parameter= {\n","    \"model\": \"microsoft/deberta-v3-base\",\n","    \"max_length\": 512,\n","    \"batch_size\": 8,\n","    \"lr\": 1e-5,\n","    \"filter_no_pii_percent_allow\": 0.2,\n","}\n"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-01-25T21:37:35.590432Z","iopub.status.busy":"2024-01-25T21:37:35.589758Z","iopub.status.idle":"2024-01-25T21:37:35.604681Z","shell.execute_reply":"2024-01-25T21:37:35.603633Z","shell.execute_reply.started":"2024-01-25T21:37:35.590393Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["6807\n","dict_keys(['document', 'full_text', 'tokens', 'trailing_whitespace', 'labels'])\n","['Design', 'Thinking', 'for', 'innovation', 'reflexion', '-', 'Avril', '2021', '-', 'Nathalie']\n","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NAME_STUDENT']\n","[True, True, True, True, False, False, True, False, False, True]\n"]}],"source":["\n","import json\n","import numpy as np\n","\n","data = json.load(open(\"data/train.json\"))\n","\n","print(len(data))\n","print(data[0].keys())\n","\n","x = data[0]\n","\n","print(x[\"tokens\"][:10])\n","print(x[\"labels\"][:10])\n","print(x[\"trailing_whitespace\"][:10])"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"data":{"text/plain":["dict_keys(['document', 'full_text', 'tokens', 'trailing_whitespace', 'labels'])"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["x.keys()"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-01-25T21:37:35.606208Z","iopub.status.busy":"2024-01-25T21:37:35.605889Z","iopub.status.idle":"2024-01-25T21:37:35.62164Z","shell.execute_reply":"2024-01-25T21:37:35.620746Z","shell.execute_reply.started":"2024-01-25T21:37:35.606175Z"},"trusted":true},"outputs":[{"data":{"text/plain":["{0: 'B-EMAIL',\n"," 1: 'B-ID_NUM',\n"," 2: 'B-NAME_STUDENT',\n"," 3: 'B-PHONE_NUM',\n"," 4: 'B-STREET_ADDRESS',\n"," 5: 'B-URL_PERSONAL',\n"," 6: 'B-USERNAME',\n"," 7: 'I-ID_NUM',\n"," 8: 'I-NAME_STUDENT',\n"," 9: 'I-PHONE_NUM',\n"," 10: 'I-STREET_ADDRESS',\n"," 11: 'I-URL_PERSONAL',\n"," 12: 'O'}"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["\n","from itertools import chain\n","\n","all_labels = sorted(list(set(chain(*[x[\"labels\"] for x in data]))))\n","label2id = {l: i for i,l in enumerate(all_labels)}\n","id2label = {v:k for k,v in label2id.items()}\n","\n","id2label"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["import random\n","\n","def tokenize(example, tokenizer, label2id, max_length):\n","    text = []\n","    labels = []\n","    for t, l, ws in zip(example[\"tokens\"], example[\"labels\"], example[\"trailing_whitespace\"]):\n","        text.append(t)\n","        labels.extend([l]*len(t))\n","        if ws:\n","            text.append(\" \")\n","            labels.append(\"O\")\n","    \n","    tokenized = tokenizer(\"\".join(text), return_offsets_mapping=True, max_length=max_length, truncation=True, padding=\"max_length\")\n","    labels = np.array(labels)\n","    text = \"\".join(text)\n","    token_labels = []\n","    \n","    for start_idx, end_idx in tokenized.offset_mapping:\n","        # CLS token\n","        if start_idx == 0 and end_idx == 0: \n","            token_labels.append(label2id[\"O\"])\n","            continue\n","        \n","        # case when token starts with whitespace\n","        if text[start_idx].isspace():\n","            start_idx += 1\n","        \n","        while start_idx >= len(labels):\n","            start_idx -= 1\n","            \n","        token_labels.append(label2id[labels[start_idx]])\n","        \n","    length = len(tokenized.input_ids)\n","    return {\n","        **tokenized,\n","        \"labels\": token_labels,\n","        \"length\": length\n","    }\n","\n","# https://www.kaggle.com/competitions/pii-detection-removal-from-educational-data/discussion/468844\n","def filter_no_pii(example, percent_allow=parameter[\"filter_no_pii_percent_allow\"]):\n","    # Return True if there is PII\n","    # Or 20% of the time if there isn't\n","    has_pii = set(\"O\") != set(example[\"labels\"])\n","    return has_pii or (random.random() < percent_allow)\n","    \n","\n"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","c:\\Users\\Shadow\\anaconda3\\envs\\mytorch\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:470: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n","  warnings.warn(\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["import torch\n","import json\n","\n","class PiiDataset(torch.utils.data.Dataset):\n","    def __init__(self, dataset, tokenizer, label2id, max_length):\n","        self.dataset = dataset\n","        self.tokenizer = tokenizer\n","        self.label2id = label2id\n","        self.max_length = max_length\n","        \n","    def __getitem__(self, idx):\n","        vals=tokenize(self.dataset[idx], self.tokenizer, self.label2id, self.max_length)\n","\n","        input_ids = torch.tensor(vals[\"input_ids\"])\n","        attention_mask = torch.tensor(vals[\"attention_mask\"])\n","        labels = torch.tensor(vals[\"labels\"], dtype=torch.long)\n","\n","        return input_ids, attention_mask, labels\n","    \n","    def __len__(self):\n","        return len(self.dataset)\n","    \n","\n","data = json.load(open(\"data/train.json\"))\n","\n","\n","from transformers import AutoTokenizer\n","\n","tokenizer = AutoTokenizer.from_pretrained(parameter[\"model\"])\n","\n","my_dataset=PiiDataset(data, tokenizer, label2id, parameter[\"max_length\"])\n"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([8, 512])\n","torch.Size([8, 512])\n","torch.Size([8, 512])\n"]}],"source":["\n","loader=torch.utils.data.DataLoader(my_dataset, batch_size=8, shuffle=True)\n","\n","for id, attention_mask, labels in loader:\n","    print(id.shape)\n","    print(attention_mask.shape)\n","    print(labels.shape)\n","    break"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["torch.Size([8, 512])\n","torch.Size([8, 512])\n","torch.Size([8, 512])\n","tensor([[12, 12, 12,  ..., 12, 12, 12],\n","        [12, 12, 12,  ..., 12, 12, 12],\n","        [12, 12, 12,  ..., 12, 12, 12],\n","        ...,\n","        [12, 12, 12,  ..., 12, 12, 12],\n","        [12, 12, 12,  ..., 12, 12, 12],\n","        [12, 12, 12,  ..., 12, 12, 12]])\n","torch.Size([8, 512, 13])\n"]}],"source":["from transformers import AutoModelForTokenClassification\n","\n","device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","class MyModel(torch.nn.Module):\n","    def __init__(self, model_name, num_labels):\n","        super().__init__()\n","        self.model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=num_labels)\n","        self.softmax=torch.nn.Softmax(dim=-1)\n","        \n","    def forward(self, input_ids, attention_mask, labels):\n","        out=self.model(input_ids, attention_mask=attention_mask, labels=labels)['logits']\n","        out=self.softmax(out)\n","        return out\n","\n","\n","model = MyModel(parameter['model'], len(label2id))\n","model= model.to(device)\n","for id, attention_mask, labels in loader:\n","    print(id.shape)\n","    print(attention_mask.shape)\n","    print(labels.shape)\n","    print(labels)\n","    id = id.to(device)\n","    attention_mask = attention_mask.to(device)\n","    labels = labels.to(device)\n","    print(model(id, attention_mask, labels).shape)\n","    break\n","\n","#free gpu memory\n","del model\n","torch.cuda.empty_cache()\n","\n"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["import neptune\n","from tqdm.notebook import tqdm\n","\n","class Learner():\n","    def __init__(self, model, train_dataloader, valid_dataloader, parameter=None):\n","        self.model=model\n","        #self.loss_fn=torch.nn.CrossEntropyLoss()\n","        self.loss_fn=torch.nn.CrossEntropyLoss(ignore_index=-100)\n","        self.device=torch.device(\"cpu\")\n","        if torch.cuda.is_available():\n","            self.device=torch.device(\"cuda\")\n","        #elif torch.backends.mps.is_available():\n","        #    self.device=torch.device(\"mps\")\n","\n","        self.model.to(self.device)\n","        self.run = neptune.init_run(\n","            project=\"bernd.heidemann/PII\",\n","            api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIzNjBlYzVkNi0zZTUwLTQ1ODYtODhlNC02NDUxNDg0MDdjNzUifQ==\",\n","        )  # your credentials\n","        self.parameter = parameter\n","        self.train_dataloader = train_dataloader\n","        self.valid_dataloader = valid_dataloader\n","        self.run[\"parameters\"] = {\n","            **self.parameter\n","        }\n","\n","    def fit(self, lr=0.001, epochs=10):\n","        optimizer=torch.optim.AdamW(self.model.parameters(), lr=lr)\n","        scheduler=scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n","        bar = tqdm(total=len(self.train_dataloader) * epochs, desc=\"Training\")\n","        bar.set_description(\"Epoch 0/{}\".format(epochs))\n","\n","        for epoch in range(epochs):\n","            self.model.train()            \n","            for ids, att_mask, labels in self.train_dataloader:\n","                \n","                ids=ids.to(self.device)\n","                labels=labels.to(self.device)\n","                att_mask=att_mask.to(self.device)\n","                pred=self.model(ids, att_mask, labels)\n","                # reshape pred to [batch_size, num_classes, sequence_length]\n","                metrics=self.f_beta_score_multiclass(labels, pred)\n","                self.run[\"train_f_beta_score\"].log(metrics[\"f_beta\"])\n","                self.run[\"train_precision\"].log(metrics[\"precision\"])\n","                self.run[\"train_recall\"].log(metrics[\"recall\"])\n","                self.run[\"train_true_positives\"].log(metrics[\"true_positives\"])\n","                self.run[\"train_false_positives\"].log(metrics[\"false_positives\"])\n","                self.run[\"train_false_negatives\"].log(metrics[\"false_negatives\"])\n","                pred = pred.permute(0, 2, 1)\n","                loss=self.loss_fn(pred, labels)\n","                self.run[\"train_loss\"].log(loss.item())\n","                loss.backward()\n","                optimizer.step()\n","                optimizer.zero_grad()\n","                bar.update(1)\n","            scheduler.step()\n","            self.model.eval()\n","            # log current state to neptune\n","            if self.valid_dataloader is not None:\n","                metrics=self.get_accuracy()\n","                self.run[\"valid_accuracy\"].log(metrics[\"accuracy\"])\n","                self.run[\"valid_loss\"].log(metrics[\"loss\"])\n","                self.run[\"valid_f_beta_score\"].log(metrics[\"f_beta_score\"])\n","                self.run[\"valid_precision\"].log(metrics[\"precision\"])\n","                self.run[\"valid_recall\"].log(metrics[\"recall\"])\n","                self.run[\"valid_true_positives\"].log(metrics[\"true_positives\"])\n","                self.run[\"valid_false_positives\"].log(metrics[\"false_positives\"])\n","                self.run[\"valid_false_negatives\"].log(metrics[\"false_negatives\"])\n","                bar.set_description(\"Epoch {}/{} validAccuracy: {:.2f} validLoss: {:.2f}\".format(epoch+1, epochs, metrics[\"accuracy\"], metrics[\"loss\"]))\n","            \n","                \n","    def get_accuracy(self):\n","        self.model.eval()\n","        with torch.no_grad():\n","            correct=0\n","            losses=[]\n","            batch_metrics=[]\n","            for ids, att_mask, labels in self.valid_dataloader:\n","                ids=ids.to(self.device)\n","                labels=labels.to(self.device)\n","                att_mask=att_mask.to(self.device)\n","                pred=self.model(ids, att_mask, labels)\n","                f_beta_score_results = self.f_beta_score_multiclass(labels, pred)\n","                batch_metrics.append(f_beta_score_results)\n","                pred = pred.permute(0, 2, 1)\n","                loss=self.loss_fn(pred, labels)\n","                losses.append(loss.item())\n","                pred=torch.argmax(pred, dim=1)\n","                correct+=torch.sum(pred==labels).item()\n","            # calc mean of the dict entries in batch_metrics\n","            f_beta_scores = np.mean([x[\"f_beta\"] for x in batch_metrics])\n","            precision = np.mean([x[\"precision\"] for x in batch_metrics])\n","            recall = np.mean([x[\"recall\"] for x in batch_metrics])\n","            true_positives = np.mean([x[\"true_positives\"] for x in batch_metrics])\n","            false_positives = np.mean([x[\"false_positives\"] for x in batch_metrics])\n","            false_negatives = np.mean([x[\"false_negatives\"] for x in batch_metrics])\n","\n","            return {\n","                \"accuracy\": correct/len(self.valid_dataloader.dataset),\n","                \"loss\": np.mean(losses),\n","                \"f_beta_score\": f_beta_scores,\n","                \"precision\": precision,\n","                \"recall\": recall,\n","                \"true_positives\": true_positives,\n","                \"false_positives\": false_positives,\n","                \"false_negatives\": false_negatives\n","            }\n","        \n","    def f_beta_score_multiclass(self, y_true, y_pred, beta=5, epsilon=1e-7):\n","\n","        # assert y_pred has values between 0 and 1\n","        assert y_pred.min() >= 0\n","        assert y_pred.max() <= 1\n","\n","        y_true_one_hot = torch.nn.functional.one_hot(y_true, num_classes=y_pred.shape[2])\n","    \n","        # Berechnung von True Positives, False Positives und False Negatives\n","        tp = torch.sum(y_true_one_hot * y_pred, dim=0)\n","        fp = torch.sum((1 - y_true_one_hot) * y_pred, dim=0)\n","        fn = torch.sum(y_true_one_hot * (1 - y_pred), dim=0)\n","\n","        # Summierung über alle Klassen\n","        tp_sum = torch.sum(tp)\n","        fp_sum = torch.sum(fp)\n","        fn_sum = torch.sum(fn)\n","\n","        # Berechnung von Präzision und Recall\n","        precision = tp_sum / (tp_sum + fp_sum + epsilon)\n","        recall = tp_sum / (tp_sum + fn_sum + epsilon)\n","\n","        # Berechnung des F-Beta-Scores\n","        f_beta = (1 + beta**2) * (precision * recall) / ((beta**2 * precision) + recall + epsilon)\n","\n","        return {\n","            \"f_beta\": f_beta.item(),\n","            \"precision\": precision.item(),\n","            \"recall\": recall.item(),\n","            \"true_positives\": tp_sum.item(),\n","            \"false_positives\": fp_sum.item(),\n","            \"false_negatives\": fn_sum.item()\n","        }\n","\n","    "]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["original len 6807\n","filtered len 2071\n","len train ds 1656\n","len valid ds 415\n"]}],"source":["data = json.load(open(\"data/train.json\"))\n","\n","print(\"original len\",  len(data))\n","# this increased the \n","data_filterd = list(filter(filter_no_pii, data))\n","print(\"filtered len\", len(data_filterd))\n","\n","data_len=len(data_filterd)\n","\n","train_len=int(len(data_filterd)*0.8)\n","valid_len=len(data_filterd)-train_len\n","\n","train_data_idx=np.random.choice(data_len, train_len, replace=False)\n","valid_data_idx=np.array(list(set(range(data_len))-set(train_data_idx)))\n","\n","train_data=[data_filterd[i] for i in train_data_idx]\n","valid_data=[data_filterd[i] for i in valid_data_idx]\n","\n","print(\"len train ds\", len(train_data))\n","print(\"len valid ds\", len(valid_data))"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"data":{"text/plain":["13"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["len(label2id)"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["https://app.neptune.ai/bernd.heidemann/PII/e/PII-27\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"56d85f0ef96a404c90e28ae3241a1c1c","version_major":2,"version_minor":0},"text/plain":["Training:   0%|          | 0/414 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from tokenizers import AddedToken\n","\n","# set environment variables: TOKENIZERS_PARALLELISM=false\n","import os\n","os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n","\n","tokenizer = AutoTokenizer.from_pretrained(parameter[\"model\"])\n","tokenizer.add_tokens(AddedToken(\"\\n\", normalized=False))\n","\n","train_dataset = PiiDataset(train_data, tokenizer, label2id, parameter[\"max_length\"])\n","valid_dataset = PiiDataset(valid_data, tokenizer, label2id, parameter[\"max_length\"])\n","train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=parameter['batch_size'], shuffle=True)\n","valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=parameter['batch_size'], shuffle=False)\n","my_model=MyModel(parameter['model'], len(label2id))\n","\n","learner=Learner(my_model, train_dataloader, valid_dataloader, parameter=parameter)\n","learner.fit(lr=parameter['lr'], epochs=2)"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["predictions: tensor([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","        4096], device='cuda:0')\n","ground truth:  tensor([   0,    0,    7,    0,    0,    0,    0,    0,    6,    0,    0,    0,\n","        4083], device='cuda:0')\n"]}],"source":["for id, attention_mask, labels in valid_dataloader:\n","    id=id.to(device)\n","    attention_mask=attention_mask.to(device)\n","    labels=labels.to(device)\n","    preds=learner.model(id, attention_mask, labels).argmax(dim=2)\n","    # print frequency of labels\n","    print(\"predictions:\" , torch.bincount(preds.flatten()))\n","    print(\"ground truth: \", torch.bincount(labels.flatten()))\n","    break"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"04a67609302b4591a727a0841f796d6a","version_major":2,"version_minor":0},"text/plain":["Training:   0%|          | 0/4140 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["learner.fit(lr=parameter['lr'], epochs=20)"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["# save model\n","\n","torch.save(learner.model.state_dict(), \"model.pt\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":7500999,"sourceId":66653,"sourceType":"competition"},{"datasetId":4319117,"sourceId":7429898,"sourceType":"datasetVersion"}],"dockerImageVersionId":30636,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
