{"cells":[{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForTokenClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight', 'deberta.embeddings.position_embeddings.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight']\n","- This IS expected if you are initializing DebertaV2ForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2ForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"name":"stdout","output_type":"stream","text":["next\n","next\n","next\n","next\n","next\n","TOKEN:wanted  --- pred:B-URL_PERSONAL\n","TOKEN:needed  --- pred:B-URL_PERSONAL\n","TOKEN:without  --- pred:I-PHONE_NUM\n","TOKEN:why  --- pred:I-PHONE_NUM\n","next\n","next\n","next\n","next\n","next\n"]}],"source":["import torch\n","from transformers import AutoTokenizer\n","from transformers import AutoModelForTokenClassification\n","import json\n","\n","\n","parameter= {\n","    \"model\": \"microsoft/deberta-v3-base\",\n","    \"max_length\": 512,\n","    \"batch_size\": 8,\n","    \"model_path\": \"../PII Models/model_nb_04.pt\"\n","}\n","\n","def tokenize(example, tokenizer, max_length):\n","    text = []\n","    for t,  ws in zip(example[\"tokens\"], example[\"trailing_whitespace\"]):\n","        text.append(t)\n","        if ws:\n","            text.append(\" \")\n","    tokenized = tokenizer(\"\".join(text), return_offsets_mapping=True, truncation=True, max_length=max_length, padding=\"max_length\")\n","    text = \"\".join(text)\n","    length = len(tokenized.input_ids)\n","    return {\n","        **tokenized,\n","        \"length\": length,\n","    }\n","\n","class PiiDatasetInference(torch.utils.data.Dataset):\n","    def __init__(self, dataset, tokenizer, max_length):\n","        self.dataset = dataset\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","        \n","    def __getitem__(self, idx):\n","        vals=tokenize(self.dataset[idx], self.tokenizer, self.max_length)\n","        input_ids = torch.tensor(vals[\"input_ids\"])\n","        attention_mask = torch.tensor(vals[\"attention_mask\"])\n","        return input_ids, attention_mask\n","    \n","    def __len__(self):\n","        return len(self.dataset)\n","    \n","class MyModel(torch.nn.Module):\n","    def __init__(self, model_name, num_labels):\n","        super().__init__()\n","        self.model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=num_labels)\n","        self.softmax=torch.nn.Softmax(dim=-1)\n","       \n","    def forward(self, input_ids, attention_mask):\n","        out=self.model(input_ids, attention_mask=attention_mask)['logits']\n","        out=self.softmax(out)\n","        return out\n","    \n","model=MyModel(parameter[\"model\"], 13)\n","state_dict = torch.load(parameter['model_path'], map_location=torch.device('cpu'))\n","model.load_state_dict(state_dict, strict=False)\n","\n","data = json.load(open(\"data/train.json\"))\n","from itertools import chain\n","all_labels = sorted(list(set(chain(*[x[\"labels\"] for x in data]))))\n","label2id = {l: i for i,l in enumerate(all_labels)}\n","id2label = {v:k for k,v in label2id.items()}\n","\n","tokenizer = AutoTokenizer.from_pretrained(parameter[\"model\"])\n","data = json.load(open(\"data/test.json\"))\n","my_dataset=PiiDatasetInference(data, tokenizer, parameter[\"max_length\"])\n","loader=torch.utils.data.DataLoader(my_dataset, batch_size=1, shuffle=True)\n","for id, attention_mask in loader:\n","    preds=model(id, attention_mask).argmax(dim=2)\n","\n","    for pred, id in zip(preds.flatten(), id.flatten()):\n","        if pred != 12:\n","            print(f\"TOKEN:{tokenizer.decode(id)}  --- pred:{id2label[pred.item()]}\")\n","    print(\"next\")\n","    \n","    "]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":7500999,"sourceId":66653,"sourceType":"competition"},{"datasetId":4319117,"sourceId":7429898,"sourceType":"datasetVersion"}],"dockerImageVersionId":30636,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"}},"nbformat":4,"nbformat_minor":4}
