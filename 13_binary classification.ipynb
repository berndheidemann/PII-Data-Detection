{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-01-25T21:37:35.578114Z","iopub.status.busy":"2024-01-25T21:37:35.577723Z","iopub.status.idle":"2024-01-25T21:37:35.587366Z","shell.execute_reply":"2024-01-25T21:37:35.5858Z","shell.execute_reply.started":"2024-01-25T21:37:35.578083Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["{'model': 'microsoft/deberta-v3-large', 'max_length': 512, 'batch_size': 5, 'lr': 0.001, 'filter_no_pii_percent_allow': 0.1, 'notebook': '13_binary classification.ipynb', 'CROSS_ENTROPY_WEIGHT_MULTI': 200, 'num_classes': 2}\n"]}],"source":["# import Path\n","\n","from pathlib import Path\n","import numpy as np\n","import torch\n","\n","    \n","parameter= {\n","    \"model\": \"microsoft/deberta-v3-large\",\n","    \"max_length\": 512,\n","    \"batch_size\": 5,\n","    \"lr\": 1e-3,\n","    \"filter_no_pii_percent_allow\": 0.1,\n","    \"notebook\": \"13_binary classification.ipynb\",\n","    \"CROSS_ENTROPY_WEIGHT_MULTI\": 200,\n","    'num_classes': 2\n","}\n","\n","\n","CROSS_ENTROPY_WEIGHTS = [1, parameter['CROSS_ENTROPY_WEIGHT_MULTI']]\n","\n","\n","print(parameter)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["CROSS_ENTROPY_WEIGHTS"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-01-25T21:37:35.590432Z","iopub.status.busy":"2024-01-25T21:37:35.589758Z","iopub.status.idle":"2024-01-25T21:37:35.604681Z","shell.execute_reply":"2024-01-25T21:37:35.603633Z","shell.execute_reply.started":"2024-01-25T21:37:35.590393Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["6807\n","dict_keys(['document', 'full_text', 'tokens', 'trailing_whitespace', 'labels'])\n","['Design', 'Thinking', 'for', 'innovation', 'reflexion', '-', 'Avril', '2021', '-', 'Nathalie']\n","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NAME_STUDENT']\n","[True, True, True, True, False, False, True, False, False, True]\n"]}],"source":["\n","import json\n","\n","data = json.load(open(\"data/train.json\"))\n","\n","print(len(data))\n","print(data[0].keys())\n","\n","x = data[0]\n","\n","print(x[\"tokens\"][:10])\n","print(x[\"labels\"][:10])\n","print(x[\"trailing_whitespace\"][:10])"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"data":{"text/plain":["dict_keys(['document', 'full_text', 'tokens', 'trailing_whitespace', 'labels'])"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["x.keys()"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["target = [\n","    'B-EMAIL', 'B-ID_NUM', 'B-NAME_STUDENT', 'B-PHONE_NUM', \n","    'B-STREET_ADDRESS', 'B-URL_PERSONAL', 'B-USERNAME', 'I-ID_NUM', \n","    'I-NAME_STUDENT', 'I-PHONE_NUM', 'I-STREET_ADDRESS', 'I-URL_PERSONAL'\n","]"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-01-25T21:37:35.606208Z","iopub.status.busy":"2024-01-25T21:37:35.605889Z","iopub.status.idle":"2024-01-25T21:37:35.62164Z","shell.execute_reply":"2024-01-25T21:37:35.620746Z","shell.execute_reply.started":"2024-01-25T21:37:35.606175Z"},"trusted":true},"outputs":[],"source":["\n","from itertools import chain\n","\n","\n","\n","label2id={\n","\n","}\n","\n","for label in target:\n","    label2id[label]=1\n","\n","label2id['O']=0\n","\n","\n","id2label = {\n","    0: 'NON_PII',\n","    1: 'PII'\n","}"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","c:\\Users\\Bernd\\anaconda3\\envs\\mytorch\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:470: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n","  warnings.warn(\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"name":"stdout","output_type":"stream","text":["token [CLS] label 0\n","token Design label 0\n","token Thinking label 0\n","token for label 0\n","token innovation label 0\n","token reflex label 0\n","token ion label 0\n","token - label 0\n","token Av label 0\n","token ril label 0\n","token 2021 label 0\n","token - label 0\n","token N label 1\n","token atha label 1\n","token lie label 1\n","token S label 1\n","token ylla label 1\n","token Challenge label 0\n","token & label 0\n","token selection label 0\n","token The label 0\n","token tool label 0\n","token I label 0\n","token use label 0\n","token to label 0\n","token help label 0\n","token all label 0\n","token stakeholders label 0\n","token finding label 0\n","token [SEP] label 0\n"]}],"source":["import random\n","from transformers import AutoTokenizer\n","\n","def tokenize(example, tokenizer, label2id, max_length):\n","    text = []\n","\n","    # these are at the character level\n","    labels = []\n","    targets = []\n","\n","    for t, l, ws in zip(example[\"tokens\"], example[\"labels\"], example[\"trailing_whitespace\"]):\n","\n","        text.append(t)\n","        labels.extend([l]*len(t))\n","        \n","        if l in target:\n","            targets.append(1)\n","        else:\n","            targets.append(0)\n","        # if there is trailing whitespace\n","        if ws:\n","            text.append(\" \")\n","            labels.append(\"O\")\n","\n","    tokenized = tokenizer(\"\".join(text), return_offsets_mapping=True, truncation=True, max_length=max_length, padding=\"max_length\")\n","    \n","    target_num = sum(targets)\n","    labels = np.array(labels)\n","\n","    text = \"\".join(text)\n","    token_labels = []\n","\n","    for start_idx, end_idx in tokenized.offset_mapping:\n","\n","        # CLS token\n","        if start_idx == 0 and end_idx == 0: \n","            token_labels.append(label2id[\"O\"])\n","            continue\n","\n","        # case when token starts with whitespace\n","        if text[start_idx].isspace():\n","            start_idx += 1\n","\n","        try:\n","            token_labels.append(label2id[labels[start_idx]])\n","        except:\n","            token_labels.append(label2id[\"O\"])\n","\n","    length = len(tokenized.input_ids)\n","\n","    return {\n","        **tokenized,\n","        \"labels\": token_labels,\n","        \"length\": length,\n","        \"target_num\": target_num,\n","        \"group\": 1 if target_num>0 else 0\n","    }\n","\n","# https://www.kaggle.com/competitions/pii-detection-removal-from-educational-data/discussion/468844\n","def filter_no_pii(example, percent_allow=parameter[\"filter_no_pii_percent_allow\"]):\n","    # Return True if there is PII\n","    # Or 20% of the time if there isn't\n","    has_pii = set(\"O\") != set(example[\"labels\"])\n","    return has_pii or (random.random() < percent_allow)\n","    \n","tokenizer = AutoTokenizer.from_pretrained(parameter[\"model\"])\n","\n","input_ids=tokenize(x, tokenizer, label2id, 30)['input_ids']\n","labels=tokenize(x, tokenizer, label2id, 30)['labels']\n","\n","for i in range(30):\n","    print(\"token\", tokenizer.decode(input_ids[i]), \"label\", labels[i])"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["import torch\n","import json\n","\n","class PiiDataset(torch.utils.data.Dataset):\n","    def __init__(self, dataset, tokenizer, label2id, max_length):\n","        self.dataset = dataset\n","        self.tokenizer = tokenizer\n","        self.label2id = label2id\n","        self.max_length = max_length\n","        \n","    def __getitem__(self, idx):\n","        vals=tokenize(self.dataset[idx], self.tokenizer, self.label2id, self.max_length)\n","\n","        input_ids = torch.tensor(vals[\"input_ids\"])\n","        attention_mask = torch.tensor(vals[\"attention_mask\"])\n","        labels = torch.tensor(vals[\"labels\"], dtype=torch.long)\n","\n","        return input_ids, attention_mask, labels\n","    \n","    def __len__(self):\n","        return len(self.dataset)\n","    \n","\n","data = json.load(open(\"data/train.json\"))\n","\n","\n","from transformers import AutoTokenizer\n","\n","tokenizer = AutoTokenizer.from_pretrained(parameter[\"model\"])\n","\n","my_dataset=PiiDataset(data, tokenizer, label2id, parameter[\"max_length\"])\n","\n","\n"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([8, 512])\n","torch.Size([8, 512])\n","torch.Size([8, 512])\n"]}],"source":["\n","loader=torch.utils.data.DataLoader(my_dataset, batch_size=8, shuffle=True)\n","\n","for id, attention_mask, labels in loader:\n","    print(id.shape)\n","    print(attention_mask.shape)\n","    print(labels.shape)\n","    break"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["still learning classifier.weight parameter_size: torch.Size([2, 1024])\n","still learning classifier.bias parameter_size: torch.Size([2])\n","torch.Size([8, 512])\n","torch.Size([8, 512])\n","torch.Size([8, 512])\n","tensor([[0, 0, 0,  ..., 0, 0, 0],\n","        [0, 0, 0,  ..., 0, 0, 0],\n","        [0, 0, 0,  ..., 0, 0, 0],\n","        ...,\n","        [0, 0, 0,  ..., 0, 0, 0],\n","        [0, 0, 0,  ..., 0, 0, 0],\n","        [0, 0, 0,  ..., 0, 0, 0]])\n","torch.Size([8, 512, 2])\n"]}],"source":["from transformers import AutoModelForTokenClassification\n","\n","device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","class MyModel(torch.nn.Module):\n","    def __init__(self, model_name, num_labels, dropout_p=0.4):\n","        super().__init__()\n","        self.model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=num_labels)\n","        self.softmax=torch.nn.Softmax(dim=-1)\n","        self.freeze()\n","\n","    def freeze(self):\n","        # freeze all parameters\n","        for param in self.model.parameters():\n","            param.requires_grad = False\n","        # unfreeze classifier layer\n","        for param in self.model.classifier.parameters():\n","            param.requires_grad = True\n","        for name, param in self.model.named_parameters():\n","            if param.requires_grad==True:\n","                print(\"still learning\", name, \"parameter_size:\", param.size())\n","\n","    def unfreeze(self):\n","        for param in self.model.parameters():\n","            param.requires_grad = True\n","        \n","    def forward(self, input_ids, attention_mask, labels=None):\n","        if labels is not None:\n","            out=self.model(input_ids, attention_mask=attention_mask, labels=labels)['logits']\n","        else:\n","            out=self.model(input_ids, attention_mask=attention_mask)['logits']\n","        out=self.softmax(out)\n","        return out\n","\n","\n","model = MyModel(parameter['model'], parameter['num_classes'])\n","\n","model= model.to(device)\n","for id, attention_mask, labels in loader:\n","    print(id.shape)\n","    print(attention_mask.shape)\n","    print(labels.shape)\n","    print(labels)\n","    id = id.to(device)\n","    attention_mask = attention_mask.to(device)\n","    labels = labels.to(device)\n","    print(model(id, attention_mask, labels).shape)\n","    break\n","\n","#free gpu memory\n","del model\n","torch.cuda.empty_cache()\n","\n"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["import neptune\n","from tqdm.notebook import tqdm\n","\n","\n","# import CosineAnnealingWarmRestarts\n","from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n","\n","\n","class Learner():\n","    def __init__(self, model, train_dataloader, valid_dataloader, parameter=None):\n","        self.model=model\n","        #self.loss_fn=torch.nn.CrossEntropyLoss()\n","        self.loss_fn=torch.nn.CrossEntropyLoss(ignore_index=-100, weight=torch.tensor(CROSS_ENTROPY_WEIGHTS, dtype=torch.float32).to(device))\n","        self.device=torch.device(\"cpu\")\n","        if torch.cuda.is_available():\n","            self.device=torch.device(\"cuda\")\n","        #elif torch.backends.mps.is_available():\n","        #    self.device=torch.device(\"mps\")\n","\n","        self.model.to(self.device)\n","        self.run = neptune.init_run(\n","            project=\"bernd.heidemann/PII\",\n","            api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIzNjBlYzVkNi0zZTUwLTQ1ODYtODhlNC02NDUxNDg0MDdjNzUifQ==\",\n","        )  # your credentials\n","        self.parameter = parameter\n","        self.train_dataloader = train_dataloader\n","        self.valid_dataloader = valid_dataloader\n","        self.run[\"parameters\"] = {\n","            **self.parameter\n","        }\n","        self.non_pii_label=label2id[\"O\"]\n","\n","    def fit(self, lr=0.1, epochs=10):\n","        \n","        optimizer=torch.optim.AdamW(self.model.parameters(), lr=lr)\n","        T_0 = epochs//3          # Number of epochs before the first restart\n","        T_mult = 2        # Factor by which T_0 is multiplied after each restart\n","        eta_min = lr*0.01   # Minimum learning rate at restarts\n","\n","        # Create the CosineAnnealingWarmRestarts scheduler\n","        scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=T_0, T_mult=T_mult, eta_min=eta_min)\n","        bar = tqdm(total=len(self.train_dataloader) * epochs, desc=\"Training\")\n","        bar.set_description(\"Epoch 0/{}\".format(epochs))\n","        for epoch in range(epochs):\n","            self.model.train()     \n","            pii_count=0       \n","            for ids, att_mask, labels in self.train_dataloader:\n","                ids=ids.to(self.device)\n","                labels=labels.to(self.device)\n","                att_mask=att_mask.to(self.device)\n","                pred=self.model(ids, att_mask, labels)\n","                # reshape pred to [batch_size, num_classes, sequence_length]\n","                metrics=self.f_beta_score_multiclass(labels, pred)\n","                self.run[\"train_f_beta_score\"].log(metrics[\"f_beta\"])\n","                self.run[\"train_precision\"].log(metrics[\"precision\"])\n","                self.run[\"train_recall\"].log(metrics[\"recall\"])\n","                self.run[\"train_true_positives\"].log(metrics[\"true_positives\"])\n","                self.run[\"train_false_positives\"].log(metrics[\"false_positives\"])\n","                self.run[\"train_false_negatives\"].log(metrics[\"false_negatives\"])\n","                pred = pred.permute(0, 2, 1)\n","                loss=self.loss_fn(pred, labels)\n","                self.run[\"train_loss\"].log(loss.item())\n","                loss.backward()\n","                optimizer.step()\n","                optimizer.zero_grad()\n","                bar.update(1)\n","                #count all predictions that do not have the label \"O\"\n","                pii_count_batch = torch.sum(torch.argmax(pred, dim=1) != self.non_pii_label).item()\n","                pii_count += pii_count_batch\n","            self.run[\"train_pii_count\"].log(pii_count)\n","            self.run[\"learnrate\"].log(optimizer.param_groups[0][\"lr\"])\n","            scheduler.step()\n","            self.model.eval()\n","            # log current state to neptune\n","            if self.valid_dataloader is not None:\n","                metrics=self.get_accuracy()\n","                self.run[\"valid_accuracy\"].log(metrics[\"accuracy\"])\n","                self.run[\"valid_loss\"].log(metrics[\"loss\"])\n","                self.run[\"valid_f_beta_score\"].log(metrics[\"f_beta_score\"])\n","                self.run[\"valid_precision\"].log(metrics[\"precision\"])\n","                self.run[\"valid_recall\"].log(metrics[\"recall\"])\n","                self.run[\"valid_true_positives\"].log(metrics[\"true_positives\"])\n","                self.run[\"valid_false_positives\"].log(metrics[\"false_positives\"])\n","                self.run[\"valid_false_negatives\"].log(metrics[\"false_negatives\"])\n","                self.run[\"valid_pii_count\"].log(metrics[\"pii_count\"])\n","                bar.set_description(\"Epoch {}/{} validAccuracy: {:.2f} validLoss: {:.2f}\".format(epoch+1, epochs, metrics[\"accuracy\"], metrics[\"loss\"]))\n","\n","                \n","    def get_accuracy(self):\n","        self.model.eval()\n","        with torch.no_grad():\n","            correct=0\n","            losses=[]\n","            batch_metrics=[]\n","            pii_count=0       \n","            for ids, att_mask, labels in self.valid_dataloader:\n","                ids=ids.to(self.device)\n","                labels=labels.to(self.device)\n","                att_mask=att_mask.to(self.device)\n","                pred=self.model(ids, att_mask, labels)\n","                f_beta_score_results = self.f_beta_score_multiclass(labels, pred)\n","                batch_metrics.append(f_beta_score_results)\n","                pred = pred.permute(0, 2, 1)\n","                loss=self.loss_fn(pred, labels)\n","                losses.append(loss.item())\n","                pred=torch.argmax(pred, dim=1)\n","                correct+=torch.sum(pred==labels).item()\n","                pii_count_batch = torch.sum((pred != self.non_pii_label)).item()\n","                pii_count += pii_count_batch\n","            # calc mean of the dict entries in batch_metrics\n","            f_beta_scores = np.mean([x[\"f_beta\"] for x in batch_metrics])\n","            precision = np.mean([x[\"precision\"] for x in batch_metrics])\n","            recall = np.mean([x[\"recall\"] for x in batch_metrics])\n","            true_positives = np.mean([x[\"true_positives\"] for x in batch_metrics])\n","            false_positives = np.mean([x[\"false_positives\"] for x in batch_metrics])\n","            false_negatives = np.mean([x[\"false_negatives\"] for x in batch_metrics])\n","\n","            return {\n","                \"accuracy\": correct/len(self.valid_dataloader.dataset),\n","                \"loss\": np.mean(losses),\n","                \"f_beta_score\": f_beta_scores,\n","                \"precision\": precision,\n","                \"recall\": recall,\n","                \"true_positives\": true_positives,\n","                \"false_positives\": false_positives,\n","                \"false_negatives\": false_negatives,\n","                 \"pii_count\": pii_count\n","            }\n","        \n","    def f_beta_score_multiclass(self, y_true, y_pred, beta=5, epsilon=1e-7):\n","\n","        # assert y_pred has values between 0 and 1\n","        assert y_pred.min() >= 0\n","        assert y_pred.max() <= 1\n","\n","        y_true_one_hot = torch.nn.functional.one_hot(y_true, num_classes=y_pred.shape[2])\n","    \n","        # Berechnung von True Positives, False Positives und False Negatives\n","        tp = torch.sum(y_true_one_hot * y_pred, dim=0)\n","        fp = torch.sum((1 - y_true_one_hot) * y_pred, dim=0)\n","        fn = torch.sum(y_true_one_hot * (1 - y_pred), dim=0)\n","\n","        # Summierung über alle Klassen\n","        tp_sum = torch.sum(tp)\n","        fp_sum = torch.sum(fp)\n","        fn_sum = torch.sum(fn)\n","\n","        # Berechnung von Präzision und Recall\n","        precision = tp_sum / (tp_sum + fp_sum + epsilon)\n","        recall = tp_sum / (tp_sum + fn_sum + epsilon)\n","        # Berechnung des F-Beta-Scores\n","        f_beta = (1 + beta**2) * (precision * recall) / ((beta**2 * precision) + recall + epsilon)\n","\n","        return {\n","            \"f_beta\": f_beta.item(),\n","            \"precision\": precision.item(),\n","            \"recall\": recall.item(),\n","            \"true_positives\": tp_sum.item(),\n","            \"false_positives\": fp_sum.item(),\n","            \"false_negatives\": fn_sum.item()\n","        }\n","\n","    "]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["def inference(model):\n","    import torch\n","    from transformers import AutoTokenizer\n","    from transformers import AutoModelForTokenClassification\n","    import json\n","\n","\n","    parameter_inference= {\n","        \"max_length\": 512,\n","        \"batch_size\": 8,\n","        \"model_path\": \"../PII Models/model_nb_04.pt\",\n","        \"model\": \"microsoft/deberta-v3-base\"\n","    }\n","\n","    def tokenize(example, tokenizer, max_length):\n","        text = []\n","        for t,  ws in zip(example[\"tokens\"], example[\"trailing_whitespace\"]):\n","            text.append(t)\n","            if ws:\n","                text.append(\" \")\n","        tokenized = tokenizer(\"\".join(text), return_offsets_mapping=True, truncation=True, max_length=max_length, padding=\"max_length\")\n","        text = \"\".join(text)\n","        length = len(tokenized.input_ids)\n","        return {\n","            **tokenized,\n","            \"length\": length,\n","        }\n","\n","    class PiiDatasetInference(torch.utils.data.Dataset):\n","        def __init__(self, dataset, tokenizer, max_length):\n","            self.dataset = dataset\n","            self.tokenizer = tokenizer\n","            self.max_length = max_length\n","            \n","        def __getitem__(self, idx):\n","            vals=tokenize(self.dataset[idx], self.tokenizer, self.max_length)\n","            input_ids = torch.tensor(vals[\"input_ids\"])\n","            attention_mask = torch.tensor(vals[\"attention_mask\"])\n","            return input_ids, attention_mask\n","        \n","        def __len__(self):\n","            return len(self.dataset)\n","        \n","    data = json.load(open(\"data/train.json\")) # foo\n","    from itertools import chain\n","    \n","    label2id={}\n","    for label in target:\n","        label2id[label]=1\n","    label2id['O']=0\n","\n","    id2label = {\n","        0: 'NON_PII',\n","        1: 'PII'\n","    }\n","\n","    tokenizer = AutoTokenizer.from_pretrained(parameter_inference[\"model\"])\n","    data = json.load(open(\"data/test.json\"))\n","    my_dataset=PiiDatasetInference(data, tokenizer, parameter_inference[\"max_length\"])\n","    loader=torch.utils.data.DataLoader(my_dataset, batch_size=parameter_inference['batch_size'], shuffle=True)\n","    for id, attention_mask in loader:\n","        id = id.to(device)\n","        attention_mask = attention_mask.to(device)\n","        preds=model(id, attention_mask).argmax(dim=2)\n","\n","        for pred, id in zip(preds.flatten(), id.flatten()):\n","            if pred != 0:\n","                print(f\"TOKEN:{tokenizer.decode(id)}  --- pred:{id2label[pred.item()]}\")\n","        print(\"next\")"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["original len 6807\n","filtered len 1567\n","len train ds 1253\n","len valid ds 314\n"]}],"source":["data = json.load(open(\"data/train.json\"))\n","\n","print(\"original len\",  len(data))\n","# this increased the \n","data_filterd = list(filter(filter_no_pii, data))\n","print(\"filtered len\", len(data_filterd))\n","\n","data_len=len(data_filterd)\n","\n","train_len=int(len(data_filterd)*0.8)\n","valid_len=len(data_filterd)-train_len\n","\n","train_data_idx=np.random.choice(data_len, train_len, replace=False)\n","valid_data_idx=np.array(list(set(range(data_len))-set(train_data_idx)))\n","\n","train_data=[data_filterd[i] for i in train_data_idx]\n","valid_data=[data_filterd[i] for i in valid_data_idx]\n","\n","print(\"len train ds\", len(train_data))\n","print(\"len valid ds\", len(valid_data))"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","c:\\Users\\Bernd\\anaconda3\\envs\\mytorch\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:470: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n","  warnings.warn(\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["still learning classifier.weight parameter_size: torch.Size([2, 1024])\n","still learning classifier.bias parameter_size: torch.Size([2])\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\Bernd\\anaconda3\\envs\\mytorch\\lib\\site-packages\\neptune\\common\\warnings.py:71: NeptuneWarning: The following monitoring options are disabled by default in interactive sessions: 'capture_stdout', 'capture_stderr', 'capture_traceback', and 'capture_hardware_metrics'. To enable them, set each parameter to 'True' when initializing the run. The monitoring will continue until you call run.stop() or the kernel stops. Also note: Your source files can only be tracked if you pass the path(s) to the 'source_code' argument. For help, see the Neptune docs: https://docs.neptune.ai/logging/source_code/\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["https://app.neptune.ai/bernd.heidemann/PII/e/PII-122\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"39d38876d5c54ab5b4f8335ba5c0d3d0","version_major":2,"version_minor":0},"text/plain":["Training:   0%|          | 0/1506 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from tokenizers import AddedToken\n","\n","# set environment variables: TOKENIZERS_PARALLELISM=false\n","import os\n","os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n","\n","tokenizer = AutoTokenizer.from_pretrained(parameter[\"model\"])\n","tokenizer.add_tokens(AddedToken(\"\\n\", normalized=False))\n","\n","train_dataset = PiiDataset(train_data, tokenizer, label2id, parameter[\"max_length\"])\n","valid_dataset = PiiDataset(valid_data, tokenizer, label2id, parameter[\"max_length\"])\n","train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=parameter['batch_size'], shuffle=True)\n","valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=parameter['batch_size'], shuffle=False)\n","my_model=MyModel(parameter['model'], parameter['num_classes'])\n","\n","learner=Learner(my_model, train_dataloader, valid_dataloader, parameter=parameter)\n","learner.fit(lr=parameter['lr'], epochs=6)"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","c:\\Users\\Bernd\\anaconda3\\envs\\mytorch\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:470: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n","  warnings.warn(\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"name":"stdout","output_type":"stream","text":["TOKEN:Francisco  --- pred:PII\n","TOKEN:Ferreira  --- pred:PII\n","TOKEN:Ant  --- pred:PII\n","TOKEN:Carlos  --- pred:PII\n","TOKEN:Silvia  --- pred:PII\n","TOKEN:Villa  --- pred:PII\n","TOKEN:lobo  --- pred:PII\n","TOKEN:s  --- pred:PII\n","TOKEN:Nadine  --- pred:PII\n","TOKEN:Born  --- pred:PII\n","TOKEN:p  --- pred:PII\n","TOKEN:49  --- pred:PII\n","TOKEN:)  --- pred:PII\n","TOKEN:Storytelling  --- pred:PII\n","TOKEN:Dr  --- pred:PII\n","TOKEN:Saki  --- pred:PII\n","TOKEN:r  --- pred:PII\n","TOKEN:Ahmad  --- pred:PII\n","TOKEN:Design  --- pred:PII\n","TOKEN:Thinking  --- pred:PII\n","TOKEN:for  --- pred:PII\n","TOKEN:innovation  --- pred:PII\n","TOKEN:reflex  --- pred:PII\n","TOKEN:ion  --- pred:PII\n","TOKEN:-  --- pred:PII\n","TOKEN:Av  --- pred:PII\n","TOKEN:ril  --- pred:PII\n","TOKEN:2021  --- pred:PII\n","TOKEN:-  --- pred:PII\n","TOKEN:N  --- pred:PII\n","TOKEN:atha  --- pred:PII\n","TOKEN:lie  --- pred:PII\n","TOKEN:S  --- pred:PII\n","TOKEN:ylla  --- pred:PII\n","TOKEN:Buz  --- pred:PII\n","TOKEN:an  --- pred:PII\n","TOKEN:T  --- pred:PII\n","TOKEN:Buz  --- pred:PII\n","TOKEN:an  --- pred:PII\n","TOKEN:B  --- pred:PII\n","TOKEN:Paris  --- pred:PII\n","TOKEN:Cf  --- pred:PII\n","TOKEN:Annex  --- pred:PII\n","TOKEN:Design  --- pred:PII\n","TOKEN:Thinking  --- pred:PII\n","TOKEN:for  --- pred:PII\n","TOKEN:innovation  --- pred:PII\n","TOKEN:reflex  --- pred:PII\n","TOKEN:ion  --- pred:PII\n","TOKEN:-  --- pred:PII\n","TOKEN:Av  --- pred:PII\n","TOKEN:ril  --- pred:PII\n","TOKEN:2021  --- pred:PII\n","TOKEN:-  --- pred:PII\n","TOKEN:N  --- pred:PII\n","TOKEN:atha  --- pred:PII\n","TOKEN:lie  --- pred:PII\n","TOKEN:S  --- pred:PII\n","TOKEN:ylla  --- pred:PII\n","TOKEN:Ela  --- pred:PII\n","TOKEN:dio  --- pred:PII\n","TOKEN:Amaya  --- pred:PII\n","TOKEN:Sind  --- pred:PII\n","TOKEN:y  --- pred:PII\n","TOKEN:Sam  --- pred:PII\n","TOKEN:aca  --- pred:PII\n","TOKEN:Gita  --- pred:PII\n","TOKEN:m  --- pred:PII\n","TOKEN:University  --- pred:PII\n","TOKEN:December  --- pred:PII\n","TOKEN:2021  --- pred:PII\n","TOKEN:George  --- pred:PII\n","TOKEN:Geoff  --- pred:PII\n","TOKEN:Gandhi  --- pred:PII\n","TOKEN:Higher  --- pred:PII\n","TOKEN:Stefano  --- pred:PII\n","TOKEN:Lovato  --- pred:PII\n","TOKEN:MD  --- pred:PII\n","TOKEN:I  --- pred:PII\n","TOKEN:-  --- pred:PII\n","TOKEN:191  --- pred:PII\n","TOKEN:Sathya  --- pred:PII\n","TOKEN:b  --- pred:PII\n","TOKEN:ama  --- pred:PII\n","TOKEN:April  --- pred:PII\n","TOKEN:2020  --- pred:PII\n","TOKEN:Stuart  --- pred:PII\n","TOKEN:Gera  --- pred:PII\n","TOKEN:shchenko  --- pred:PII\n","TOKEN:Igor  --- pred:PII\n","TOKEN:Alexander  --- pred:PII\n","TOKEN:Sh  --- pred:PII\n","TOKEN:m  --- pred:PII\n","TOKEN:akov  --- pred:PII\n","next\n","TOKEN:Gilberto  --- pred:PII\n","TOKEN:Gamb  --- pred:PII\n","TOKEN:oa  --- pred:PII\n","TOKEN:Diego  --- pred:PII\n","TOKEN:Estrada  --- pred:PII\n","TOKEN:Diego  --- pred:PII\n","TOKEN:Estrada  --- pred:PII\n","next\n"]}],"source":["inference(learner.model)"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["learner.model.unfreeze()"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5cc34331e5484ce1be7d0a41d5c3e731","version_major":2,"version_minor":0},"text/plain":["Training:   0%|          | 0/1506 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"name":"stdout","output_type":"stream","text":["TOKEN:Sind  --- pred:PII\n","TOKEN:y  --- pred:PII\n","TOKEN:Sam  --- pred:PII\n","TOKEN:aca  --- pred:PII\n","TOKEN:Gita  --- pred:PII\n","TOKEN:m  --- pred:PII\n","TOKEN:University  --- pred:PII\n","TOKEN:George  --- pred:PII\n","TOKEN:Geoff  --- pred:PII\n","TOKEN:Diego  --- pred:PII\n","TOKEN:Estrada  --- pred:PII\n","TOKEN:Diego  --- pred:PII\n","TOKEN:Estrada  --- pred:PII\n","TOKEN:Nadine  --- pred:PII\n","TOKEN:Born  --- pred:PII\n","TOKEN:Gilberto  --- pred:PII\n","TOKEN:Gamb  --- pred:PII\n","TOKEN:oa  --- pred:PII\n","TOKEN:Dr  --- pred:PII\n","TOKEN:Saki  --- pred:PII\n","TOKEN:r  --- pred:PII\n","TOKEN:Ahmad  --- pred:PII\n","TOKEN:Silvia  --- pred:PII\n","TOKEN:Villa  --- pred:PII\n","TOKEN:lobo  --- pred:PII\n","TOKEN:s  --- pred:PII\n","TOKEN:N  --- pred:PII\n","TOKEN:atha  --- pred:PII\n","TOKEN:lie  --- pred:PII\n","TOKEN:S  --- pred:PII\n","TOKEN:ylla  --- pred:PII\n","TOKEN:N  --- pred:PII\n","TOKEN:atha  --- pred:PII\n","TOKEN:lie  --- pred:PII\n","TOKEN:S  --- pred:PII\n","TOKEN:ylla  --- pred:PII\n","TOKEN:Ela  --- pred:PII\n","TOKEN:dio  --- pred:PII\n","TOKEN:Amaya  --- pred:PII\n","next\n","TOKEN:Francisco  --- pred:PII\n","TOKEN:Ferreira  --- pred:PII\n","TOKEN:Stefano  --- pred:PII\n","TOKEN:Lovato  --- pred:PII\n","TOKEN:Igor  --- pred:PII\n","next\n"]}],"source":["learner.fit(lr=parameter['lr']*0.01, epochs=6)\n","inference(learner.model)"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"340fbe25870241d3bd5b97247ab3daab","version_major":2,"version_minor":0},"text/plain":["Training:   0%|          | 0/3012 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mlearner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameter\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m inference(learner\u001b[38;5;241m.\u001b[39mmodel)\n","Cell \u001b[1;32mIn[10], line 53\u001b[0m, in \u001b[0;36mLearner.fit\u001b[1;34m(self, lr, epochs)\u001b[0m\n\u001b[0;32m     51\u001b[0m pred\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(ids, att_mask, labels)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# reshape pred to [batch_size, num_classes, sequence_length]\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m metrics\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_beta_score_multiclass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_f_beta_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mlog(metrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf_beta\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_precision\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mlog(metrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecision\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n","Cell \u001b[1;32mIn[10], line 133\u001b[0m, in \u001b[0;36mLearner.f_beta_score_multiclass\u001b[1;34m(self, y_true, y_pred, beta, epsilon)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mf_beta_score_multiclass\u001b[39m(\u001b[38;5;28mself\u001b[39m, y_true, y_pred, beta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-7\u001b[39m):\n\u001b[0;32m    131\u001b[0m \n\u001b[0;32m    132\u001b[0m     \u001b[38;5;66;03m# assert y_pred has values between 0 and 1\u001b[39;00m\n\u001b[1;32m--> 133\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[43my_pred\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m y_pred\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    136\u001b[0m     y_true_one_hot \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mone_hot(y_true, num_classes\u001b[38;5;241m=\u001b[39my_pred\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m])\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["learner.fit(lr=parameter['lr'], epochs=12)\n","inference(learner.model)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b41979d088434768a8ae41fc88fffe28","version_major":2,"version_minor":0},"text/plain":["Training:   0%|          | 0/950 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[74], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mlearner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameter\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n","Cell \u001b[1;32mIn[66], line 42\u001b[0m, in \u001b[0;36mLearner.fit\u001b[1;34m(self, lr, epochs)\u001b[0m\n\u001b[0;32m     40\u001b[0m pred\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(ids, att_mask, labels)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# reshape pred to [batch_size, num_classes, sequence_length]\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m metrics\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_beta_score_multiclass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_f_beta_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mlog(metrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf_beta\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_precision\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mlog(metrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecision\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n","Cell \u001b[1;32mIn[66], line 104\u001b[0m, in \u001b[0;36mLearner.f_beta_score_multiclass\u001b[1;34m(self, y_true, y_pred, beta, epsilon)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mf_beta_score_multiclass\u001b[39m(\u001b[38;5;28mself\u001b[39m, y_true, y_pred, beta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-7\u001b[39m):\n\u001b[0;32m    102\u001b[0m \n\u001b[0;32m    103\u001b[0m     \u001b[38;5;66;03m# assert y_pred has values between 0 and 1\u001b[39;00m\n\u001b[1;32m--> 104\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[43my_pred\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m y_pred\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    107\u001b[0m     y_true_one_hot \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mone_hot(y_true, num_classes\u001b[38;5;241m=\u001b[39my_pred\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m])\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["learner.fit(lr=parameter['lr'], epochs=12)\n","inference(learner.model)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"name":"stdout","output_type":"stream","text":["TOKEN:[SEP]  --- pred:I-PHONE_NUM\n","next\n","TOKEN:[SEP]  --- pred:I-PHONE_NUM\n","next\n"]}],"source":["learner.fit(lr=parameter['lr'], epochs=12)\n","inference(learner.model)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"name":"stdout","output_type":"stream","text":["next\n"]}],"source":["inference(learner.model)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ee40a29fa88d4aef8597a481b4e7a126","version_major":2,"version_minor":0},"text/plain":["Training:   0%|          | 0/950 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["learner.fit(lr=parameter['lr'], epochs=50)\n","inference(learner.model)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["learner.fit(lr=parameter['lr'], epochs=50)\n","inference(learner.model)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":7500999,"sourceId":66653,"sourceType":"competition"},{"datasetId":4319117,"sourceId":7429898,"sourceType":"datasetVersion"}],"dockerImageVersionId":30636,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
