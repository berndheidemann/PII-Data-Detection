{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-01-25T21:37:35.578114Z","iopub.status.busy":"2024-01-25T21:37:35.577723Z","iopub.status.idle":"2024-01-25T21:37:35.587366Z","shell.execute_reply":"2024-01-25T21:37:35.5858Z","shell.execute_reply.started":"2024-01-25T21:37:35.578083Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["{'model': 'microsoft/deberta-v3-large', 'max_length': 512, 'batch_size': 5, 'lr': 0.001, 'filter_no_pii_percent_allow': 0.1, 'notebook': '11_weighted loss.ipynb', 'CROSS_ENTROPY_WEIGHT_MULTI': 200}\n"]}],"source":["# import Path\n","\n","from pathlib import Path\n","import numpy as np\n","import torch\n","\n","cross_entropy_weight_multi = 200\n","\n","CROSS_ENTROPY_WEIGHTS = [cross_entropy_weight_multi]*12\n","CROSS_ENTROPY_WEIGHTS.append(1)\n","\n","parameter= {\n","    \"model\": \"microsoft/deberta-v3-large\",\n","    \"max_length\": 512,\n","    \"batch_size\": 5,\n","    \"lr\": 1e-3,\n","    \"filter_no_pii_percent_allow\": 0.1,\n","    \"notebook\": \"11_weighted loss.ipynb\",\n","    \"CROSS_ENTROPY_WEIGHT_MULTI\": cross_entropy_weight_multi\n","}\n","\n","print(parameter)"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-01-25T21:37:35.590432Z","iopub.status.busy":"2024-01-25T21:37:35.589758Z","iopub.status.idle":"2024-01-25T21:37:35.604681Z","shell.execute_reply":"2024-01-25T21:37:35.603633Z","shell.execute_reply.started":"2024-01-25T21:37:35.590393Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["6807\n","dict_keys(['document', 'full_text', 'tokens', 'trailing_whitespace', 'labels'])\n","['Design', 'Thinking', 'for', 'innovation', 'reflexion', '-', 'Avril', '2021', '-', 'Nathalie']\n","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NAME_STUDENT']\n","[True, True, True, True, False, False, True, False, False, True]\n"]}],"source":["\n","import json\n","\n","data = json.load(open(\"data/train.json\"))\n","\n","print(len(data))\n","print(data[0].keys())\n","\n","x = data[0]\n","\n","print(x[\"tokens\"][:10])\n","print(x[\"labels\"][:10])\n","print(x[\"trailing_whitespace\"][:10])"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"data":{"text/plain":["dict_keys(['document', 'full_text', 'tokens', 'trailing_whitespace', 'labels'])"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["x.keys()"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["target = [\n","    'B-EMAIL', 'B-ID_NUM', 'B-NAME_STUDENT', 'B-PHONE_NUM', \n","    'B-STREET_ADDRESS', 'B-URL_PERSONAL', 'B-USERNAME', 'I-ID_NUM', \n","    'I-NAME_STUDENT', 'I-PHONE_NUM', 'I-STREET_ADDRESS', 'I-URL_PERSONAL'\n","]"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-01-25T21:37:35.606208Z","iopub.status.busy":"2024-01-25T21:37:35.605889Z","iopub.status.idle":"2024-01-25T21:37:35.62164Z","shell.execute_reply":"2024-01-25T21:37:35.620746Z","shell.execute_reply.started":"2024-01-25T21:37:35.606175Z"},"trusted":true},"outputs":[{"data":{"text/plain":["{0: 'B-EMAIL',\n"," 1: 'B-ID_NUM',\n"," 2: 'B-NAME_STUDENT',\n"," 3: 'B-PHONE_NUM',\n"," 4: 'B-STREET_ADDRESS',\n"," 5: 'B-URL_PERSONAL',\n"," 6: 'B-USERNAME',\n"," 7: 'I-ID_NUM',\n"," 8: 'I-NAME_STUDENT',\n"," 9: 'I-PHONE_NUM',\n"," 10: 'I-STREET_ADDRESS',\n"," 11: 'I-URL_PERSONAL',\n"," 12: 'O'}"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["\n","from itertools import chain\n","\n","all_labels = sorted(list(set(chain(*[x[\"labels\"] for x in data]))))\n","label2id = {l: i for i,l in enumerate(all_labels)}\n","id2label = {v:k for k,v in label2id.items()}\n","\n","id2label"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["import random\n","\n","def tokenize(example, tokenizer, label2id, max_length):\n","    text = []\n","\n","    # these are at the character level\n","    labels = []\n","    targets = []\n","\n","    for t, l, ws in zip(example[\"tokens\"], example[\"labels\"], example[\"trailing_whitespace\"]):\n","\n","        text.append(t)\n","        labels.extend([l]*len(t))\n","        \n","        if l in target:\n","            targets.append(1)\n","        else:\n","            targets.append(0)\n","        # if there is trailing whitespace\n","        if ws:\n","            text.append(\" \")\n","            labels.append(\"O\")\n","\n","    tokenized = tokenizer(\"\".join(text), return_offsets_mapping=True, truncation=True, max_length=max_length, padding=\"max_length\")\n","    \n","    target_num = sum(targets)\n","    labels = np.array(labels)\n","\n","    text = \"\".join(text)\n","    token_labels = []\n","\n","    for start_idx, end_idx in tokenized.offset_mapping:\n","\n","        # CLS token\n","        if start_idx == 0 and end_idx == 0: \n","            token_labels.append(label2id[\"O\"])\n","            continue\n","\n","        # case when token starts with whitespace\n","        if text[start_idx].isspace():\n","            start_idx += 1\n","\n","        try:\n","            token_labels.append(label2id[labels[start_idx]])\n","        except:\n","            token_labels.append(label2id[\"O\"])\n","\n","    length = len(tokenized.input_ids)\n","\n","    return {\n","        **tokenized,\n","        \"labels\": token_labels,\n","        \"length\": length,\n","        \"target_num\": target_num,\n","        \"group\": 1 if target_num>0 else 0\n","    }\n","\n","# https://www.kaggle.com/competitions/pii-detection-removal-from-educational-data/discussion/468844\n","def filter_no_pii(example, percent_allow=parameter[\"filter_no_pii_percent_allow\"]):\n","    # Return True if there is PII\n","    # Or 20% of the time if there isn't\n","    has_pii = set(\"O\") != set(example[\"labels\"])\n","    return has_pii or (random.random() < percent_allow)\n","    \n","\n"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","c:\\Users\\Bernd\\anaconda3\\envs\\mytorch\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:470: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n","  warnings.warn(\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["import torch\n","import json\n","\n","class PiiDataset(torch.utils.data.Dataset):\n","    def __init__(self, dataset, tokenizer, label2id, max_length):\n","        self.dataset = dataset\n","        self.tokenizer = tokenizer\n","        self.label2id = label2id\n","        self.max_length = max_length\n","        \n","    def __getitem__(self, idx):\n","        vals=tokenize(self.dataset[idx], self.tokenizer, self.label2id, self.max_length)\n","\n","        input_ids = torch.tensor(vals[\"input_ids\"])\n","        attention_mask = torch.tensor(vals[\"attention_mask\"])\n","        labels = torch.tensor(vals[\"labels\"], dtype=torch.long)\n","\n","        return input_ids, attention_mask, labels\n","    \n","    def __len__(self):\n","        return len(self.dataset)\n","    \n","\n","data = json.load(open(\"data/train.json\"))\n","\n","\n","from transformers import AutoTokenizer\n","\n","\n","\n","tokenizer = AutoTokenizer.from_pretrained(parameter[\"model\"])\n","\n","my_dataset=PiiDataset(data, tokenizer, label2id, parameter[\"max_length\"])\n","\n","\n"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([8, 512])\n","torch.Size([8, 512])\n","torch.Size([8, 512])\n"]}],"source":["\n","loader=torch.utils.data.DataLoader(my_dataset, batch_size=8, shuffle=True)\n","\n","for id, attention_mask, labels in loader:\n","    print(id.shape)\n","    print(attention_mask.shape)\n","    print(labels.shape)\n","    break"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["still learning classifier.weight parameter_size: torch.Size([13, 1024])\n","still learning classifier.bias parameter_size: torch.Size([13])\n","torch.Size([8, 512])\n","torch.Size([8, 512])\n","torch.Size([8, 512])\n","tensor([[12, 12, 12,  ..., 12, 12, 12],\n","        [12, 12, 12,  ..., 12, 12, 12],\n","        [12, 12, 12,  ..., 12, 12, 12],\n","        ...,\n","        [12, 12, 12,  ..., 12, 12, 12],\n","        [12, 12, 12,  ..., 12, 12, 12],\n","        [12, 12, 12,  ..., 12, 12, 12]])\n","torch.Size([8, 512, 13])\n"]}],"source":["from transformers import AutoModelForTokenClassification\n","\n","device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","class MyModel(torch.nn.Module):\n","    def __init__(self, model_name, num_labels, dropout_p=0.4):\n","        super().__init__()\n","        self.model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=num_labels)\n","        self.softmax=torch.nn.Softmax(dim=-1)\n","        self.freeze()\n","\n","    def freeze(self):\n","        # freeze all parameters\n","        for param in self.model.parameters():\n","            param.requires_grad = False\n","        # unfreeze classifier layer\n","        for param in self.model.classifier.parameters():\n","            param.requires_grad = True\n","        for name, param in self.model.named_parameters():\n","            if param.requires_grad==True:\n","                print(\"still learning\", name, \"parameter_size:\", param.size())\n","\n","    def unfreeze(self):\n","        for param in self.model.parameters():\n","            param.requires_grad = True\n","        \n","    def forward(self, input_ids, attention_mask, labels=None):\n","        if labels is not None:\n","            out=self.model(input_ids, attention_mask=attention_mask, labels=labels)['logits']\n","        else:\n","            out=self.model(input_ids, attention_mask=attention_mask)['logits']\n","        out=self.softmax(out)\n","        return out\n","\n","\n","model = MyModel(parameter['model'], len(label2id))\n","\n","model= model.to(device)\n","for id, attention_mask, labels in loader:\n","    print(id.shape)\n","    print(attention_mask.shape)\n","    print(labels.shape)\n","    print(labels)\n","    id = id.to(device)\n","    attention_mask = attention_mask.to(device)\n","    labels = labels.to(device)\n","    print(model(id, attention_mask, labels).shape)\n","    break\n","\n","#free gpu memory\n","del model\n","torch.cuda.empty_cache()\n","\n"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["import neptune\n","from tqdm.notebook import tqdm\n","\n","\n","# import CosineAnnealingWarmRestarts\n","from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n","\n","\n","class Learner():\n","    def __init__(self, model, train_dataloader, valid_dataloader, parameter=None):\n","        self.model=model\n","        #self.loss_fn=torch.nn.CrossEntropyLoss()\n","        self.loss_fn=torch.nn.CrossEntropyLoss(ignore_index=-100, weight=torch.tensor(CROSS_ENTROPY_WEIGHTS, dtype=torch.float32).to(device))\n","        self.device=torch.device(\"cpu\")\n","        if torch.cuda.is_available():\n","            self.device=torch.device(\"cuda\")\n","        #elif torch.backends.mps.is_available():\n","        #    self.device=torch.device(\"mps\")\n","\n","        self.model.to(self.device)\n","        self.run = neptune.init_run(\n","            project=\"bernd.heidemann/PII\",\n","            api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIzNjBlYzVkNi0zZTUwLTQ1ODYtODhlNC02NDUxNDg0MDdjNzUifQ==\",\n","        )  # your credentials\n","        self.parameter = parameter\n","        self.train_dataloader = train_dataloader\n","        self.valid_dataloader = valid_dataloader\n","        self.run[\"parameters\"] = {\n","            **self.parameter\n","        }\n","        self.non_pii_label=label2id[\"O\"]\n","\n","    def fit(self, lr=0.1, epochs=10):\n","        \n","        optimizer=torch.optim.AdamW(self.model.parameters(), lr=lr)\n","        T_0 = epochs//3          # Number of epochs before the first restart\n","        T_mult = 2        # Factor by which T_0 is multiplied after each restart\n","        eta_min = lr*0.01   # Minimum learning rate at restarts\n","\n","        # Create the CosineAnnealingWarmRestarts scheduler\n","        scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=T_0, T_mult=T_mult, eta_min=eta_min)\n","        bar = tqdm(total=len(self.train_dataloader) * epochs, desc=\"Training\")\n","        bar.set_description(\"Epoch 0/{}\".format(epochs))\n","        for epoch in range(epochs):\n","            self.model.train()     \n","            pii_count=0       \n","            for ids, att_mask, labels in self.train_dataloader:\n","                ids=ids.to(self.device)\n","                labels=labels.to(self.device)\n","                att_mask=att_mask.to(self.device)\n","                pred=self.model(ids, att_mask, labels)\n","                # reshape pred to [batch_size, num_classes, sequence_length]\n","                metrics=self.f_beta_score_multiclass(labels, pred)\n","                self.run[\"train_f_beta_score\"].log(metrics[\"f_beta\"])\n","                self.run[\"train_precision\"].log(metrics[\"precision\"])\n","                self.run[\"train_recall\"].log(metrics[\"recall\"])\n","                self.run[\"train_true_positives\"].log(metrics[\"true_positives\"])\n","                self.run[\"train_false_positives\"].log(metrics[\"false_positives\"])\n","                self.run[\"train_false_negatives\"].log(metrics[\"false_negatives\"])\n","                pred = pred.permute(0, 2, 1)\n","                loss=self.loss_fn(pred, labels)\n","                self.run[\"train_loss\"].log(loss.item())\n","                loss.backward()\n","                optimizer.step()\n","                optimizer.zero_grad()\n","                bar.update(1)\n","                #count all predictions that do not have the label \"O\"\n","                pii_count_batch = torch.sum(torch.argmax(pred, dim=1) != self.non_pii_label).item()\n","                pii_count += pii_count_batch\n","            self.run[\"train_pii_count\"].log(pii_count)\n","            self.run[\"learnrate\"].log(optimizer.param_groups[0][\"lr\"])\n","            scheduler.step()\n","            self.model.eval()\n","            # log current state to neptune\n","            if self.valid_dataloader is not None:\n","                metrics=self.get_accuracy()\n","                self.run[\"valid_accuracy\"].log(metrics[\"accuracy\"])\n","                self.run[\"valid_loss\"].log(metrics[\"loss\"])\n","                self.run[\"valid_f_beta_score\"].log(metrics[\"f_beta_score\"])\n","                self.run[\"valid_precision\"].log(metrics[\"precision\"])\n","                self.run[\"valid_recall\"].log(metrics[\"recall\"])\n","                self.run[\"valid_true_positives\"].log(metrics[\"true_positives\"])\n","                self.run[\"valid_false_positives\"].log(metrics[\"false_positives\"])\n","                self.run[\"valid_false_negatives\"].log(metrics[\"false_negatives\"])\n","                self.run[\"valid_pii_count\"].log(metrics[\"pii_count\"])\n","                bar.set_description(\"Epoch {}/{} validAccuracy: {:.2f} validLoss: {:.2f}\".format(epoch+1, epochs, metrics[\"accuracy\"], metrics[\"loss\"]))\n","\n","                \n","    def get_accuracy(self):\n","        self.model.eval()\n","        with torch.no_grad():\n","            correct=0\n","            losses=[]\n","            batch_metrics=[]\n","            pii_count=0       \n","            for ids, att_mask, labels in self.valid_dataloader:\n","                ids=ids.to(self.device)\n","                labels=labels.to(self.device)\n","                att_mask=att_mask.to(self.device)\n","                pred=self.model(ids, att_mask, labels)\n","                f_beta_score_results = self.f_beta_score_multiclass(labels, pred)\n","                batch_metrics.append(f_beta_score_results)\n","                pred = pred.permute(0, 2, 1)\n","                loss=self.loss_fn(pred, labels)\n","                losses.append(loss.item())\n","                pred=torch.argmax(pred, dim=1)\n","                correct+=torch.sum(pred==labels).item()\n","                pii_count_batch = torch.sum((pred != self.non_pii_label)).item()\n","                pii_count += pii_count_batch\n","            # calc mean of the dict entries in batch_metrics\n","            f_beta_scores = np.mean([x[\"f_beta\"] for x in batch_metrics])\n","            precision = np.mean([x[\"precision\"] for x in batch_metrics])\n","            recall = np.mean([x[\"recall\"] for x in batch_metrics])\n","            true_positives = np.mean([x[\"true_positives\"] for x in batch_metrics])\n","            false_positives = np.mean([x[\"false_positives\"] for x in batch_metrics])\n","            false_negatives = np.mean([x[\"false_negatives\"] for x in batch_metrics])\n","\n","            return {\n","                \"accuracy\": correct/len(self.valid_dataloader.dataset),\n","                \"loss\": np.mean(losses),\n","                \"f_beta_score\": f_beta_scores,\n","                \"precision\": precision,\n","                \"recall\": recall,\n","                \"true_positives\": true_positives,\n","                \"false_positives\": false_positives,\n","                \"false_negatives\": false_negatives,\n","                 \"pii_count\": pii_count\n","            }\n","        \n","    def f_beta_score_multiclass(self, y_true, y_pred, beta=5, epsilon=1e-7):\n","\n","        # assert y_pred has values between 0 and 1\n","        assert y_pred.min() >= 0\n","        assert y_pred.max() <= 1\n","\n","        y_true_one_hot = torch.nn.functional.one_hot(y_true, num_classes=y_pred.shape[2])\n","    \n","        # Berechnung von True Positives, False Positives und False Negatives\n","        tp = torch.sum(y_true_one_hot * y_pred, dim=0)\n","        fp = torch.sum((1 - y_true_one_hot) * y_pred, dim=0)\n","        fn = torch.sum(y_true_one_hot * (1 - y_pred), dim=0)\n","\n","        # Summierung über alle Klassen\n","        tp_sum = torch.sum(tp)\n","        fp_sum = torch.sum(fp)\n","        fn_sum = torch.sum(fn)\n","\n","        # Berechnung von Präzision und Recall\n","        precision = tp_sum / (tp_sum + fp_sum + epsilon)\n","        recall = tp_sum / (tp_sum + fn_sum + epsilon)\n","        # Berechnung des F-Beta-Scores\n","        f_beta = (1 + beta**2) * (precision * recall) / ((beta**2 * precision) + recall + epsilon)\n","\n","        return {\n","            \"f_beta\": f_beta.item(),\n","            \"precision\": precision.item(),\n","            \"recall\": recall.item(),\n","            \"true_positives\": tp_sum.item(),\n","            \"false_positives\": fp_sum.item(),\n","            \"false_negatives\": fn_sum.item()\n","        }\n","\n","    "]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["def inference(model):\n","    import torch\n","    from transformers import AutoTokenizer\n","    from transformers import AutoModelForTokenClassification\n","    import json\n","\n","\n","    parameter_inference= {\n","        \"max_length\": 512,\n","        \"batch_size\": 8,\n","        \"model_path\": \"../PII Models/model_nb_04.pt\",\n","        \"model\": \"microsoft/deberta-v3-base\"\n","    }\n","\n","    def tokenize(example, tokenizer, max_length):\n","        text = []\n","        for t,  ws in zip(example[\"tokens\"], example[\"trailing_whitespace\"]):\n","            text.append(t)\n","            if ws:\n","                text.append(\" \")\n","        tokenized = tokenizer(\"\".join(text), return_offsets_mapping=True, truncation=True, max_length=max_length, padding=\"max_length\")\n","        text = \"\".join(text)\n","        length = len(tokenized.input_ids)\n","        return {\n","            **tokenized,\n","            \"length\": length,\n","        }\n","\n","    class PiiDatasetInference(torch.utils.data.Dataset):\n","        def __init__(self, dataset, tokenizer, max_length):\n","            self.dataset = dataset\n","            self.tokenizer = tokenizer\n","            self.max_length = max_length\n","            \n","        def __getitem__(self, idx):\n","            vals=tokenize(self.dataset[idx], self.tokenizer, self.max_length)\n","            input_ids = torch.tensor(vals[\"input_ids\"])\n","            attention_mask = torch.tensor(vals[\"attention_mask\"])\n","            return input_ids, attention_mask\n","        \n","        def __len__(self):\n","            return len(self.dataset)\n","        \n","    data = json.load(open(\"data/train.json\")) # foo\n","    from itertools import chain\n","    all_labels = sorted(list(set(chain(*[x[\"labels\"] for x in data]))))\n","    label2id = {l: i for i,l in enumerate(all_labels)}\n","    id2label = {v:k for k,v in label2id.items()}\n","\n","    tokenizer = AutoTokenizer.from_pretrained(parameter_inference[\"model\"])\n","    data = json.load(open(\"data/test.json\"))\n","    my_dataset=PiiDatasetInference(data, tokenizer, parameter_inference[\"max_length\"])\n","    loader=torch.utils.data.DataLoader(my_dataset, batch_size=parameter_inference['batch_size'], shuffle=True)\n","    for id, attention_mask in loader:\n","        id = id.to(device)\n","        attention_mask = attention_mask.to(device)\n","        preds=model(id, attention_mask).argmax(dim=2)\n","\n","        for pred, id in zip(preds.flatten(), id.flatten()):\n","            if pred != 12:\n","                print(f\"TOKEN:{tokenizer.decode(id)}  --- pred:{id2label[pred.item()]}\")\n","        print(\"next\")"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["original len 6807\n","filtered len 1485\n","len train ds 1188\n","len valid ds 297\n"]}],"source":["data = json.load(open(\"data/train.json\"))\n","\n","print(\"original len\",  len(data))\n","# this increased the \n","data_filterd = list(filter(filter_no_pii, data))\n","print(\"filtered len\", len(data_filterd))\n","\n","data_len=len(data_filterd)\n","\n","train_len=int(len(data_filterd)*0.8)\n","valid_len=len(data_filterd)-train_len\n","\n","train_data_idx=np.random.choice(data_len, train_len, replace=False)\n","valid_data_idx=np.array(list(set(range(data_len))-set(train_data_idx)))\n","\n","train_data=[data_filterd[i] for i in train_data_idx]\n","valid_data=[data_filterd[i] for i in valid_data_idx]\n","\n","print(\"len train ds\", len(train_data))\n","print(\"len valid ds\", len(valid_data))"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","c:\\Users\\Bernd\\anaconda3\\envs\\mytorch\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:470: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n","  warnings.warn(\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","c:\\Users\\Bernd\\anaconda3\\envs\\mytorch\\lib\\site-packages\\neptune\\common\\warnings.py:71: NeptuneWarning: The following monitoring options are disabled by default in interactive sessions: 'capture_stdout', 'capture_stderr', 'capture_traceback', and 'capture_hardware_metrics'. To enable them, set each parameter to 'True' when initializing the run. The monitoring will continue until you call run.stop() or the kernel stops. Also note: Your source files can only be tracked if you pass the path(s) to the 'source_code' argument. For help, see the Neptune docs: https://docs.neptune.ai/logging/source_code/\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["still learning classifier.weight parameter_size: torch.Size([13, 1024])\n","still learning classifier.bias parameter_size: torch.Size([13])\n","https://app.neptune.ai/bernd.heidemann/PII/e/PII-127\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c710d72533964358b0068c2fa949c861","version_major":2,"version_minor":0},"text/plain":["Training:   0%|          | 0/2856 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from tokenizers import AddedToken\n","\n","# set environment variables: TOKENIZERS_PARALLELISM=false\n","import os\n","os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n","\n","tokenizer = AutoTokenizer.from_pretrained(parameter[\"model\"])\n","tokenizer.add_tokens(AddedToken(\"\\n\", normalized=False))\n","\n","train_dataset = PiiDataset(train_data, tokenizer, label2id, parameter[\"max_length\"])\n","valid_dataset = PiiDataset(valid_data, tokenizer, label2id, parameter[\"max_length\"])\n","train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=parameter['batch_size'], shuffle=True)\n","valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=parameter['batch_size'], shuffle=False)\n","my_model=MyModel(parameter['model'], len(label2id))\n","\n","learner=Learner(my_model, train_dataloader, valid_dataloader, parameter=parameter)\n","learner.fit(lr=parameter['lr'], epochs=12)\n","\n","\n"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["learner.model.unfreeze()"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5a8a6b2ef72a45bd9d19afb47b3875c9","version_major":2,"version_minor":0},"text/plain":["Training:   0%|          | 0/2856 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","c:\\Users\\Bernd\\anaconda3\\envs\\mytorch\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:470: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n","  warnings.warn(\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"name":"stdout","output_type":"stream","text":["TOKEN:N  --- pred:B-NAME_STUDENT\n","TOKEN:atha  --- pred:B-NAME_STUDENT\n","TOKEN:lie  --- pred:B-NAME_STUDENT\n","TOKEN:S  --- pred:I-NAME_STUDENT\n","TOKEN:ylla  --- pred:I-NAME_STUDENT\n","TOKEN:N  --- pred:B-NAME_STUDENT\n","TOKEN:atha  --- pred:B-NAME_STUDENT\n","TOKEN:lie  --- pred:B-NAME_STUDENT\n","TOKEN:S  --- pred:I-NAME_STUDENT\n","TOKEN:ylla  --- pred:I-NAME_STUDENT\n","TOKEN:Gilberto  --- pred:B-NAME_STUDENT\n","TOKEN:Gamb  --- pred:I-NAME_STUDENT\n","TOKEN:oa  --- pred:I-NAME_STUDENT\n","TOKEN:Ela  --- pred:B-NAME_STUDENT\n","TOKEN:dio  --- pred:B-NAME_STUDENT\n","TOKEN:Amaya  --- pred:I-NAME_STUDENT\n","TOKEN:Saki  --- pred:B-NAME_STUDENT\n","TOKEN:r  --- pred:B-NAME_STUDENT\n","TOKEN:Ahmad  --- pred:I-NAME_STUDENT\n","TOKEN:Sind  --- pred:B-NAME_STUDENT\n","TOKEN:y  --- pred:B-NAME_STUDENT\n","TOKEN:Sam  --- pred:I-NAME_STUDENT\n","TOKEN:aca  --- pred:I-NAME_STUDENT\n","TOKEN:Gita  --- pred:I-NAME_STUDENT\n","TOKEN:m  --- pred:I-NAME_STUDENT\n","TOKEN:University  --- pred:I-NAME_STUDENT\n","TOKEN:George  --- pred:B-NAME_STUDENT\n","TOKEN:Geoff  --- pred:B-NAME_STUDENT\n","TOKEN:Nadine  --- pred:B-NAME_STUDENT\n","TOKEN:Born  --- pred:I-NAME_STUDENT\n","TOKEN:Diego  --- pred:B-NAME_STUDENT\n","TOKEN:Estrada  --- pred:I-NAME_STUDENT\n","TOKEN:Diego  --- pred:B-NAME_STUDENT\n","TOKEN:Estrada  --- pred:I-NAME_STUDENT\n","TOKEN:Silvia  --- pred:B-NAME_STUDENT\n","TOKEN:Villa  --- pred:I-NAME_STUDENT\n","TOKEN:lobo  --- pred:I-NAME_STUDENT\n","TOKEN:s  --- pred:I-NAME_STUDENT\n","next\n","TOKEN:Stefano  --- pred:B-NAME_STUDENT\n","TOKEN:Lovato  --- pred:I-NAME_STUDENT\n","TOKEN:Gera  --- pred:B-NAME_STUDENT\n","TOKEN:shchenko  --- pred:B-NAME_STUDENT\n","TOKEN:Igor  --- pred:I-NAME_STUDENT\n","TOKEN:Alexander  --- pred:B-NAME_STUDENT\n","TOKEN:Sh  --- pred:I-NAME_STUDENT\n","TOKEN:m  --- pred:I-NAME_STUDENT\n","TOKEN:akov  --- pred:I-NAME_STUDENT\n","TOKEN:Francisco  --- pred:B-NAME_STUDENT\n","TOKEN:Ferreira  --- pred:I-NAME_STUDENT\n","next\n"]},{"ename":"TypeError","evalue":"unsupported operand type(s) for +: 'int' and 'str'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[17], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m learner\u001b[38;5;241m.\u001b[39mfit(lr\u001b[38;5;241m=\u001b[39mparameter[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m0.01\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m)\n\u001b[0;32m      3\u001b[0m inference(learner\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[1;32m----> 4\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(learner\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_nb_12_unfreeze.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m)\n","\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'int' and 'str'"]}],"source":["for i in range(10):\n","    learner.fit(lr=parameter['lr']*0.01, epochs=12)\n","    inference(learner.model)\n","    torch.save(learner.model.state_dict(), str(i)+\"model_nb_12_unfreeze.pt\")"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["#save model\n","\n","torch.save(learner.model.state_dict(), \"model_nb_12_unfreeze.pt\")"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["torch.save(learner.model.state_dict(), str(1)+\"model_nb_12_unfreeze.pt\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":7500999,"sourceId":66653,"sourceType":"competition"},{"datasetId":4319117,"sourceId":7429898,"sourceType":"datasetVersion"}],"dockerImageVersionId":30636,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
