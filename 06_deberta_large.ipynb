{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-01-25T21:37:35.578114Z","iopub.status.busy":"2024-01-25T21:37:35.577723Z","iopub.status.idle":"2024-01-25T21:37:35.587366Z","shell.execute_reply":"2024-01-25T21:37:35.5858Z","shell.execute_reply.started":"2024-01-25T21:37:35.578083Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["{'model': 'microsoft/deberta-v3-large', 'max_length': 512, 'batch_size': 8, 'lr': 1e-05, 'filter_no_pii_percent_allow': 0.2, 'freeze_embeddings': True, 'freeze_encoder_num': 6, 'notebook': '06_deberta_large.ipynb'}\n"]}],"source":["# import Path\n","\n","from pathlib import Path\n","\n","\n","parameter= {\n","    \"model\": \"microsoft/deberta-v3-large\",\n","    \"max_length\": 512,\n","    \"batch_size\": 8,\n","    \"lr\": 1e-5,\n","    \"filter_no_pii_percent_allow\": 0.2,\n","    \"freeze_embeddings\": True,\n","    \"freeze_encoder_num\": 6,\n","    \"notebook\": \"06_deberta_large.ipynb\",\n","}\n","\n","\n","print(parameter)"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-01-25T21:37:35.590432Z","iopub.status.busy":"2024-01-25T21:37:35.589758Z","iopub.status.idle":"2024-01-25T21:37:35.604681Z","shell.execute_reply":"2024-01-25T21:37:35.603633Z","shell.execute_reply.started":"2024-01-25T21:37:35.590393Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["6807\n","dict_keys(['document', 'full_text', 'tokens', 'trailing_whitespace', 'labels'])\n","['Design', 'Thinking', 'for', 'innovation', 'reflexion', '-', 'Avril', '2021', '-', 'Nathalie']\n","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NAME_STUDENT']\n","[True, True, True, True, False, False, True, False, False, True]\n"]}],"source":["\n","import json\n","import numpy as np\n","\n","data = json.load(open(\"data/train.json\"))\n","\n","print(len(data))\n","print(data[0].keys())\n","\n","x = data[0]\n","\n","print(x[\"tokens\"][:10])\n","print(x[\"labels\"][:10])\n","print(x[\"trailing_whitespace\"][:10])"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"data":{"text/plain":["dict_keys(['document', 'full_text', 'tokens', 'trailing_whitespace', 'labels'])"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["x.keys()"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["target = [\n","    'B-EMAIL', 'B-ID_NUM', 'B-NAME_STUDENT', 'B-PHONE_NUM', \n","    'B-STREET_ADDRESS', 'B-URL_PERSONAL', 'B-USERNAME', 'I-ID_NUM', \n","    'I-NAME_STUDENT', 'I-PHONE_NUM', 'I-STREET_ADDRESS', 'I-URL_PERSONAL'\n","]"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-01-25T21:37:35.606208Z","iopub.status.busy":"2024-01-25T21:37:35.605889Z","iopub.status.idle":"2024-01-25T21:37:35.62164Z","shell.execute_reply":"2024-01-25T21:37:35.620746Z","shell.execute_reply.started":"2024-01-25T21:37:35.606175Z"},"trusted":true},"outputs":[{"data":{"text/plain":["{0: 'B-EMAIL',\n"," 1: 'B-ID_NUM',\n"," 2: 'B-NAME_STUDENT',\n"," 3: 'B-PHONE_NUM',\n"," 4: 'B-STREET_ADDRESS',\n"," 5: 'B-URL_PERSONAL',\n"," 6: 'B-USERNAME',\n"," 7: 'I-ID_NUM',\n"," 8: 'I-NAME_STUDENT',\n"," 9: 'I-PHONE_NUM',\n"," 10: 'I-STREET_ADDRESS',\n"," 11: 'I-URL_PERSONAL',\n"," 12: 'O'}"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["\n","from itertools import chain\n","\n","all_labels = sorted(list(set(chain(*[x[\"labels\"] for x in data]))))\n","label2id = {l: i for i,l in enumerate(all_labels)}\n","id2label = {v:k for k,v in label2id.items()}\n","\n","id2label"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["import random\n","\n","def tokenize(example, tokenizer, label2id, max_length):\n","    text = []\n","\n","    # these are at the character level\n","    labels = []\n","    targets = []\n","\n","    for t, l, ws in zip(example[\"tokens\"], example[\"labels\"], example[\"trailing_whitespace\"]):\n","\n","        text.append(t)\n","        labels.extend([l]*len(t))\n","        \n","        if l in target:\n","            targets.append(1)\n","        else:\n","            targets.append(0)\n","        # if there is trailing whitespace\n","        if ws:\n","            text.append(\" \")\n","            labels.append(\"O\")\n","\n","    tokenized = tokenizer(\"\".join(text), return_offsets_mapping=True, truncation=True, max_length=max_length, padding=\"max_length\")\n","    \n","    target_num = sum(targets)\n","    labels = np.array(labels)\n","\n","    text = \"\".join(text)\n","    token_labels = []\n","\n","    for start_idx, end_idx in tokenized.offset_mapping:\n","\n","        # CLS token\n","        if start_idx == 0 and end_idx == 0: \n","            token_labels.append(label2id[\"O\"])\n","            continue\n","\n","        # case when token starts with whitespace\n","        if text[start_idx].isspace():\n","            start_idx += 1\n","\n","        try:\n","            token_labels.append(label2id[labels[start_idx]])\n","        except:\n","            token_labels.append(label2id[\"O\"])\n","\n","    length = len(tokenized.input_ids)\n","\n","    return {\n","        **tokenized,\n","        \"labels\": token_labels,\n","        \"length\": length,\n","        \"target_num\": target_num,\n","        \"group\": 1 if target_num>0 else 0\n","    }\n","\n","# https://www.kaggle.com/competitions/pii-detection-removal-from-educational-data/discussion/468844\n","def filter_no_pii(example, percent_allow=parameter[\"filter_no_pii_percent_allow\"]):\n","    # Return True if there is PII\n","    # Or 20% of the time if there isn't\n","    has_pii = set(\"O\") != set(example[\"labels\"])\n","    return has_pii or (random.random() < percent_allow)\n","    \n","\n"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","/opt/homebrew/Caskroom/miniconda/base/envs/torch_ds/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:454: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n","  warnings.warn(\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["import torch\n","import json\n","\n","class PiiDataset(torch.utils.data.Dataset):\n","    def __init__(self, dataset, tokenizer, label2id, max_length):\n","        self.dataset = dataset\n","        self.tokenizer = tokenizer\n","        self.label2id = label2id\n","        self.max_length = max_length\n","        \n","    def __getitem__(self, idx):\n","        vals=tokenize(self.dataset[idx], self.tokenizer, self.label2id, self.max_length)\n","\n","        input_ids = torch.tensor(vals[\"input_ids\"])\n","        attention_mask = torch.tensor(vals[\"attention_mask\"])\n","        labels = torch.tensor(vals[\"labels\"], dtype=torch.long)\n","\n","        return input_ids, attention_mask, labels\n","    \n","    def __len__(self):\n","        return len(self.dataset)\n","    \n","\n","data = json.load(open(\"data/train.json\"))\n","\n","\n","from transformers import AutoTokenizer\n","\n","tokenizer = AutoTokenizer.from_pretrained(parameter[\"model\"])\n","\n","my_dataset=PiiDataset(data, tokenizer, label2id, parameter[\"max_length\"])\n"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([8, 512])\n","torch.Size([8, 512])\n","torch.Size([8, 512])\n"]}],"source":["\n","loader=torch.utils.data.DataLoader(my_dataset, batch_size=8, shuffle=True)\n","\n","for id, attention_mask, labels in loader:\n","    print(id.shape)\n","    print(attention_mask.shape)\n","    print(labels.shape)\n","    break"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2ForTokenClassification: ['mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.classifier.weight', 'deberta.embeddings.position_embeddings.weight', 'lm_predictions.lm_head.dense.bias']\n","- This IS expected if you are initializing DebertaV2ForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2ForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["torch.Size([8, 512])\n","torch.Size([8, 512])\n","torch.Size([8, 512])\n","tensor([[12, 12, 12,  ..., 12, 12, 12],\n","        [12, 12, 12,  ..., 12, 12, 12],\n","        [12, 12, 12,  ..., 12, 12, 12],\n","        ...,\n","        [12, 12, 12,  ..., 12, 12, 12],\n","        [12, 12, 12,  ..., 12, 12, 12],\n","        [12, 12, 12,  ..., 12, 12, 12]])\n","torch.Size([8, 512, 13])\n"]}],"source":["from transformers import AutoModelForTokenClassification\n","\n","device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","class MyModel(torch.nn.Module):\n","    def __init__(self, model_name, num_labels):\n","        super().__init__()\n","        self.model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=num_labels)\n","        self.softmax=torch.nn.Softmax(dim=-1)\n","        self.freeze()\n","\n","    def freeze(self):\n","        if parameter[\"freeze_embeddings\"]:\n","            for param in self.model.get_input_embeddings().parameters():\n","                param.requires_grad = False\n","        \n","        for layer in self.model.deberta.encoder.layer:\n","            for param in layer.parameters():\n","                param.requires_grad = False\n","        \n","    def forward(self, input_ids, attention_mask, labels=None):\n","        if labels is not None:\n","            out=self.model(input_ids, attention_mask=attention_mask, labels=labels)['logits']\n","        else:\n","            out=self.model(input_ids, attention_mask=attention_mask)['logits']\n","        out=self.softmax(out)\n","        return out\n","\n","\n","model = MyModel(parameter['model'], len(label2id))\n","\n","model= model.to(device)\n","for id, attention_mask, labels in loader:\n","    print(id.shape)\n","    print(attention_mask.shape)\n","    print(labels.shape)\n","    print(labels)\n","    id = id.to(device)\n","    attention_mask = attention_mask.to(device)\n","    labels = labels.to(device)\n","    print(model(id, attention_mask, labels).shape)\n","    break\n","\n","#free gpu memory\n","del model\n","torch.cuda.empty_cache()\n","\n"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["import neptune\n","from tqdm.notebook import tqdm\n","\n","class Learner():\n","    def __init__(self, model, train_dataloader, valid_dataloader, parameter=None):\n","        self.model=model\n","        #self.loss_fn=torch.nn.CrossEntropyLoss()\n","        self.loss_fn=torch.nn.CrossEntropyLoss(ignore_index=-100)\n","        self.device=torch.device(\"cpu\")\n","        if torch.cuda.is_available():\n","            self.device=torch.device(\"cuda\")\n","        #elif torch.backends.mps.is_available():\n","        #    self.device=torch.device(\"mps\")\n","\n","        self.model.to(self.device)\n","        self.run = neptune.init_run(\n","            project=\"bernd.heidemann/PII\",\n","            api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIzNjBlYzVkNi0zZTUwLTQ1ODYtODhlNC02NDUxNDg0MDdjNzUifQ==\",\n","        )  # your credentials\n","        self.parameter = parameter\n","        self.train_dataloader = train_dataloader\n","        self.valid_dataloader = valid_dataloader\n","        self.run[\"parameters\"] = {\n","            **self.parameter\n","        }\n","\n","    def fit(self, lr=0.001, epochs=10):\n","        optimizer=torch.optim.AdamW(self.model.parameters(), lr=lr)\n","        scheduler=scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n","        bar = tqdm(total=len(self.train_dataloader) * epochs, desc=\"Training\")\n","        bar.set_description(\"Epoch 0/{}\".format(epochs))\n","\n","        for epoch in range(epochs):\n","            self.model.train()            \n","            for ids, att_mask, labels in self.train_dataloader:\n","                \n","                ids=ids.to(self.device)\n","                labels=labels.to(self.device)\n","                att_mask=att_mask.to(self.device)\n","                pred=self.model(ids, att_mask, labels)\n","                # reshape pred to [batch_size, num_classes, sequence_length]\n","                metrics=self.f_beta_score_multiclass(labels, pred)\n","                self.run[\"train_f_beta_score\"].log(metrics[\"f_beta\"])\n","                self.run[\"train_precision\"].log(metrics[\"precision\"])\n","                self.run[\"train_recall\"].log(metrics[\"recall\"])\n","                self.run[\"train_true_positives\"].log(metrics[\"true_positives\"])\n","                self.run[\"train_false_positives\"].log(metrics[\"false_positives\"])\n","                self.run[\"train_false_negatives\"].log(metrics[\"false_negatives\"])\n","                pred = pred.permute(0, 2, 1)\n","                loss=self.loss_fn(pred, labels)\n","                self.run[\"train_loss\"].log(loss.item())\n","                loss.backward()\n","                optimizer.step()\n","                optimizer.zero_grad()\n","                bar.update(1)\n","            scheduler.step()\n","            self.model.eval()\n","            # log current state to neptune\n","            if self.valid_dataloader is not None:\n","                metrics=self.get_accuracy()\n","                self.run[\"valid_accuracy\"].log(metrics[\"accuracy\"])\n","                self.run[\"valid_loss\"].log(metrics[\"loss\"])\n","                self.run[\"valid_f_beta_score\"].log(metrics[\"f_beta_score\"])\n","                self.run[\"valid_precision\"].log(metrics[\"precision\"])\n","                self.run[\"valid_recall\"].log(metrics[\"recall\"])\n","                self.run[\"valid_true_positives\"].log(metrics[\"true_positives\"])\n","                self.run[\"valid_false_positives\"].log(metrics[\"false_positives\"])\n","                self.run[\"valid_false_negatives\"].log(metrics[\"false_negatives\"])\n","                bar.set_description(\"Epoch {}/{} validAccuracy: {:.2f} validLoss: {:.2f}\".format(epoch+1, epochs, metrics[\"accuracy\"], metrics[\"loss\"]))\n","                \n","    def get_accuracy(self):\n","        self.model.eval()\n","        with torch.no_grad():\n","            correct=0\n","            losses=[]\n","            batch_metrics=[]\n","            for ids, att_mask, labels in self.valid_dataloader:\n","                ids=ids.to(self.device)\n","                labels=labels.to(self.device)\n","                att_mask=att_mask.to(self.device)\n","                pred=self.model(ids, att_mask, labels)\n","                f_beta_score_results = self.f_beta_score_multiclass(labels, pred)\n","                batch_metrics.append(f_beta_score_results)\n","                pred = pred.permute(0, 2, 1)\n","                loss=self.loss_fn(pred, labels)\n","                losses.append(loss.item())\n","                pred=torch.argmax(pred, dim=1)\n","                correct+=torch.sum(pred==labels).item()\n","            # calc mean of the dict entries in batch_metrics\n","            f_beta_scores = np.mean([x[\"f_beta\"] for x in batch_metrics])\n","            precision = np.mean([x[\"precision\"] for x in batch_metrics])\n","            recall = np.mean([x[\"recall\"] for x in batch_metrics])\n","            true_positives = np.mean([x[\"true_positives\"] for x in batch_metrics])\n","            false_positives = np.mean([x[\"false_positives\"] for x in batch_metrics])\n","            false_negatives = np.mean([x[\"false_negatives\"] for x in batch_metrics])\n","\n","            return {\n","                \"accuracy\": correct/len(self.valid_dataloader.dataset),\n","                \"loss\": np.mean(losses),\n","                \"f_beta_score\": f_beta_scores,\n","                \"precision\": precision,\n","                \"recall\": recall,\n","                \"true_positives\": true_positives,\n","                \"false_positives\": false_positives,\n","                \"false_negatives\": false_negatives\n","            }\n","        \n","    def f_beta_score_multiclass(self, y_true, y_pred, beta=5, epsilon=1e-7):\n","\n","        # assert y_pred has values between 0 and 1\n","        assert y_pred.min() >= 0\n","        assert y_pred.max() <= 1\n","\n","        y_true_one_hot = torch.nn.functional.one_hot(y_true, num_classes=y_pred.shape[2])\n","    \n","        # Berechnung von True Positives, False Positives und False Negatives\n","        tp = torch.sum(y_true_one_hot * y_pred, dim=0)\n","        fp = torch.sum((1 - y_true_one_hot) * y_pred, dim=0)\n","        fn = torch.sum(y_true_one_hot * (1 - y_pred), dim=0)\n","\n","        # Summierung über alle Klassen\n","        tp_sum = torch.sum(tp)\n","        fp_sum = torch.sum(fp)\n","        fn_sum = torch.sum(fn)\n","\n","        # Berechnung von Präzision und Recall\n","        precision = tp_sum / (tp_sum + fp_sum + epsilon)\n","        recall = tp_sum / (tp_sum + fn_sum + epsilon)\n","        # Berechnung des F-Beta-Scores\n","        f_beta = (1 + beta**2) * (precision * recall) / ((beta**2 * precision) + recall + epsilon)\n","\n","        return {\n","            \"f_beta\": f_beta.item(),\n","            \"precision\": precision.item(),\n","            \"recall\": recall.item(),\n","            \"true_positives\": tp_sum.item(),\n","            \"false_positives\": fp_sum.item(),\n","            \"false_negatives\": fn_sum.item()\n","        }\n","\n","    "]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["def inference(model):\n","    import torch\n","    from transformers import AutoTokenizer\n","    from transformers import AutoModelForTokenClassification\n","    import json\n","\n","\n","    parameter_inference= {\n","        \"max_length\": 512,\n","        \"batch_size\": 8,\n","        \"model_path\": \"../PII Models/model_nb_04.pt\",\n","        \"model\": \"microsoft/deberta-v3-base\"\n","    }\n","\n","    def tokenize(example, tokenizer, max_length):\n","        text = []\n","        for t,  ws in zip(example[\"tokens\"], example[\"trailing_whitespace\"]):\n","            text.append(t)\n","            if ws:\n","                text.append(\" \")\n","        tokenized = tokenizer(\"\".join(text), return_offsets_mapping=True, truncation=True, max_length=max_length, padding=\"max_length\")\n","        text = \"\".join(text)\n","        length = len(tokenized.input_ids)\n","        return {\n","            **tokenized,\n","            \"length\": length,\n","        }\n","\n","    class PiiDatasetInference(torch.utils.data.Dataset):\n","        def __init__(self, dataset, tokenizer, max_length):\n","            self.dataset = dataset\n","            self.tokenizer = tokenizer\n","            self.max_length = max_length\n","            \n","        def __getitem__(self, idx):\n","            vals=tokenize(self.dataset[idx], self.tokenizer, self.max_length)\n","            input_ids = torch.tensor(vals[\"input_ids\"])\n","            attention_mask = torch.tensor(vals[\"attention_mask\"])\n","            return input_ids, attention_mask\n","        \n","        def __len__(self):\n","            return len(self.dataset)\n","        \n","    data = json.load(open(\"data/train.json\")) # foo\n","    from itertools import chain\n","    all_labels = sorted(list(set(chain(*[x[\"labels\"] for x in data]))))\n","    label2id = {l: i for i,l in enumerate(all_labels)}\n","    id2label = {v:k for k,v in label2id.items()}\n","\n","    tokenizer = AutoTokenizer.from_pretrained(parameter_inference[\"model\"])\n","    data = json.load(open(\"data/test.json\"))\n","    my_dataset=PiiDatasetInference(data, tokenizer, parameter_inference[\"max_length\"])\n","    loader=torch.utils.data.DataLoader(my_dataset, batch_size=parameter_inference['batch_size'], shuffle=True)\n","    for id, attention_mask in loader:\n","        id = id.to(device)\n","        attention_mask = attention_mask.to(device)\n","        preds=model(id, attention_mask).argmax(dim=2)\n","\n","        for pred, id in zip(preds.flatten(), id.flatten()):\n","            if pred != 12:\n","                print(f\"TOKEN:{tokenizer.decode(id)}  --- pred:{id2label[pred.item()]}\")\n","        print(\"next\")"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["original len 6807\n","filtered len 2138\n","len train ds 1710\n","len valid ds 428\n"]}],"source":["data = json.load(open(\"data/train.json\"))\n","\n","print(\"original len\",  len(data))\n","# this increased the \n","data_filterd = list(filter(filter_no_pii, data))\n","print(\"filtered len\", len(data_filterd))\n","\n","data_len=len(data_filterd)\n","\n","train_len=int(len(data_filterd)*0.8)\n","valid_len=len(data_filterd)-train_len\n","\n","train_data_idx=np.random.choice(data_len, train_len, replace=False)\n","valid_data_idx=np.array(list(set(range(data_len))-set(train_data_idx)))\n","\n","train_data=[data_filterd[i] for i in train_data_idx]\n","valid_data=[data_filterd[i] for i in valid_data_idx]\n","\n","print(\"len train ds\", len(train_data))\n","print(\"len valid ds\", len(valid_data))"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","/opt/homebrew/Caskroom/miniconda/base/envs/torch_ds/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:454: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n","  warnings.warn(\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2ForTokenClassification: ['mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.classifier.weight', 'deberta.embeddings.position_embeddings.weight', 'lm_predictions.lm_head.dense.bias']\n","- This IS expected if you are initializing DebertaV2ForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2ForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/var/folders/yl/qjs6b9wn4zx7nh630c4my9lw0000gn/T/ipykernel_28469/2897371199.py:16: NeptuneWarning: The following monitoring options are disabled by default in interactive sessions: 'capture_stdout', 'capture_stderr', 'capture_traceback', and 'capture_hardware_metrics'. To enable them, set each parameter to 'True' when initializing the run. The monitoring will continue until you call run.stop() or the kernel stops. Also note: Your source files can only be tracked if you pass the path(s) to the 'source_code' argument. For help, see the Neptune docs: https://docs.neptune.ai/logging/source_code/\n","  self.run = neptune.init_run(\n"]},{"name":"stdout","output_type":"stream","text":["https://app.neptune.ai/bernd.heidemann/PII/e/PII-42\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2d993280f26946cab801f73c92c2d23a","version_major":2,"version_minor":0},"text/plain":["Training:   0%|          | 0/428 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from tokenizers import AddedToken\n","\n","# set environment variables: TOKENIZERS_PARALLELISM=false\n","import os\n","os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n","\n","tokenizer = AutoTokenizer.from_pretrained(parameter[\"model\"])\n","tokenizer.add_tokens(AddedToken(\"\\n\", normalized=False))\n","\n","train_dataset = PiiDataset(train_data, tokenizer, label2id, parameter[\"max_length\"])\n","valid_dataset = PiiDataset(valid_data, tokenizer, label2id, parameter[\"max_length\"])\n","train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=parameter['batch_size'], shuffle=True)\n","valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=parameter['batch_size'], shuffle=False)\n","my_model=MyModel(parameter['model'], len(label2id))\n","\n","learner=Learner(my_model, train_dataloader, valid_dataloader, parameter=parameter)\n","learner.fit(lr=parameter['lr'], epochs=2)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["predictions: tensor([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","        4096], device='cuda:0')\n","ground truth:  tensor([   0,    0,    7,    0,    0,    0,    0,    0,    6,    0,    0,    0,\n","        4083], device='cuda:0')\n"]}],"source":["inference(learner.model)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"04a67609302b4591a727a0841f796d6a","version_major":2,"version_minor":0},"text/plain":["Training:   0%|          | 0/4140 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["learner.fit(lr=parameter['lr'], epochs=5)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["inference(learner.model)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","    \n","    "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":7500999,"sourceId":66653,"sourceType":"competition"},{"datasetId":4319117,"sourceId":7429898,"sourceType":"datasetVersion"}],"dockerImageVersionId":30636,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"}},"nbformat":4,"nbformat_minor":4}
