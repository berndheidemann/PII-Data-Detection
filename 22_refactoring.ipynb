{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from transformers import AutoTokenizer, TrainingArguments, Trainer, AutoModelForTokenClassification\n","from pathlib import Path\n","import numpy as np\n","import torch\n","from tokenizers import AddedToken\n","from tqdm.notebook import tqdm\n","from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n","import pandas as pd\n","from datasets import Dataset\n","\n","kaggle=False\n","\n","path=\"/kaggle/input/pii-detection-removal-from-educational-data\" if kaggle else \"data\"\n","train_path = path + \"/train.json\"\n","test_path = path + \"/test.json\"\n","\n","model_path = \"/kaggle/input/huggingfacedebertav3variants/deberta-v3-base\" if kaggle else \"microsoft/deberta-v3-base\"\n","\n","if not kaggle: import neptune\n","if not kaggle: from seqeval.metrics import recall_score, precision_score, f1_score, accuracy_score"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-01-25T21:37:35.578114Z","iopub.status.busy":"2024-01-25T21:37:35.577723Z","iopub.status.idle":"2024-01-25T21:37:35.587366Z","shell.execute_reply":"2024-01-25T21:37:35.5858Z","shell.execute_reply.started":"2024-01-25T21:37:35.578083Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["{'model': 'microsoft/deberta-v3-base', 'max_length': 1024, 'inference_max_length': 2000, 'batch_size': 4, 'inference_batch_size': 1, 'lr': 5e-05, 'lr_scale_unfreeze': 0.1, 'filter_no_pii_percent_allow': 0.2, 'notebook': '20_deberta base_1024len.ipynb', 'CROSS_ENTROPY_WEIGHT_MULTI': 400, 'epochs_before_unfreeze': 2, 'epochs_after_unfreeze': 6, 'repeat_unfreeze_train_n_times': 2, 'validate_every_n_epochs': 2, 'train_test_split': 0.2, 'num_proc': 16, 'freeze_embeddings': False, 'freeze_layers': 6}\n"]}],"source":["cross_entropy_weight_multi = 400\n","\n","CROSS_ENTROPY_WEIGHTS = [cross_entropy_weight_multi]*12\n","CROSS_ENTROPY_WEIGHTS.append(1)\n","\n","parameter= {\n","    \"model\": model_path,\n","    \"max_length\": 1024,\n","    \"inference_max_length\": 2000,\n","    \"batch_size\": 4,\n","    \"inference_batch_size\": 1,\n","    \"lr\": 5e-05,\n","    \"lr_scale_unfreeze\": 0.1,\n","    \"filter_no_pii_percent_allow\": 0.2,\n","    \"notebook\": \"20_deberta base_1024len.ipynb\",\n","    \"CROSS_ENTROPY_WEIGHT_MULTI\": cross_entropy_weight_multi,\n","    \"epochs_before_unfreeze\": 2,\n","    \"epochs_after_unfreeze\": 6,\n","    \"repeat_unfreeze_train_n_times\": 2,\n","    \"validate_every_n_epochs\": 2,\n","    \"train_test_split\": 0.2,\n","    \"num_proc\": 16, \n","    \"freeze_embeddings\": False,\n","    \"freeze_layers\": 6\n","}\n","\n","print(parameter)"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["target = [\n","    'B-EMAIL', 'B-ID_NUM', 'B-NAME_STUDENT', 'B-PHONE_NUM', \n","    'B-STREET_ADDRESS', 'B-URL_PERSONAL', 'B-USERNAME', 'I-ID_NUM', \n","    'I-NAME_STUDENT', 'I-PHONE_NUM', 'I-STREET_ADDRESS', 'I-URL_PERSONAL'\n","]"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-01-25T21:37:35.606208Z","iopub.status.busy":"2024-01-25T21:37:35.605889Z","iopub.status.idle":"2024-01-25T21:37:35.62164Z","shell.execute_reply":"2024-01-25T21:37:35.620746Z","shell.execute_reply.started":"2024-01-25T21:37:35.606175Z"},"trusted":true},"outputs":[],"source":["from itertools import chain\n","import json\n","\n","data = json.load(open(train_path))\n","all_labels = sorted(list(set(chain(*[x[\"labels\"] for x in data]))))\n","label2id = {l: i for i,l in enumerate(all_labels)}\n","id2label = {v:k for k,v in label2id.items()}"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["import random\n","\n","def tokenize(example, tokenizer, label2id, max_length, all_labels_list):\n","    text = []\n","    import numpy as np\n","\n","    # these are at the character level\n","    labels = []\n","    targets = []\n","\n","    for t, l, ws in zip(example[\"tokens\"], example[\"labels\"], example[\"trailing_whitespace\"]):\n","\n","        text.append(t)\n","        labels.extend([l]*len(t))\n","        \n","        if l in all_labels_list:\n","            targets.append(1)\n","        else:\n","            targets.append(0)\n","        # if there is trailing whitespace\n","        if ws:\n","            text.append(\" \")\n","            labels.append(\"O\")\n","\n","    tokenized = tokenizer(\"\".join(text), return_offsets_mapping=True, truncation=True, max_length=max_length, padding=\"max_length\")\n","    \n","    target_num = sum(targets)\n","    labels = np.array(labels)\n","\n","    text = \"\".join(text)\n","    token_labels = []\n","\n","    for start_idx, end_idx in tokenized.offset_mapping:\n","\n","        # CLS token\n","        if start_idx == 0 and end_idx == 0: \n","            token_labels.append(label2id[\"O\"])\n","            continue\n","\n","        # case when token starts with whitespace\n","        if text[start_idx].isspace():\n","            start_idx += 1\n","\n","        try:\n","            token_labels.append(label2id[labels[start_idx]])\n","        except:\n","            token_labels.append(label2id[\"O\"])\n","\n","    length = len(tokenized.input_ids)\n","\n","    return {\n","        **tokenized,\n","        \"labels\": token_labels,\n","        \"length\": length,\n","        \"target_num\": target_num,\n","        \"group\": 1 if target_num>0 else 0\n","    }\n","\n","# https://www.kaggle.com/competitions/pii-detection-removal-from-educational-data/discussion/468844\n","def filter_no_pii(example, percent_allow=parameter[\"filter_no_pii_percent_allow\"]):\n","    # Return True if there is PII\n","    # Or 20% of the time if there isn't\n","    has_pii = set(\"O\") != set(example[\"labels\"])\n","    return has_pii or (random.random() < percent_allow)"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","c:\\Users\\Bernd\\anaconda3\\envs\\mytorch\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:470: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n","  warnings.warn(\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8f2a8bc1f2c64f68b2cc33f077e9a4ce","version_major":2,"version_minor":0},"text/plain":["Map (num_proc=16):   0%|          | 0/6807 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1e1ab40ea7bd4ecfa0b44149ad22a6f1","version_major":2,"version_minor":0},"text/plain":["Filter (num_proc=16):   0%|          | 0/6807 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["train_len 5445\n","valid_len 1362\n"]}],"source":["data = json.load(open(train_path))\n","ds = Dataset.from_dict({\n","    \"full_text\": [x[\"full_text\"] for x in data],\n","    \"document\": [str(x[\"document\"]) for x in data],\n","    \"tokens\": [x[\"tokens\"] for x in data],\n","    \"trailing_whitespace\": [x[\"trailing_whitespace\"] for x in data],\n","    \"labels\": [x[\"labels\"] for x in data],\n","})\n","    \n","tokenizer = AutoTokenizer.from_pretrained(parameter[\"model\"])\n","ds = ds.map(tokenize, fn_kwargs={\"tokenizer\": tokenizer, \"label2id\": label2id, \"max_length\": parameter[\"max_length\"], \"all_labels_list\": target}, num_proc=parameter[\"num_proc\"])\n","ds=ds.filter(filter_no_pii, num_proc=parameter[\"num_proc\"])\n","\n","\n","data_len=len(ds)\n","train_len=int(len(ds)*(1-parameter[\"train_test_split\"]))\n","valid_len=len(ds)-train_len\n","train_data_idx=np.random.choice(data_len, train_len, replace=False)\n","valid_data_idx=np.array(list(set(range(data_len))-set(train_data_idx)))\n","print(\"train_len\", train_len)\n","print(\"valid_len\", valid_len)\n","\n","# split ds in train and valid\n","train_ds=ds.select(train_data_idx)\n","valid_ds=ds.select(valid_data_idx)"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"data":{"text/plain":["[1,\n"," 34314,\n"," 67112,\n"," 15895,\n"," 1637,\n"," 267,\n"," 2307,\n"," 265,\n"," 3306,\n"," 6938,\n"," 19992,\n"," 6738,\n"," 429,\n"," 14700,\n"," 7057,\n"," 273,\n"," 1594,\n"," 718,\n"," 2307,\n"," 264,\n"," 726,\n"," 266,\n"," 1034,\n"," 265,\n"," 3306,\n"," 6938,\n"," 19992,\n"," 261,\n"," 537,\n"," 275,\n"," 262,\n"," 872,\n"," 4110,\n"," 265,\n"," 751,\n"," 3666,\n"," 265,\n"," 356,\n"," 747,\n"," 260,\n"," 273,\n"," 284,\n"," 397,\n"," 2149,\n"," 267,\n"," 312,\n"," 996,\n"," 946,\n"," 265,\n"," 109211,\n"," 2503,\n"," 261,\n"," 401,\n"," 265,\n"," 266,\n"," 455,\n"," 517,\n"," 267,\n"," 291,\n"," 1146,\n"," 261,\n"," 304,\n"," 433,\n"," 266,\n"," 379,\n"," 1739,\n"," 492,\n"," 265,\n"," 514,\n"," 1436,\n"," 1423,\n"," 263,\n"," 4291,\n"," 261,\n"," 353,\n"," 1870,\n"," 2170,\n"," 1655,\n"," 18768,\n"," 261,\n"," 353,\n"," 3981,\n"," 1050,\n"," 260,\n"," 17734,\n"," 4844,\n"," 1150,\n"," 276,\n"," 297,\n"," 499,\n"," 261,\n"," 263,\n"," 273,\n"," 330,\n"," 264,\n"," 799,\n"," 278,\n"," 287,\n"," 1628,\n"," 489,\n"," 285,\n"," 293,\n"," 653,\n"," 260,\n"," 3466,\n"," 4331,\n"," 399,\n"," 379,\n"," 2136,\n"," 261,\n"," 304,\n"," 322,\n"," 264,\n"," 9409,\n"," 273,\n"," 505,\n"," 1113,\n"," 1585,\n"," 653,\n"," 2705,\n"," 7230,\n"," 401,\n"," 3978,\n"," 69886,\n"," 2573,\n"," 261,\n"," 263,\n"," 330,\n"," 264,\n"," 266,\n"," 12876,\n"," 262,\n"," 947,\n"," 293,\n"," 1113,\n"," 260,\n"," 279,\n"," 454,\n"," 788,\n"," 273,\n"," 696,\n"," 262,\n"," 535,\n"," 271,\n"," 15540,\n"," 265,\n"," 2169,\n"," 12103,\n"," 261,\n"," 263,\n"," 1331,\n"," 314,\n"," 262,\n"," 1637,\n"," 51146,\n"," 9395,\n"," 260,\n"," 5736,\n"," 620,\n"," 362,\n"," 261,\n"," 273,\n"," 696,\n"," 264,\n"," 365,\n"," 266,\n"," 686,\n"," 275,\n"," 305,\n"," 262,\n"," 467,\n"," 2175,\n"," 265,\n"," 439,\n"," 261,\n"," 7419,\n"," 293,\n"," 266,\n"," 1650,\n"," 287,\n"," 42266,\n"," 3308,\n"," 265,\n"," 439,\n"," 285,\n"," 261,\n"," 293,\n"," 262,\n"," 5392,\n"," 287,\n"," 5710,\n"," 1339,\n"," 351,\n"," 291,\n"," 439,\n"," 320,\n"," 8552,\n"," 295,\n"," 273,\n"," 433,\n"," 278,\n"," 285,\n"," 263,\n"," 668,\n"," 262,\n"," 1043,\n"," 265,\n"," 1854,\n"," 4820,\n"," 260,\n"," 434,\n"," 262,\n"," 1547,\n"," 278,\n"," 284,\n"," 397,\n"," 270,\n"," 266,\n"," 872,\n"," 8100,\n"," 261,\n"," 304,\n"," 278,\n"," 1150,\n"," 276,\n"," 297,\n"," 618,\n"," 261,\n"," 273,\n"," 449,\n"," 284,\n"," 1125,\n"," 264,\n"," 2059,\n"," 1564,\n"," 2204,\n"," 261,\n"," 324,\n"," 273,\n"," 412,\n"," 273,\n"," 1042,\n"," 9209,\n"," 265,\n"," 262,\n"," 1415,\n"," 261,\n"," 972,\n"," 312,\n"," 451,\n"," 261,\n"," 753,\n"," 42124,\n"," 275,\n"," 262,\n"," 2105,\n"," 264,\n"," 262,\n"," 467,\n"," 439,\n"," 261,\n"," 7419,\n"," 293,\n"," 5101,\n"," 261,\n"," 398,\n"," 1531,\n"," 271,\n"," 15404,\n"," 260,\n"," 463,\n"," 266,\n"," 567,\n"," 1095,\n"," 273,\n"," 681,\n"," 262,\n"," 370,\n"," 427,\n"," 263,\n"," 539,\n"," 2175,\n"," 263,\n"," 3344,\n"," 261,\n"," 263,\n"," 3034,\n"," 278,\n"," 264,\n"," 467,\n"," 3962,\n"," 261,\n"," 275,\n"," 266,\n"," 782,\n"," 3175,\n"," 261,\n"," 1670,\n"," 2105,\n"," 399,\n"," 264,\n"," 433,\n"," 262,\n"," 439,\n"," 261,\n"," 263,\n"," 370,\n"," 539,\n"," 262,\n"," 11715,\n"," 457,\n"," 262,\n"," 467,\n"," 1884,\n"," 260,\n"," 279,\n"," 11715,\n"," 284,\n"," 1992,\n"," 264,\n"," 262,\n"," 596,\n"," 263,\n"," 1018,\n"," 266,\n"," 426,\n"," 266,\n"," 29813,\n"," 8031,\n"," 260,\n"," 325,\n"," 284,\n"," 1216,\n"," 264,\n"," 413,\n"," 2006,\n"," 278,\n"," 283,\n"," 266,\n"," 1653,\n"," 270,\n"," 2510,\n"," 261,\n"," 263,\n"," 270,\n"," 723,\n"," 742,\n"," 265,\n"," 266,\n"," 596,\n"," 260,\n"," 2,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," ...]"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["train_ds[0][\"input_ids\"]"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["def tokenize_inference(example, tokenizer, max_length):\n","        text = []\n","        for t,  ws in zip(example[\"tokens\"], example[\"trailing_whitespace\"]):\n","            text.append(t)\n","            if ws:\n","                text.append(\" \")\n","        tokenized = tokenizer(\"\".join(text), return_offsets_mapping=True, truncation=True, max_length=max_length, padding=\"max_length\")\n","        text = \"\".join(text)\n","        length = len(tokenized.input_ids)\n","        return {\n","            **tokenized,\n","            \"length\": length,\n","        }\n","        \n","class TestTokenizer():\n","    def __init__(self, tokenizer):\n","        self.tokenizer = tokenizer\n","    \n","    def preprocess(self, example):\n","        # Preprocess the tokens and labels by adding trailing whitespace and labels\n","        tokens = []\n","        tokens_without_ws = []\n","        token_map = [] # Use the index as labels\n","        index = 0\n","        for token, t_ws in zip(example[\"tokens\"], example[\"trailing_whitespace\"]):\n","            tokens_without_ws.append(token)\n","            tokens.append(token)\n","            token_map.extend([index] * len(token))\n","            # Added trailing whitespace and label if true and \n","            if t_ws:\n","                tokens.append(\" \")\n","                token_map.append(-1)\n","            index += 1\n","        return tokens, token_map, tokens_without_ws\n","    \n","    def tokenize(self, example):\n","        tokens, token_map, tokens_without_ws = self.preprocess(example)\n","        text = \"\".join(tokens)\n","        tokenized = self.tokenizer(text, return_offsets_mapping=True, padding=\"max_length\",\n","                                   truncation=True, max_length=parameter[\"inference_max_length\"])\n","        return {**tokenized, \"token_map\": token_map, \"tokens\": tokens, \"tokens_without_ws\": tokens_without_ws} \n","\n","class PiiDatasetInference(torch.utils.data.Dataset):\n","        def __init__(self, dataset, tokenizer):\n","            self.dataset = dataset\n","            self.tokenizer=TestTokenizer(tokenizer)\n","            \n","        def __getitem__(self, idx):\n","            vals=self.tokenizer.tokenize(self.dataset[idx])\n","            input_ids = torch.tensor(vals[\"input_ids\"])\n","            attention_mask = torch.tensor(vals[\"attention_mask\"])\n","            document_id = self.dataset[idx][\"document\"]\n","            return input_ids, attention_mask, document_id, vals\n","        \n","        def __len__(self):\n","            return len(self.dataset)\n","\n","# Convert preds to a list of dictionaries\n","def to_test_submission(preds=None, dataset=None, document_ids=None, id2label=None):\n","    triplets = []\n","    row_id = 0\n","    results = []\n","    \n","    for i in range(len(preds)):\n","        input_ids, attention_mask, document_id, vals = dataset[i]\n","        token_map=vals[\"token_map\"]\n","        offsets=vals[\"offset_mapping\"]\n","        tokens=vals[\"tokens_without_ws\"]\n","        #print(\"tokens\", tokens)\n","        pred=preds[i]\n","        original_text = tokenizer.decode(input_ids)[6:] # skip CLS token\n","        #print(\"original_text\", original_text)\n","        #print(\"token_map\", token_map)\n","        #print(\"offsets\", offsets)   \n","        #print(\"pred\", pred)\n","\n","        for token_pred, input_id, (start_idx, end_idx) in zip(pred, input_ids, offsets):\n","            #print(\"\\nnow doing \", start_idx,  end_idx, token_pred)\n","            if start_idx == 0 and end_idx == 0: # Skip 0 offset\n","                continue\n","            # Skip spaces \n","            while start_idx < len(token_map):\n","                #print(\"loop, start_idx now\", start_idx) \n","                #print(\" tokens[token_map[start_idx]]: \", tokens[token_map[start_idx]] if not tokens[token_map[start_idx]].isspace() else \"WHITESPACE\")          \n","                if token_map[start_idx] == -1: # Skip unknown tokens               \n","                    start_idx += 1\n","                elif tokens[token_map[start_idx]].isspace(): # Skip white space\n","                    start_idx += 1\n","                else:\n","                    break\n","            # Ensure start index < length\n","            if start_idx < len(token_map):\n","                token_id = token_map[start_idx]\n","                #print(\"token_id\", token_id)\n","                #token_id= input_id.item()\n","                label_pred = id2label[token_pred.item()]\n","                #print(\"label_pred\", label_pred)\n","                # ignore \"O\" and whitespace preds\n","                if label_pred != \"O\" and token_id != -1:\n","                    #print(\"is PII\", token_id, label_pred)\n","                    token_str = tokens[token_id]\n","                    triplet = (label_pred, token_id, token_str)\n","                    if triplet not in triplets:\n","                        results.append({\n","                            \"row_id\": row_id, \n","                            \"document\": document_id,\n","                            \"token\": token_id, \n","                            \"label\": label_pred,\n","                            \"token_str\": token_str\n","                        })\n","                        triplets.append(triplet)\n","                        row_id += 1\n","\n","    # Create a dataframe \n","    return results\n","\n","def create_submission(model, filename=\"submission.csv\"):\n","    data = json.load(open(train_path))\n","    from itertools import chain\n","    all_labels = sorted(list(set(chain(*[x[\"labels\"] for x in data]))))\n","    label2id = {l: i for i,l in enumerate(all_labels)}\n","    id2label = {v:k for k,v in label2id.items()}\n","\n","    data=json.load(open(test_path))\n","    tokenizer = AutoTokenizer.from_pretrained(parameter[\"model\"])\n","    my_dataset=PiiDatasetInference(data, tokenizer)\n","    loader=torch.utils.data.DataLoader(my_dataset, batch_size=1, shuffle=False)\n","\n","    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model.eval()\n","    \n","    # stack all predictions into tensor\n","    all_preds = []\n","\n","    for id, attention_mask, document_ids, vals in loader:\n","        id=id.to(device)\n","        attention_mask=attention_mask.to(device)\n","        preds=model(id, attention_mask).get('logits').argmax(dim=2)\n","        all_preds.append(preds)\n","        #for pred, id in zip(preds.flatten(), id.flatten()):\n","        #    if pred != 12:\n","                #print(f\"Document: {document_id.item()} TOKEN:{tokenizer.decode(id)}  --- pred:{id2label[pred.item()]}\")\n","        #        output[row_id]={\"document\":document_id.item(), \"token\":id.item(), \"label\":id2label[pred.item()]}\n","        #        row_id+=1\n","        #for pred, id in zip(preds.flatten(), id.flatten()):\n","        #    if pred != 12:\n","        #        print(f\"TOKEN:{tokenizer.decode(id)}  --- pred:{id2label[pred.item()]}\")\n","    \n","   \n","    all_preds = torch.cat(all_preds, dim=0)\n","    \n","    results = to_test_submission(preds=all_preds, dataset=my_dataset, document_ids=document_ids, id2label=id2label)\n","    if len(results) == 0:\n","        print(\"Error in create_submission(): No predictions made, probably because the model is not learning. Check the model and the data.\")\n","        return\n","    df = pd.DataFrame(results)\n","    df=df[[\"row_id\", \"document\", \"token\", \"label\"]]\n","    print(df)\n","    df.to_csv(filename, index=False)\n","\n","#create_submission(MyModel(parameter['model'], len(label2id)).to(device), \"submission_just_dumb.csv\")\n","# create_submission(model, \"submission.csv\")\n"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["from transformers import DataCollatorForTokenClassification\n","\n","collator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=16)"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["https://app.neptune.ai/bernd.heidemann/PII/e/PII-221\n"]}],"source":["# using Trainer and TrainingArguments from transformers\n","\n","\n","def compute_metrics(p, all_labels):\n","    predictions, labels = p\n","    predictions = np.argmax(predictions, axis=2)\n","\n","    # Remove ignored index (special tokens)\n","    true_predictions = [\n","        [all_labels[p] for (p, l) in zip(prediction, label) if l != -100]\n","        for prediction, label in zip(predictions, labels)\n","    ]\n","    true_labels = [\n","        [all_labels[l] for (p, l) in zip(prediction, label) if l != -100]\n","        for prediction, label in zip(predictions, labels)\n","    ]\n","    \n","    recall = recall_score(true_labels, true_predictions)\n","    precision = precision_score(true_labels, true_predictions)\n","    f1_score = (1 + 5*5) * recall * precision / (5*5*precision + recall)\n","    \n","    results = {\n","        'recall': recall,\n","        'precision': precision,\n","        'f1': f1_score\n","    }\n","    return results\n","\n","if not kaggle:\n","    from transformers.integrations import NeptuneCallback\n","\n","    run = neptune.init_run(\n","        project=\"bernd.heidemann/PII\",\n","        api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIzNjBlYzVkNi0zZTUwLTQ1ODYtODhlNC02NDUxNDg0MDdjNzUifQ==\",\n","    )  # your credentials\n","    run[\"parameters\"] = {\n","    **parameter\n","    }\n","\n","    neptune_callback = NeptuneCallback(run=run, log_model_weights=False, log_parameters=False)\n","\n","from functools import partial\n","\n","def get_trainer(model, train_dataloader, valid_dataloader, learnrate_multiplier=1.0):\n","    training_args = TrainingArguments(\n","        output_dir='./results',          # output directory\n","        num_train_epochs=parameter[\"epochs_before_unfreeze\"],              # total number of training epochs\n","        per_device_train_batch_size=parameter[\"batch_size\"],  # batch size per device during training\n","        per_device_eval_batch_size=parameter[\"inference_batch_size\"],   # batch size for evaluation\n","        warmup_steps=500,                # number of warmup steps for learning rate scheduler\n","        weight_decay=0.01,               # strength of weight decay\n","        logging_dir='./logs',            # directory for storing logs\n","        logging_steps=10,\n","        evaluation_strategy=\"steps\",\n","        eval_steps=400,\n","        save_steps=400,\n","        save_total_limit=3,\n","        load_best_model_at_end=False,\n","        metric_for_best_model=\"f1\" if not kaggle else \"eval_loss\",\n","        greater_is_better=True,\n","        overwrite_output_dir=True,\n","        report_to=\"none\",\n","        learning_rate=parameter[\"lr\"]*learnrate_multiplier\n","\n","        #callback for neptune\n","    )\n","\n","    class MyTrainer(Trainer):\n","        def __init__(self, model=None, args=None, train_dataset=None, eval_dataset=None, compute_metrics=None, callbacks=None):\n","            super().__init__(model=model, args=args, train_dataset=train_dataset, eval_dataset=eval_dataset, compute_metrics=compute_metrics, callbacks=callbacks)\n","            # Definieren Sie hier Ihre Gewichte für die Klassen, z.B. torch.tensor([1.0, 2.0, 0.5])\n","            self.weight = torch.tensor(CROSS_ENTROPY_WEIGHTS).to(device)\n","            self.loss_func=torch.nn.CrossEntropyLoss(ignore_index=-100, weight=torch.tensor(CROSS_ENTROPY_WEIGHTS, dtype=torch.float32).to(device))\n","\n","        def compute_loss(self, model, inputs, return_outputs=False):\n","            labels = inputs.get(\"labels\")\n","            outputs = model(**inputs)\n","            logits = outputs.get('logits')\n","            loss = self.loss_func(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n","            return (loss, outputs) if return_outputs else loss\n","        \n","\n","    trainer = MyTrainer(\n","        model=model,                         # the instantiated 🤗 Transformers model to be trained\n","        args=training_args,                  # training arguments, defined above\n","        train_dataset=train_dataloader,         # training dataset\n","        eval_dataset=valid_dataloader,             # evaluation dataset\n","        compute_metrics=partial(compute_metrics, all_labels=all_labels) if not kaggle else None,\n","        callbacks=[neptune_callback] if not kaggle else None\n","    )\n","    return trainer"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["def unfreeze(model):\n","    for param in model.base_model.parameters():\n","        param.requires_grad = True\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["DebertaV2Config {\n","  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"B-EMAIL\",\n","    \"1\": \"B-ID_NUM\",\n","    \"2\": \"B-NAME_STUDENT\",\n","    \"3\": \"B-PHONE_NUM\",\n","    \"4\": \"B-STREET_ADDRESS\",\n","    \"5\": \"B-URL_PERSONAL\",\n","    \"6\": \"B-USERNAME\",\n","    \"7\": \"I-ID_NUM\",\n","    \"8\": \"I-NAME_STUDENT\",\n","    \"9\": \"I-PHONE_NUM\",\n","    \"10\": \"I-STREET_ADDRESS\",\n","    \"11\": \"I-URL_PERSONAL\",\n","    \"12\": \"O\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"B-EMAIL\": 0,\n","    \"B-ID_NUM\": 1,\n","    \"B-NAME_STUDENT\": 2,\n","    \"B-PHONE_NUM\": 3,\n","    \"B-STREET_ADDRESS\": 4,\n","    \"B-URL_PERSONAL\": 5,\n","    \"B-USERNAME\": 6,\n","    \"I-ID_NUM\": 7,\n","    \"I-NAME_STUDENT\": 8,\n","    \"I-PHONE_NUM\": 9,\n","    \"I-STREET_ADDRESS\": 10,\n","    \"I-URL_PERSONAL\": 11,\n","    \"O\": 12\n","  },\n","  \"layer_norm_eps\": 1e-07,\n","  \"max_position_embeddings\": 512,\n","  \"max_relative_positions\": -1,\n","  \"model_type\": \"deberta-v2\",\n","  \"norm_rel_ebd\": \"layer_norm\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"pooler_dropout\": 0,\n","  \"pooler_hidden_act\": \"gelu\",\n","  \"pooler_hidden_size\": 768,\n","  \"pos_att_type\": [\n","    \"p2c\",\n","    \"c2p\"\n","  ],\n","  \"position_biased_input\": false,\n","  \"position_buckets\": 256,\n","  \"relative_attention\": true,\n","  \"share_att_key\": true,\n","  \"transformers_version\": \"4.31.0\",\n","  \"type_vocab_size\": 0,\n","  \"vocab_size\": 128100\n","}\n","\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\Bernd\\anaconda3\\envs\\mytorch\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"50634f6c68154b439de48d427fc608d8","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2724 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'loss': 2.5539, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.01}\n","{'loss': 2.4349, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.01}\n","{'loss': 2.2854, 'learning_rate': 3e-06, 'epoch': 0.02}\n","{'loss': 1.9869, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.03}\n","{'loss': 1.4402, 'learning_rate': 5e-06, 'epoch': 0.04}\n","{'loss': 1.214, 'learning_rate': 6e-06, 'epoch': 0.04}\n","{'loss': 0.8297, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.05}\n","{'loss': 0.5887, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.06}\n","{'loss': 0.371, 'learning_rate': 9e-06, 'epoch': 0.07}\n","{'loss': 1.4613, 'learning_rate': 1e-05, 'epoch': 0.07}\n","{'loss': 0.4691, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.08}\n","{'loss': 0.8174, 'learning_rate': 1.2e-05, 'epoch': 0.09}\n","{'loss': 0.6609, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.1}\n","{'loss': 0.6188, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.1}\n","{'loss': 0.329, 'learning_rate': 1.5e-05, 'epoch': 0.11}\n","{'loss': 0.3835, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.12}\n","{'loss': 0.1783, 'learning_rate': 1.7000000000000003e-05, 'epoch': 0.12}\n","{'loss': 0.3124, 'learning_rate': 1.8e-05, 'epoch': 0.13}\n","{'loss': 0.0776, 'learning_rate': 1.9e-05, 'epoch': 0.14}\n","{'loss': 0.0707, 'learning_rate': 2e-05, 'epoch': 0.15}\n","{'loss': 0.203, 'learning_rate': 2.1e-05, 'epoch': 0.15}\n","{'loss': 0.2739, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.16}\n","{'loss': 0.0547, 'learning_rate': 2.3000000000000003e-05, 'epoch': 0.17}\n","{'loss': 0.2176, 'learning_rate': 2.4e-05, 'epoch': 0.18}\n","{'loss': 0.2662, 'learning_rate': 2.5e-05, 'epoch': 0.18}\n","{'loss': 0.4468, 'learning_rate': 2.6000000000000002e-05, 'epoch': 0.19}\n","{'loss': 0.0155, 'learning_rate': 2.7000000000000002e-05, 'epoch': 0.2}\n","{'loss': 0.219, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.21}\n","{'loss': 0.0843, 'learning_rate': 2.9e-05, 'epoch': 0.21}\n","{'loss': 0.7634, 'learning_rate': 3e-05, 'epoch': 0.22}\n","{'loss': 0.2344, 'learning_rate': 3.1e-05, 'epoch': 0.23}\n","{'loss': 0.1874, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.23}\n","{'loss': 0.15, 'learning_rate': 3.3e-05, 'epoch': 0.24}\n","{'loss': 0.0079, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.25}\n","{'loss': 0.0384, 'learning_rate': 3.5e-05, 'epoch': 0.26}\n","{'loss': 0.0116, 'learning_rate': 3.6e-05, 'epoch': 0.26}\n","{'loss': 0.0142, 'learning_rate': 3.7e-05, 'epoch': 0.27}\n","{'loss': 0.0006, 'learning_rate': 3.8e-05, 'epoch': 0.28}\n","{'loss': 0.1302, 'learning_rate': 3.9000000000000006e-05, 'epoch': 0.29}\n","{'loss': 0.0015, 'learning_rate': 4e-05, 'epoch': 0.29}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a90158cb82b241ab8d08279c6263360d","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1362 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.10392133891582489, 'eval_recall': 0.3175, 'eval_precision': 0.3762962962962963, 'eval_f1': 0.31941958887545346, 'eval_runtime': 41.7154, 'eval_samples_per_second': 32.65, 'eval_steps_per_second': 32.65, 'epoch': 0.29}\n","{'loss': 0.0098, 'learning_rate': 4.1e-05, 'epoch': 0.3}\n","{'loss': 0.7357, 'learning_rate': 4.2e-05, 'epoch': 0.31}\n","{'loss': 0.0637, 'learning_rate': 4.3e-05, 'epoch': 0.32}\n","{'loss': 0.0098, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.32}\n","{'loss': 0.1429, 'learning_rate': 4.5e-05, 'epoch': 0.33}\n","{'loss': 0.0082, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.34}\n","{'loss': 0.1529, 'learning_rate': 4.7e-05, 'epoch': 0.35}\n","{'loss': 0.0395, 'learning_rate': 4.8e-05, 'epoch': 0.35}\n","{'loss': 0.0064, 'learning_rate': 4.9e-05, 'epoch': 0.36}\n","{'loss': 0.0006, 'learning_rate': 5e-05, 'epoch': 0.37}\n","{'loss': 0.1416, 'learning_rate': 4.977517985611511e-05, 'epoch': 0.37}\n","{'loss': 0.0045, 'learning_rate': 4.9550359712230215e-05, 'epoch': 0.38}\n","{'loss': 0.1117, 'learning_rate': 4.9325539568345325e-05, 'epoch': 0.39}\n","{'loss': 0.1467, 'learning_rate': 4.9100719424460435e-05, 'epoch': 0.4}\n","{'loss': 0.2248, 'learning_rate': 4.8875899280575545e-05, 'epoch': 0.4}\n","{'loss': 0.1233, 'learning_rate': 4.865107913669065e-05, 'epoch': 0.41}\n","{'loss': 0.1707, 'learning_rate': 4.842625899280576e-05, 'epoch': 0.42}\n","{'loss': 0.0356, 'learning_rate': 4.820143884892087e-05, 'epoch': 0.43}\n","{'loss': 0.0031, 'learning_rate': 4.797661870503598e-05, 'epoch': 0.43}\n","{'loss': 0.1253, 'learning_rate': 4.775179856115108e-05, 'epoch': 0.44}\n","{'loss': 0.056, 'learning_rate': 4.752697841726619e-05, 'epoch': 0.45}\n","{'loss': 0.0117, 'learning_rate': 4.7302158273381294e-05, 'epoch': 0.46}\n","{'loss': 0.0216, 'learning_rate': 4.7077338129496404e-05, 'epoch': 0.46}\n","{'loss': 0.003, 'learning_rate': 4.685251798561151e-05, 'epoch': 0.47}\n","{'loss': 0.4161, 'learning_rate': 4.662769784172662e-05, 'epoch': 0.48}\n","{'loss': 0.0028, 'learning_rate': 4.640287769784173e-05, 'epoch': 0.48}\n","{'loss': 0.1717, 'learning_rate': 4.617805755395684e-05, 'epoch': 0.49}\n","{'loss': 0.0451, 'learning_rate': 4.595323741007194e-05, 'epoch': 0.5}\n","{'loss': 0.0749, 'learning_rate': 4.572841726618705e-05, 'epoch': 0.51}\n","{'loss': 0.0039, 'learning_rate': 4.550359712230216e-05, 'epoch': 0.51}\n","{'loss': 0.0122, 'learning_rate': 4.527877697841727e-05, 'epoch': 0.52}\n","{'loss': 0.0864, 'learning_rate': 4.505395683453237e-05, 'epoch': 0.53}\n","{'loss': 0.062, 'learning_rate': 4.482913669064748e-05, 'epoch': 0.54}\n","{'loss': 0.0017, 'learning_rate': 4.460431654676259e-05, 'epoch': 0.54}\n","{'loss': 0.1192, 'learning_rate': 4.43794964028777e-05, 'epoch': 0.55}\n","{'loss': 0.0277, 'learning_rate': 4.4154676258992806e-05, 'epoch': 0.56}\n","{'loss': 0.0046, 'learning_rate': 4.3929856115107916e-05, 'epoch': 0.57}\n","{'loss': 0.1009, 'learning_rate': 4.3705035971223026e-05, 'epoch': 0.57}\n","{'loss': 0.056, 'learning_rate': 4.3480215827338136e-05, 'epoch': 0.58}\n","{'loss': 0.0104, 'learning_rate': 4.325539568345324e-05, 'epoch': 0.59}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cbf65ab8e2524e30b3748afdd83bb497","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1362 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.06335439532995224, 'eval_recall': 0.67625, 'eval_precision': 0.555441478439425, 'eval_f1': 0.6706398398016592, 'eval_runtime': 41.5663, 'eval_samples_per_second': 32.767, 'eval_steps_per_second': 32.767, 'epoch': 0.59}\n","{'loss': 0.0598, 'learning_rate': 4.303057553956835e-05, 'epoch': 0.59}\n","{'loss': 0.1014, 'learning_rate': 4.280575539568346e-05, 'epoch': 0.6}\n","{'loss': 0.3791, 'learning_rate': 4.258093525179856e-05, 'epoch': 0.61}\n","{'loss': 0.0031, 'learning_rate': 4.235611510791367e-05, 'epoch': 0.62}\n","{'loss': 0.0471, 'learning_rate': 4.2131294964028775e-05, 'epoch': 0.62}\n","{'loss': 0.0761, 'learning_rate': 4.1906474820143885e-05, 'epoch': 0.63}\n","{'loss': 0.0138, 'learning_rate': 4.1681654676258994e-05, 'epoch': 0.64}\n","{'loss': 0.0058, 'learning_rate': 4.14568345323741e-05, 'epoch': 0.65}\n","{'loss': 0.0016, 'learning_rate': 4.123201438848921e-05, 'epoch': 0.65}\n","{'loss': 0.0867, 'learning_rate': 4.100719424460432e-05, 'epoch': 0.66}\n","{'loss': 0.1281, 'learning_rate': 4.078237410071943e-05, 'epoch': 0.67}\n","{'loss': 0.0052, 'learning_rate': 4.055755395683453e-05, 'epoch': 0.68}\n","{'loss': 0.0007, 'learning_rate': 4.033273381294964e-05, 'epoch': 0.68}\n","{'loss': 0.1731, 'learning_rate': 4.010791366906475e-05, 'epoch': 0.69}\n","{'loss': 0.1419, 'learning_rate': 3.988309352517986e-05, 'epoch': 0.7}\n","{'loss': 0.0854, 'learning_rate': 3.965827338129496e-05, 'epoch': 0.7}\n","{'loss': 0.0018, 'learning_rate': 3.943345323741007e-05, 'epoch': 0.71}\n","{'loss': 0.0016, 'learning_rate': 3.920863309352518e-05, 'epoch': 0.72}\n","{'loss': 0.0007, 'learning_rate': 3.898381294964029e-05, 'epoch': 0.73}\n","{'loss': 0.0021, 'learning_rate': 3.8758992805755396e-05, 'epoch': 0.73}\n","{'loss': 0.0034, 'learning_rate': 3.8534172661870506e-05, 'epoch': 0.74}\n","{'loss': 0.0015, 'learning_rate': 3.8309352517985616e-05, 'epoch': 0.75}\n","{'loss': 0.0017, 'learning_rate': 3.8084532374100726e-05, 'epoch': 0.76}\n","{'loss': 0.0024, 'learning_rate': 3.785971223021583e-05, 'epoch': 0.76}\n","{'loss': 0.0003, 'learning_rate': 3.763489208633094e-05, 'epoch': 0.77}\n","{'loss': 0.0015, 'learning_rate': 3.741007194244605e-05, 'epoch': 0.78}\n","{'loss': 0.1645, 'learning_rate': 3.718525179856115e-05, 'epoch': 0.79}\n","{'loss': 0.0118, 'learning_rate': 3.696043165467626e-05, 'epoch': 0.79}\n","{'loss': 0.1443, 'learning_rate': 3.6735611510791365e-05, 'epoch': 0.8}\n","{'loss': 0.0188, 'learning_rate': 3.6510791366906475e-05, 'epoch': 0.81}\n","{'loss': 0.0052, 'learning_rate': 3.6285971223021585e-05, 'epoch': 0.81}\n","{'loss': 0.0194, 'learning_rate': 3.606115107913669e-05, 'epoch': 0.82}\n","{'loss': 0.1943, 'learning_rate': 3.58363309352518e-05, 'epoch': 0.83}\n","{'loss': 0.0012, 'learning_rate': 3.561151079136691e-05, 'epoch': 0.84}\n","{'loss': 0.0041, 'learning_rate': 3.538669064748202e-05, 'epoch': 0.84}\n","{'loss': 0.1239, 'learning_rate': 3.516187050359712e-05, 'epoch': 0.85}\n","{'loss': 0.0009, 'learning_rate': 3.493705035971223e-05, 'epoch': 0.86}\n","{'loss': 0.0014, 'learning_rate': 3.471223021582734e-05, 'epoch': 0.87}\n","{'loss': 0.0027, 'learning_rate': 3.448741007194245e-05, 'epoch': 0.87}\n","{'loss': 0.0394, 'learning_rate': 3.4262589928057554e-05, 'epoch': 0.88}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a742bf31dde84240b3c0db835845125a","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1362 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.027974816039204597, 'eval_recall': 0.88625, 'eval_precision': 0.4084101382488479, 'eval_f1': 0.8480861244019138, 'eval_runtime': 41.5889, 'eval_samples_per_second': 32.749, 'eval_steps_per_second': 32.749, 'epoch': 0.88}\n","{'loss': 0.0005, 'learning_rate': 3.4037769784172664e-05, 'epoch': 0.89}\n","{'loss': 0.0204, 'learning_rate': 3.3812949640287773e-05, 'epoch': 0.9}\n","{'loss': 0.0008, 'learning_rate': 3.358812949640288e-05, 'epoch': 0.9}\n","{'loss': 0.0034, 'learning_rate': 3.3363309352517986e-05, 'epoch': 0.91}\n","{'loss': 0.0002, 'learning_rate': 3.3138489208633096e-05, 'epoch': 0.92}\n","{'loss': 0.001, 'learning_rate': 3.2913669064748206e-05, 'epoch': 0.93}\n","{'loss': 0.2645, 'learning_rate': 3.2688848920863316e-05, 'epoch': 0.93}\n","{'loss': 0.2763, 'learning_rate': 3.246402877697842e-05, 'epoch': 0.94}\n","{'loss': 0.0692, 'learning_rate': 3.223920863309353e-05, 'epoch': 0.95}\n","{'loss': 0.0096, 'learning_rate': 3.201438848920863e-05, 'epoch': 0.95}\n","{'loss': 0.03, 'learning_rate': 3.178956834532374e-05, 'epoch': 0.96}\n","{'loss': 0.004, 'learning_rate': 3.1564748201438845e-05, 'epoch': 0.97}\n","{'loss': 0.0625, 'learning_rate': 3.1339928057553955e-05, 'epoch': 0.98}\n","{'loss': 0.3129, 'learning_rate': 3.1115107913669065e-05, 'epoch': 0.98}\n","{'loss': 0.0105, 'learning_rate': 3.0890287769784175e-05, 'epoch': 0.99}\n","{'loss': 0.0026, 'learning_rate': 3.066546762589928e-05, 'epoch': 1.0}\n","{'loss': 0.0004, 'learning_rate': 3.0440647482014388e-05, 'epoch': 1.01}\n","{'loss': 0.0374, 'learning_rate': 3.0215827338129498e-05, 'epoch': 1.01}\n","{'loss': 0.0218, 'learning_rate': 2.9991007194244608e-05, 'epoch': 1.02}\n","{'loss': 0.0144, 'learning_rate': 2.976618705035971e-05, 'epoch': 1.03}\n","{'loss': 0.0481, 'learning_rate': 2.954136690647482e-05, 'epoch': 1.04}\n","{'loss': 0.0038, 'learning_rate': 2.931654676258993e-05, 'epoch': 1.04}\n","{'loss': 0.0034, 'learning_rate': 2.909172661870504e-05, 'epoch': 1.05}\n","{'loss': 0.0411, 'learning_rate': 2.8866906474820144e-05, 'epoch': 1.06}\n","{'loss': 0.0076, 'learning_rate': 2.8642086330935254e-05, 'epoch': 1.06}\n","{'loss': 0.0578, 'learning_rate': 2.841726618705036e-05, 'epoch': 1.07}\n","{'loss': 0.0417, 'learning_rate': 2.819244604316547e-05, 'epoch': 1.08}\n","{'loss': 0.0065, 'learning_rate': 2.7967625899280573e-05, 'epoch': 1.09}\n","{'loss': 0.0014, 'learning_rate': 2.7742805755395683e-05, 'epoch': 1.09}\n","{'loss': 0.0047, 'learning_rate': 2.7517985611510793e-05, 'epoch': 1.1}\n","{'loss': 0.0009, 'learning_rate': 2.7293165467625903e-05, 'epoch': 1.11}\n","{'loss': 0.0033, 'learning_rate': 2.7068345323741006e-05, 'epoch': 1.12}\n","{'loss': 0.0031, 'learning_rate': 2.6843525179856116e-05, 'epoch': 1.12}\n","{'loss': 0.0006, 'learning_rate': 2.6618705035971226e-05, 'epoch': 1.13}\n","{'loss': 0.0014, 'learning_rate': 2.6393884892086336e-05, 'epoch': 1.14}\n","{'loss': 0.0008, 'learning_rate': 2.616906474820144e-05, 'epoch': 1.15}\n","{'loss': 0.0006, 'learning_rate': 2.594424460431655e-05, 'epoch': 1.15}\n","{'loss': 0.0002, 'learning_rate': 2.5719424460431656e-05, 'epoch': 1.16}\n","{'loss': 0.0017, 'learning_rate': 2.5494604316546765e-05, 'epoch': 1.17}\n","{'loss': 0.002, 'learning_rate': 2.526978417266187e-05, 'epoch': 1.17}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7874f92439fb4962ba38b56b8bd84bb9","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1362 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.05134327709674835, 'eval_recall': 0.755, 'eval_precision': 0.6357894736842106, 'eval_f1': 0.7495942720763724, 'eval_runtime': 41.6338, 'eval_samples_per_second': 32.714, 'eval_steps_per_second': 32.714, 'epoch': 1.17}\n","{'loss': 0.0015, 'learning_rate': 2.504496402877698e-05, 'epoch': 1.18}\n","{'loss': 0.0032, 'learning_rate': 2.482014388489209e-05, 'epoch': 1.19}\n","{'loss': 0.161, 'learning_rate': 2.4595323741007195e-05, 'epoch': 1.2}\n","{'loss': 0.0004, 'learning_rate': 2.4370503597122305e-05, 'epoch': 1.2}\n","{'loss': 0.1445, 'learning_rate': 2.414568345323741e-05, 'epoch': 1.21}\n","{'loss': 0.0005, 'learning_rate': 2.392086330935252e-05, 'epoch': 1.22}\n","{'loss': 0.0006, 'learning_rate': 2.3696043165467628e-05, 'epoch': 1.23}\n","{'loss': 0.0011, 'learning_rate': 2.3471223021582738e-05, 'epoch': 1.23}\n","{'loss': 0.0004, 'learning_rate': 2.3246402877697844e-05, 'epoch': 1.24}\n","{'loss': 0.1584, 'learning_rate': 2.302158273381295e-05, 'epoch': 1.25}\n","{'loss': 0.0016, 'learning_rate': 2.2796762589928057e-05, 'epoch': 1.26}\n","{'loss': 0.0002, 'learning_rate': 2.2571942446043167e-05, 'epoch': 1.26}\n","{'loss': 0.3392, 'learning_rate': 2.2347122302158274e-05, 'epoch': 1.27}\n","{'loss': 0.0022, 'learning_rate': 2.2122302158273384e-05, 'epoch': 1.28}\n","{'loss': 0.0013, 'learning_rate': 2.189748201438849e-05, 'epoch': 1.28}\n","{'loss': 0.0003, 'learning_rate': 2.16726618705036e-05, 'epoch': 1.29}\n","{'loss': 0.0685, 'learning_rate': 2.1447841726618707e-05, 'epoch': 1.3}\n","{'loss': 0.0007, 'learning_rate': 2.1223021582733816e-05, 'epoch': 1.31}\n","{'loss': 0.0172, 'learning_rate': 2.0998201438848923e-05, 'epoch': 1.31}\n","{'loss': 0.0067, 'learning_rate': 2.077338129496403e-05, 'epoch': 1.32}\n","{'loss': 0.0016, 'learning_rate': 2.0548561151079136e-05, 'epoch': 1.33}\n","{'loss': 0.0008, 'learning_rate': 2.0323741007194246e-05, 'epoch': 1.34}\n","{'loss': 0.0009, 'learning_rate': 2.0098920863309352e-05, 'epoch': 1.34}\n","{'loss': 0.0005, 'learning_rate': 1.9874100719424462e-05, 'epoch': 1.35}\n","{'loss': 0.0001, 'learning_rate': 1.964928057553957e-05, 'epoch': 1.36}\n","{'loss': 0.0003, 'learning_rate': 1.942446043165468e-05, 'epoch': 1.37}\n","{'loss': 0.1141, 'learning_rate': 1.9199640287769785e-05, 'epoch': 1.37}\n","{'loss': 0.0526, 'learning_rate': 1.8974820143884895e-05, 'epoch': 1.38}\n","{'loss': 0.0014, 'learning_rate': 1.8750000000000002e-05, 'epoch': 1.39}\n","{'loss': 0.0045, 'learning_rate': 1.8525179856115108e-05, 'epoch': 1.4}\n","{'loss': 0.0001, 'learning_rate': 1.8300359712230218e-05, 'epoch': 1.4}\n","{'loss': 0.2723, 'learning_rate': 1.8075539568345325e-05, 'epoch': 1.41}\n","{'loss': 0.0879, 'learning_rate': 1.785071942446043e-05, 'epoch': 1.42}\n","{'loss': 0.004, 'learning_rate': 1.7625899280575538e-05, 'epoch': 1.42}\n","{'loss': 0.0972, 'learning_rate': 1.7401079136690648e-05, 'epoch': 1.43}\n","{'loss': 0.0049, 'learning_rate': 1.7176258992805754e-05, 'epoch': 1.44}\n","{'loss': 0.0157, 'learning_rate': 1.6951438848920864e-05, 'epoch': 1.45}\n","{'loss': 0.0045, 'learning_rate': 1.672661870503597e-05, 'epoch': 1.45}\n","{'loss': 0.0021, 'learning_rate': 1.650179856115108e-05, 'epoch': 1.46}\n","{'loss': 0.0016, 'learning_rate': 1.6276978417266187e-05, 'epoch': 1.47}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a86fc4d8ae024848aea7a89446172842","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1362 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.0243091844022274, 'eval_recall': 0.93875, 'eval_precision': 0.7159199237368923, 'eval_f1': 0.9276450187657371, 'eval_runtime': 46.0446, 'eval_samples_per_second': 29.58, 'eval_steps_per_second': 29.58, 'epoch': 1.47}\n","{'loss': 0.1758, 'learning_rate': 1.6052158273381297e-05, 'epoch': 1.48}\n","{'loss': 0.0053, 'learning_rate': 1.5827338129496403e-05, 'epoch': 1.48}\n","{'loss': 0.0003, 'learning_rate': 1.5602517985611513e-05, 'epoch': 1.49}\n","{'loss': 0.0025, 'learning_rate': 1.537769784172662e-05, 'epoch': 1.5}\n","{'loss': 0.0073, 'learning_rate': 1.5152877697841728e-05, 'epoch': 1.51}\n","{'loss': 0.001, 'learning_rate': 1.4928057553956835e-05, 'epoch': 1.51}\n","{'loss': 0.0011, 'learning_rate': 1.4703237410071943e-05, 'epoch': 1.52}\n","{'loss': 0.0011, 'learning_rate': 1.447841726618705e-05, 'epoch': 1.53}\n","{'loss': 0.001, 'learning_rate': 1.425359712230216e-05, 'epoch': 1.53}\n","{'loss': 0.3002, 'learning_rate': 1.4028776978417266e-05, 'epoch': 1.54}\n","{'loss': 0.0014, 'learning_rate': 1.3803956834532376e-05, 'epoch': 1.55}\n","{'loss': 0.0021, 'learning_rate': 1.3579136690647482e-05, 'epoch': 1.56}\n","{'loss': 0.0004, 'learning_rate': 1.335431654676259e-05, 'epoch': 1.56}\n","{'loss': 0.0003, 'learning_rate': 1.3129496402877697e-05, 'epoch': 1.57}\n","{'loss': 0.0002, 'learning_rate': 1.2904676258992807e-05, 'epoch': 1.58}\n","{'loss': 0.0002, 'learning_rate': 1.2679856115107913e-05, 'epoch': 1.59}\n","{'loss': 0.0014, 'learning_rate': 1.2455035971223023e-05, 'epoch': 1.59}\n","{'loss': 0.0012, 'learning_rate': 1.223021582733813e-05, 'epoch': 1.6}\n","{'loss': 0.0006, 'learning_rate': 1.2005395683453238e-05, 'epoch': 1.61}\n","{'loss': 0.1566, 'learning_rate': 1.1780575539568346e-05, 'epoch': 1.62}\n","{'loss': 0.0008, 'learning_rate': 1.1555755395683454e-05, 'epoch': 1.62}\n","{'loss': 0.0195, 'learning_rate': 1.1330935251798563e-05, 'epoch': 1.63}\n","{'loss': 0.0043, 'learning_rate': 1.1106115107913669e-05, 'epoch': 1.64}\n","{'loss': 0.0012, 'learning_rate': 1.0881294964028777e-05, 'epoch': 1.64}\n","{'loss': 0.0006, 'learning_rate': 1.0656474820143886e-05, 'epoch': 1.65}\n","{'loss': 0.0029, 'learning_rate': 1.0431654676258994e-05, 'epoch': 1.66}\n","{'loss': 0.0015, 'learning_rate': 1.0206834532374102e-05, 'epoch': 1.67}\n","{'loss': 0.0007, 'learning_rate': 9.98201438848921e-06, 'epoch': 1.67}\n","{'loss': 0.0003, 'learning_rate': 9.757194244604317e-06, 'epoch': 1.68}\n","{'loss': 0.0009, 'learning_rate': 9.532374100719425e-06, 'epoch': 1.69}\n","{'loss': 0.0013, 'learning_rate': 9.307553956834533e-06, 'epoch': 1.7}\n","{'loss': 0.0013, 'learning_rate': 9.082733812949641e-06, 'epoch': 1.7}\n","{'loss': 0.0027, 'learning_rate': 8.85791366906475e-06, 'epoch': 1.71}\n","{'loss': 0.0015, 'learning_rate': 8.633093525179858e-06, 'epoch': 1.72}\n","{'loss': 0.0001, 'learning_rate': 8.408273381294964e-06, 'epoch': 1.73}\n","{'loss': 0.0015, 'learning_rate': 8.183453237410073e-06, 'epoch': 1.73}\n","{'loss': 0.0009, 'learning_rate': 7.95863309352518e-06, 'epoch': 1.74}\n","{'loss': 0.0033, 'learning_rate': 7.733812949640289e-06, 'epoch': 1.75}\n","{'loss': 0.0001, 'learning_rate': 7.508992805755396e-06, 'epoch': 1.75}\n","{'loss': 0.0004, 'learning_rate': 7.2841726618705045e-06, 'epoch': 1.76}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"090d59421359406cad1275de3c6ce818","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1362 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.033096399158239365, 'eval_recall': 0.92125, 'eval_precision': 0.7505091649694501, 'eval_f1': 0.9132589838909542, 'eval_runtime': 41.6766, 'eval_samples_per_second': 32.68, 'eval_steps_per_second': 32.68, 'epoch': 1.76}\n","{'loss': 0.0447, 'learning_rate': 7.059352517985613e-06, 'epoch': 1.77}\n","{'loss': 0.002, 'learning_rate': 6.83453237410072e-06, 'epoch': 1.78}\n","{'loss': 0.0023, 'learning_rate': 6.609712230215828e-06, 'epoch': 1.78}\n","{'loss': 0.0011, 'learning_rate': 6.384892086330936e-06, 'epoch': 1.79}\n","{'loss': 0.0059, 'learning_rate': 6.160071942446043e-06, 'epoch': 1.8}\n","{'loss': 0.0011, 'learning_rate': 5.935251798561151e-06, 'epoch': 1.81}\n","{'loss': 0.0001, 'learning_rate': 5.7104316546762595e-06, 'epoch': 1.81}\n","{'loss': 0.0, 'learning_rate': 5.485611510791367e-06, 'epoch': 1.82}\n","{'loss': 0.0013, 'learning_rate': 5.260791366906475e-06, 'epoch': 1.83}\n","{'loss': 0.0006, 'learning_rate': 5.035971223021583e-06, 'epoch': 1.84}\n","{'loss': 0.0032, 'learning_rate': 4.811151079136691e-06, 'epoch': 1.84}\n","{'loss': 0.0972, 'learning_rate': 4.586330935251799e-06, 'epoch': 1.85}\n","{'loss': 0.1498, 'learning_rate': 4.361510791366906e-06, 'epoch': 1.86}\n","{'loss': 0.0172, 'learning_rate': 4.1366906474820145e-06, 'epoch': 1.86}\n","{'loss': 0.1427, 'learning_rate': 3.911870503597123e-06, 'epoch': 1.87}\n","{'loss': 0.03, 'learning_rate': 3.6870503597122305e-06, 'epoch': 1.88}\n","{'loss': 0.003, 'learning_rate': 3.4622302158273383e-06, 'epoch': 1.89}\n","{'loss': 0.0001, 'learning_rate': 3.237410071942446e-06, 'epoch': 1.89}\n","{'loss': 0.0031, 'learning_rate': 3.0125899280575543e-06, 'epoch': 1.9}\n","{'loss': 0.0015, 'learning_rate': 2.787769784172662e-06, 'epoch': 1.91}\n","{'loss': 0.0032, 'learning_rate': 2.56294964028777e-06, 'epoch': 1.92}\n","{'loss': 0.0007, 'learning_rate': 2.338129496402878e-06, 'epoch': 1.92}\n","{'loss': 0.0037, 'learning_rate': 2.113309352517986e-06, 'epoch': 1.93}\n","{'loss': 0.0575, 'learning_rate': 1.8884892086330936e-06, 'epoch': 1.94}\n","{'loss': 0.0003, 'learning_rate': 1.6636690647482016e-06, 'epoch': 1.95}\n","{'loss': 0.0016, 'learning_rate': 1.4388489208633094e-06, 'epoch': 1.95}\n","{'loss': 0.0049, 'learning_rate': 1.2140287769784174e-06, 'epoch': 1.96}\n","{'loss': 0.0027, 'learning_rate': 9.892086330935252e-07, 'epoch': 1.97}\n","{'loss': 0.0011, 'learning_rate': 7.643884892086331e-07, 'epoch': 1.98}\n","{'loss': 0.0031, 'learning_rate': 5.395683453237411e-07, 'epoch': 1.98}\n","{'loss': 0.0012, 'learning_rate': 3.1474820143884896e-07, 'epoch': 1.99}\n","{'loss': 0.0026, 'learning_rate': 8.992805755395684e-08, 'epoch': 2.0}\n","{'train_runtime': 1134.9922, 'train_samples_per_second': 9.595, 'train_steps_per_second': 2.4, 'train_loss': 0.11839598379236646, 'epoch': 2.0}\n","Shutting down background jobs, please wait a moment...\n","Done!\n","Waiting for the remaining 9 operations to synchronize with Neptune. Do not kill this process.\n","All 9 operations synced, thanks for waiting!\n","Explore the metadata in the Neptune app:\n","https://app.neptune.ai/bernd.heidemann/PII/e/PII-220/metadata\n"]},{"data":{"text/plain":["TrainOutput(global_step=2724, training_loss=0.11839598379236646, metrics={'train_runtime': 1134.9922, 'train_samples_per_second': 9.595, 'train_steps_per_second': 2.4, 'train_loss': 0.11839598379236646, 'epoch': 2.0})"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["# set environment variables: TOKENIZERS_PARALLELISM=false\n","import os\n","os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n","\n","tokenizer = AutoTokenizer.from_pretrained(parameter[\"model\"])\n","tokenizer.add_tokens(AddedToken(\"\\n\", normalized=False))\n","\n","model = AutoModelForTokenClassification.from_pretrained(\n","    parameter[\"model\"],\n","    num_labels=len(all_labels),\n","    id2label=id2label,\n","    label2id=label2id,\n","    ignore_mismatched_sizes=True\n",")\n","\n","if parameter['freeze_embeddings']:\n","    for param in model.deberta.embeddings.parameters():\n","        param.requires_grad = False\n","        \n","if parameter['freeze_layers'] > 0:\n","    for layer in model.deberta.encoder.layer[:parameter['freeze_layers']]:\n","        for param in layer.parameters():\n","            param.requires_grad = False\n","\n","print(model.config)\n","#my_model=MyModel(parameter['model'], len(label2id))\n","\n","trainer=get_trainer(model, train_ds, valid_ds)\n","#trainer.set_lr(0.1)\n","trainer.train()"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","c:\\Users\\Bernd\\anaconda3\\envs\\mytorch\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:470: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n","  warnings.warn(\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"name":"stdout","output_type":"stream","text":["    row_id  document  token           label\n","0        0         7      9  B-NAME_STUDENT\n","1        1         7     10  I-NAME_STUDENT\n","2        2         7    482  B-NAME_STUDENT\n","3        3         7    483  I-NAME_STUDENT\n","4        4         7    741  B-NAME_STUDENT\n","5        5         7    742  I-NAME_STUDENT\n","6        6        10      0  B-NAME_STUDENT\n","7        7        10      1  I-NAME_STUDENT\n","8        8        10    464  B-NAME_STUDENT\n","9        9        10    465  I-NAME_STUDENT\n","10      10        16      4  B-NAME_STUDENT\n","11      11        16      5  I-NAME_STUDENT\n","12      12        20      5  B-NAME_STUDENT\n","13      13        20      6  I-NAME_STUDENT\n","14      14        20      8  I-NAME_STUDENT\n","15      15        56     12  B-NAME_STUDENT\n","16      16        56     13  I-NAME_STUDENT\n","17      17        86      6  B-NAME_STUDENT\n","18      18        86      7  I-NAME_STUDENT\n","19      19        93      0  B-NAME_STUDENT\n","20      20        93      1  I-NAME_STUDENT\n","21      21       104      7  B-NAME_STUDENT\n","22      22       104      8  B-NAME_STUDENT\n","23      23       104      9  I-NAME_STUDENT\n","24      24       112      5  B-NAME_STUDENT\n","25      25       112      6  I-NAME_STUDENT\n","26      26       123     32  B-NAME_STUDENT\n","27      27       123     33  I-NAME_STUDENT\n","28      28       123   1500  B-NAME_STUDENT\n","29      29       123   1575  B-URL_PERSONAL\n","30      30       123   1575        B-ID_NUM\n","31      31       123   1648  B-URL_PERSONAL\n"]}],"source":["create_submission(model, f\"submission.csv\")"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\Bernd\\anaconda3\\envs\\mytorch\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e03f0fc2fa19452eba5ffcfa575ed2f8","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2724 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'loss': 0.0016, 'learning_rate': 1.0000000000000001e-07, 'epoch': 0.01}\n","{'loss': 0.0327, 'learning_rate': 2.0000000000000002e-07, 'epoch': 0.01}\n","{'loss': 0.0356, 'learning_rate': 3.0000000000000004e-07, 'epoch': 0.02}\n","{'loss': 0.0005, 'learning_rate': 4.0000000000000003e-07, 'epoch': 0.03}\n","{'loss': 0.0002, 'learning_rate': 5.000000000000001e-07, 'epoch': 0.04}\n","{'loss': 0.0008, 'learning_rate': 6.000000000000001e-07, 'epoch': 0.04}\n","{'loss': 0.0004, 'learning_rate': 7.000000000000001e-07, 'epoch': 0.05}\n","{'loss': 0.0029, 'learning_rate': 8.000000000000001e-07, 'epoch': 0.06}\n","{'loss': 0.0014, 'learning_rate': 9.000000000000001e-07, 'epoch': 0.07}\n","{'loss': 0.0005, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.07}\n","{'loss': 0.0001, 'learning_rate': 1.1e-06, 'epoch': 0.08}\n","{'loss': 0.0013, 'learning_rate': 1.2000000000000002e-06, 'epoch': 0.09}\n","{'loss': 0.0006, 'learning_rate': 1.3e-06, 'epoch': 0.1}\n","{'loss': 0.0019, 'learning_rate': 1.4000000000000001e-06, 'epoch': 0.1}\n","{'loss': 0.0002, 'learning_rate': 1.5e-06, 'epoch': 0.11}\n","{'loss': 0.004, 'learning_rate': 1.6000000000000001e-06, 'epoch': 0.12}\n","{'loss': 0.0001, 'learning_rate': 1.7000000000000002e-06, 'epoch': 0.12}\n","{'loss': 0.0291, 'learning_rate': 1.8000000000000001e-06, 'epoch': 0.13}\n","{'loss': 0.0008, 'learning_rate': 1.9000000000000002e-06, 'epoch': 0.14}\n","{'loss': 0.0006, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.15}\n","{'loss': 0.0005, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.15}\n","{'loss': 0.0005, 'learning_rate': 2.2e-06, 'epoch': 0.16}\n","{'loss': 0.0004, 'learning_rate': 2.3000000000000004e-06, 'epoch': 0.17}\n","{'loss': 0.1313, 'learning_rate': 2.4000000000000003e-06, 'epoch': 0.18}\n","{'loss': 0.0737, 'learning_rate': 2.5e-06, 'epoch': 0.18}\n","{'loss': 0.0004, 'learning_rate': 2.6e-06, 'epoch': 0.19}\n","{'loss': 0.0013, 'learning_rate': 2.7000000000000004e-06, 'epoch': 0.2}\n","{'loss': 0.0004, 'learning_rate': 2.8000000000000003e-06, 'epoch': 0.21}\n","{'loss': 0.002, 'learning_rate': 2.9e-06, 'epoch': 0.21}\n","{'loss': 0.0003, 'learning_rate': 3e-06, 'epoch': 0.22}\n","{'loss': 0.0002, 'learning_rate': 3.1000000000000004e-06, 'epoch': 0.23}\n","{'loss': 0.1161, 'learning_rate': 3.2000000000000003e-06, 'epoch': 0.23}\n","{'loss': 0.0496, 'learning_rate': 3.3000000000000006e-06, 'epoch': 0.24}\n","{'loss': 0.0001, 'learning_rate': 3.4000000000000005e-06, 'epoch': 0.25}\n","{'loss': 0.0017, 'learning_rate': 3.5e-06, 'epoch': 0.26}\n","{'loss': 0.0001, 'learning_rate': 3.6000000000000003e-06, 'epoch': 0.26}\n","{'loss': 0.0001, 'learning_rate': 3.7e-06, 'epoch': 0.27}\n","{'loss': 0.0003, 'learning_rate': 3.8000000000000005e-06, 'epoch': 0.28}\n","{'loss': 0.0001, 'learning_rate': 3.900000000000001e-06, 'epoch': 0.29}\n","{'loss': 0.0003, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.29}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7e4a4eab34ca44f5a0ac951967ebab5f","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1362 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.04549077898263931, 'eval_recall': 0.7625, 'eval_precision': 0.7973856209150327, 'eval_f1': 0.7637852155068625, 'eval_runtime': 42.4079, 'eval_samples_per_second': 32.117, 'eval_steps_per_second': 32.117, 'epoch': 0.29}\n","{'loss': 0.0001, 'learning_rate': 4.1e-06, 'epoch': 0.3}\n","{'loss': 0.0017, 'learning_rate': 4.2000000000000004e-06, 'epoch': 0.31}\n","{'loss': 0.001, 'learning_rate': 4.3e-06, 'epoch': 0.32}\n","{'loss': 0.002, 'learning_rate': 4.4e-06, 'epoch': 0.32}\n","{'loss': 0.0005, 'learning_rate': 4.5e-06, 'epoch': 0.33}\n","{'loss': 0.0001, 'learning_rate': 4.600000000000001e-06, 'epoch': 0.34}\n","{'loss': 0.0007, 'learning_rate': 4.7e-06, 'epoch': 0.35}\n","{'loss': 0.0001, 'learning_rate': 4.800000000000001e-06, 'epoch': 0.35}\n","{'loss': 0.0002, 'learning_rate': 4.9000000000000005e-06, 'epoch': 0.36}\n","{'loss': 0.0008, 'learning_rate': 5e-06, 'epoch': 0.37}\n","{'loss': 0.0008, 'learning_rate': 4.977517985611512e-06, 'epoch': 0.37}\n","{'loss': 0.0008, 'learning_rate': 4.955035971223021e-06, 'epoch': 0.38}\n","{'loss': 0.0114, 'learning_rate': 4.932553956834533e-06, 'epoch': 0.39}\n","{'loss': 0.0003, 'learning_rate': 4.910071942446043e-06, 'epoch': 0.4}\n","{'loss': 0.2453, 'learning_rate': 4.8875899280575545e-06, 'epoch': 0.4}\n","{'loss': 0.0677, 'learning_rate': 4.865107913669065e-06, 'epoch': 0.41}\n","{'loss': 0.0013, 'learning_rate': 4.8426258992805755e-06, 'epoch': 0.42}\n","{'loss': 0.0019, 'learning_rate': 4.820143884892087e-06, 'epoch': 0.43}\n","{'loss': 0.0002, 'learning_rate': 4.797661870503597e-06, 'epoch': 0.43}\n","{'loss': 0.0049, 'learning_rate': 4.775179856115108e-06, 'epoch': 0.44}\n","{'loss': 0.003, 'learning_rate': 4.752697841726619e-06, 'epoch': 0.45}\n","{'loss': 0.0207, 'learning_rate': 4.73021582733813e-06, 'epoch': 0.46}\n","{'loss': 0.0003, 'learning_rate': 4.707733812949641e-06, 'epoch': 0.46}\n","{'loss': 0.0002, 'learning_rate': 4.685251798561151e-06, 'epoch': 0.47}\n","{'loss': 0.0002, 'learning_rate': 4.662769784172662e-06, 'epoch': 0.48}\n","{'loss': 0.0001, 'learning_rate': 4.640287769784173e-06, 'epoch': 0.48}\n","{'loss': 0.0009, 'learning_rate': 4.617805755395684e-06, 'epoch': 0.49}\n","{'loss': 0.0008, 'learning_rate': 4.595323741007194e-06, 'epoch': 0.5}\n","{'loss': 0.0001, 'learning_rate': 4.5728417266187055e-06, 'epoch': 0.51}\n","{'loss': 0.0001, 'learning_rate': 4.550359712230216e-06, 'epoch': 0.51}\n","{'loss': 0.0007, 'learning_rate': 4.527877697841727e-06, 'epoch': 0.52}\n","{'loss': 0.0002, 'learning_rate': 4.505395683453238e-06, 'epoch': 0.53}\n","{'loss': 0.0009, 'learning_rate': 4.482913669064748e-06, 'epoch': 0.54}\n","{'loss': 0.0022, 'learning_rate': 4.46043165467626e-06, 'epoch': 0.54}\n","{'loss': 0.0009, 'learning_rate': 4.43794964028777e-06, 'epoch': 0.55}\n","{'loss': 0.008, 'learning_rate': 4.415467625899281e-06, 'epoch': 0.56}\n","{'loss': 0.0024, 'learning_rate': 4.392985611510792e-06, 'epoch': 0.57}\n","{'loss': 0.0013, 'learning_rate': 4.370503597122302e-06, 'epoch': 0.57}\n","{'loss': 0.0009, 'learning_rate': 4.348021582733814e-06, 'epoch': 0.58}\n","{'loss': 0.0003, 'learning_rate': 4.325539568345324e-06, 'epoch': 0.59}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d198c8a98e304ada865324267462679e","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1362 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.04293151944875717, 'eval_recall': 0.85, 'eval_precision': 0.8232445520581114, 'eval_f1': 0.8489388264669161, 'eval_runtime': 42.3859, 'eval_samples_per_second': 32.133, 'eval_steps_per_second': 32.133, 'epoch': 0.59}\n","{'loss': 0.0009, 'learning_rate': 4.303057553956835e-06, 'epoch': 0.59}\n","{'loss': 0.0061, 'learning_rate': 4.280575539568346e-06, 'epoch': 0.6}\n","{'loss': 0.0034, 'learning_rate': 4.2580935251798565e-06, 'epoch': 0.61}\n","{'loss': 0.0004, 'learning_rate': 4.235611510791367e-06, 'epoch': 0.62}\n","{'loss': 0.0012, 'learning_rate': 4.213129496402878e-06, 'epoch': 0.62}\n","{'loss': 0.0002, 'learning_rate': 4.190647482014389e-06, 'epoch': 0.63}\n","{'loss': 0.0002, 'learning_rate': 4.1681654676259e-06, 'epoch': 0.64}\n","{'loss': 0.0016, 'learning_rate': 4.14568345323741e-06, 'epoch': 0.65}\n","{'loss': 0.0003, 'learning_rate': 4.123201438848921e-06, 'epoch': 0.65}\n","{'loss': 0.001, 'learning_rate': 4.100719424460432e-06, 'epoch': 0.66}\n","{'loss': 0.0004, 'learning_rate': 4.078237410071943e-06, 'epoch': 0.67}\n","{'loss': 0.0001, 'learning_rate': 4.055755395683453e-06, 'epoch': 0.68}\n","{'loss': 0.0, 'learning_rate': 4.033273381294964e-06, 'epoch': 0.68}\n","{'loss': 0.0765, 'learning_rate': 4.010791366906475e-06, 'epoch': 0.69}\n","{'loss': 0.001, 'learning_rate': 3.9883093525179865e-06, 'epoch': 0.7}\n","{'loss': 0.0003, 'learning_rate': 3.965827338129496e-06, 'epoch': 0.7}\n","{'loss': 0.0005, 'learning_rate': 3.9433453237410075e-06, 'epoch': 0.71}\n","{'loss': 0.0009, 'learning_rate': 3.920863309352518e-06, 'epoch': 0.72}\n","{'loss': 0.0001, 'learning_rate': 3.898381294964029e-06, 'epoch': 0.73}\n","{'loss': 0.0, 'learning_rate': 3.87589928057554e-06, 'epoch': 0.73}\n","{'loss': 0.0005, 'learning_rate': 3.85341726618705e-06, 'epoch': 0.74}\n","{'loss': 0.0001, 'learning_rate': 3.830935251798562e-06, 'epoch': 0.75}\n","{'loss': 0.0, 'learning_rate': 3.8084532374100725e-06, 'epoch': 0.76}\n","{'loss': 0.0125, 'learning_rate': 3.785971223021583e-06, 'epoch': 0.76}\n","{'loss': 0.0001, 'learning_rate': 3.763489208633094e-06, 'epoch': 0.77}\n","{'loss': 0.0006, 'learning_rate': 3.741007194244605e-06, 'epoch': 0.78}\n","{'loss': 0.3312, 'learning_rate': 3.7185251798561157e-06, 'epoch': 0.79}\n","{'loss': 0.0001, 'learning_rate': 3.696043165467626e-06, 'epoch': 0.79}\n","{'loss': 0.0184, 'learning_rate': 3.673561151079137e-06, 'epoch': 0.8}\n","{'loss': 0.0004, 'learning_rate': 3.651079136690648e-06, 'epoch': 0.81}\n","{'loss': 0.0002, 'learning_rate': 3.628597122302159e-06, 'epoch': 0.81}\n","{'loss': 0.0004, 'learning_rate': 3.606115107913669e-06, 'epoch': 0.82}\n","{'loss': 0.0055, 'learning_rate': 3.58363309352518e-06, 'epoch': 0.83}\n","{'loss': 0.0001, 'learning_rate': 3.561151079136691e-06, 'epoch': 0.84}\n","{'loss': 0.0007, 'learning_rate': 3.538669064748202e-06, 'epoch': 0.84}\n","{'loss': 0.1053, 'learning_rate': 3.516187050359712e-06, 'epoch': 0.85}\n","{'loss': 0.0007, 'learning_rate': 3.493705035971223e-06, 'epoch': 0.86}\n","{'loss': 0.0001, 'learning_rate': 3.471223021582734e-06, 'epoch': 0.87}\n","{'loss': 0.0004, 'learning_rate': 3.4487410071942453e-06, 'epoch': 0.87}\n","{'loss': 0.0004, 'learning_rate': 3.4262589928057554e-06, 'epoch': 0.88}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7a8a8212d5d84744bcb1d7ce2455b18b","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1362 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.03278534114360809, 'eval_recall': 0.83125, 'eval_precision': 0.7983193277310925, 'eval_f1': 0.8299332789324629, 'eval_runtime': 42.4533, 'eval_samples_per_second': 32.082, 'eval_steps_per_second': 32.082, 'epoch': 0.88}\n","{'loss': 0.0, 'learning_rate': 3.4037769784172663e-06, 'epoch': 0.89}\n","{'loss': 0.166, 'learning_rate': 3.381294964028777e-06, 'epoch': 0.9}\n","{'loss': 0.0001, 'learning_rate': 3.358812949640288e-06, 'epoch': 0.9}\n","{'loss': 0.0004, 'learning_rate': 3.3363309352517986e-06, 'epoch': 0.91}\n","{'loss': 0.0, 'learning_rate': 3.3138489208633095e-06, 'epoch': 0.92}\n","{'loss': 0.0005, 'learning_rate': 3.2913669064748204e-06, 'epoch': 0.93}\n","{'loss': 0.0009, 'learning_rate': 3.2688848920863313e-06, 'epoch': 0.93}\n","{'loss': 0.0074, 'learning_rate': 3.2464028776978418e-06, 'epoch': 0.94}\n","{'loss': 0.0016, 'learning_rate': 3.2239208633093527e-06, 'epoch': 0.95}\n","{'loss': 0.0002, 'learning_rate': 3.2014388489208636e-06, 'epoch': 0.95}\n","{'loss': 0.0029, 'learning_rate': 3.1789568345323745e-06, 'epoch': 0.96}\n","{'loss': 0.0443, 'learning_rate': 3.156474820143885e-06, 'epoch': 0.97}\n","{'loss': 0.0329, 'learning_rate': 3.133992805755396e-06, 'epoch': 0.98}\n","{'loss': 0.1412, 'learning_rate': 3.1115107913669068e-06, 'epoch': 0.98}\n","{'loss': 0.0051, 'learning_rate': 3.0890287769784177e-06, 'epoch': 0.99}\n","{'loss': 0.0017, 'learning_rate': 3.066546762589928e-06, 'epoch': 1.0}\n","{'loss': 0.0006, 'learning_rate': 3.044064748201439e-06, 'epoch': 1.01}\n","{'loss': 0.0, 'learning_rate': 3.02158273381295e-06, 'epoch': 1.01}\n","{'loss': 0.0024, 'learning_rate': 2.999100719424461e-06, 'epoch': 1.02}\n","{'loss': 0.0001, 'learning_rate': 2.9766187050359714e-06, 'epoch': 1.03}\n","{'loss': 0.001, 'learning_rate': 2.9541366906474823e-06, 'epoch': 1.04}\n","{'loss': 0.0016, 'learning_rate': 2.931654676258993e-06, 'epoch': 1.04}\n","{'loss': 0.0001, 'learning_rate': 2.909172661870504e-06, 'epoch': 1.05}\n","{'loss': 0.0004, 'learning_rate': 2.8866906474820146e-06, 'epoch': 1.06}\n","{'loss': 0.0018, 'learning_rate': 2.8642086330935255e-06, 'epoch': 1.06}\n","{'loss': 0.0001, 'learning_rate': 2.8417266187050364e-06, 'epoch': 1.07}\n","{'loss': 0.0002, 'learning_rate': 2.8192446043165473e-06, 'epoch': 1.08}\n","{'loss': 0.0003, 'learning_rate': 2.7967625899280578e-06, 'epoch': 1.09}\n","{'loss': 0.0005, 'learning_rate': 2.7742805755395687e-06, 'epoch': 1.09}\n","{'loss': 0.0376, 'learning_rate': 2.7517985611510796e-06, 'epoch': 1.1}\n","{'loss': 0.0003, 'learning_rate': 2.7293165467625905e-06, 'epoch': 1.11}\n","{'loss': 0.0004, 'learning_rate': 2.706834532374101e-06, 'epoch': 1.12}\n","{'loss': 0.0024, 'learning_rate': 2.684352517985612e-06, 'epoch': 1.12}\n","{'loss': 0.0004, 'learning_rate': 2.6618705035971228e-06, 'epoch': 1.13}\n","{'loss': 0.034, 'learning_rate': 2.6393884892086337e-06, 'epoch': 1.14}\n","{'loss': 0.0005, 'learning_rate': 2.6169064748201437e-06, 'epoch': 1.15}\n","{'loss': 0.0007, 'learning_rate': 2.5944244604316547e-06, 'epoch': 1.15}\n","{'loss': 0.0001, 'learning_rate': 2.571942446043166e-06, 'epoch': 1.16}\n","{'loss': 0.0006, 'learning_rate': 2.549460431654677e-06, 'epoch': 1.17}\n","{'loss': 0.0009, 'learning_rate': 2.526978417266187e-06, 'epoch': 1.17}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cb523739adde4fbabf52f1ad53587a03","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1362 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.03456269949674606, 'eval_recall': 0.8375, 'eval_precision': 0.776361529548088, 'eval_f1': 0.8349710012941572, 'eval_runtime': 42.3206, 'eval_samples_per_second': 32.183, 'eval_steps_per_second': 32.183, 'epoch': 1.17}\n","{'loss': 0.0006, 'learning_rate': 2.504496402877698e-06, 'epoch': 1.18}\n","{'loss': 0.0002, 'learning_rate': 2.4820143884892088e-06, 'epoch': 1.19}\n","{'loss': 0.0487, 'learning_rate': 2.4595323741007197e-06, 'epoch': 1.2}\n","{'loss': 0.0002, 'learning_rate': 2.4370503597122306e-06, 'epoch': 1.2}\n","{'loss': 0.0169, 'learning_rate': 2.414568345323741e-06, 'epoch': 1.21}\n","{'loss': 0.0001, 'learning_rate': 2.392086330935252e-06, 'epoch': 1.22}\n","{'loss': 0.0001, 'learning_rate': 2.369604316546763e-06, 'epoch': 1.23}\n","{'loss': 0.0017, 'learning_rate': 2.3471223021582738e-06, 'epoch': 1.23}\n","{'loss': 0.0001, 'learning_rate': 2.3246402877697843e-06, 'epoch': 1.24}\n","{'loss': 0.0551, 'learning_rate': 2.302158273381295e-06, 'epoch': 1.25}\n","{'loss': 0.0001, 'learning_rate': 2.279676258992806e-06, 'epoch': 1.26}\n","{'loss': 0.0001, 'learning_rate': 2.257194244604317e-06, 'epoch': 1.26}\n","{'loss': 0.2853, 'learning_rate': 2.2347122302158275e-06, 'epoch': 1.27}\n","{'loss': 0.0009, 'learning_rate': 2.2122302158273384e-06, 'epoch': 1.28}\n","{'loss': 0.0013, 'learning_rate': 2.1897482014388493e-06, 'epoch': 1.28}\n","{'loss': 0.0, 'learning_rate': 2.16726618705036e-06, 'epoch': 1.29}\n","{'loss': 0.0004, 'learning_rate': 2.1447841726618707e-06, 'epoch': 1.3}\n","{'loss': 0.0001, 'learning_rate': 2.1223021582733816e-06, 'epoch': 1.31}\n","{'loss': 0.0001, 'learning_rate': 2.099820143884892e-06, 'epoch': 1.31}\n","{'loss': 0.0001, 'learning_rate': 2.0773381294964034e-06, 'epoch': 1.32}\n","{'loss': 0.0007, 'learning_rate': 2.054856115107914e-06, 'epoch': 1.33}\n","{'loss': 0.0002, 'learning_rate': 2.0323741007194248e-06, 'epoch': 1.34}\n","{'loss': 0.0003, 'learning_rate': 2.0098920863309352e-06, 'epoch': 1.34}\n","{'loss': 0.0004, 'learning_rate': 1.987410071942446e-06, 'epoch': 1.35}\n","{'loss': 0.0001, 'learning_rate': 1.964928057553957e-06, 'epoch': 1.36}\n","{'loss': 0.0003, 'learning_rate': 1.942446043165468e-06, 'epoch': 1.37}\n","{'loss': 0.0206, 'learning_rate': 1.9199640287769784e-06, 'epoch': 1.37}\n","{'loss': 0.0988, 'learning_rate': 1.8974820143884896e-06, 'epoch': 1.38}\n","{'loss': 0.0017, 'learning_rate': 1.8750000000000003e-06, 'epoch': 1.39}\n","{'loss': 0.0018, 'learning_rate': 1.8525179856115107e-06, 'epoch': 1.4}\n","{'loss': 0.0001, 'learning_rate': 1.8300359712230216e-06, 'epoch': 1.4}\n","{'loss': 0.0006, 'learning_rate': 1.8075539568345323e-06, 'epoch': 1.41}\n","{'loss': 0.0229, 'learning_rate': 1.7850719424460432e-06, 'epoch': 1.42}\n","{'loss': 0.0018, 'learning_rate': 1.762589928057554e-06, 'epoch': 1.42}\n","{'loss': 0.0001, 'learning_rate': 1.7401079136690648e-06, 'epoch': 1.43}\n","{'loss': 0.0009, 'learning_rate': 1.7176258992805755e-06, 'epoch': 1.44}\n","{'loss': 0.0031, 'learning_rate': 1.6951438848920864e-06, 'epoch': 1.45}\n","{'loss': 0.0008, 'learning_rate': 1.6726618705035971e-06, 'epoch': 1.45}\n","{'loss': 0.0003, 'learning_rate': 1.650179856115108e-06, 'epoch': 1.46}\n","{'loss': 0.0014, 'learning_rate': 1.6276978417266187e-06, 'epoch': 1.47}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ce4e601e2b3a46069ad3bbab32562be5","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1362 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.04099245369434357, 'eval_recall': 0.78125, 'eval_precision': 0.8245382585751979, 'eval_f1': 0.7828307158685808, 'eval_runtime': 42.3135, 'eval_samples_per_second': 32.188, 'eval_steps_per_second': 32.188, 'epoch': 1.47}\n","{'loss': 0.0846, 'learning_rate': 1.6052158273381296e-06, 'epoch': 1.48}\n","{'loss': 0.0001, 'learning_rate': 1.5827338129496403e-06, 'epoch': 1.48}\n","{'loss': 0.0, 'learning_rate': 1.5602517985611513e-06, 'epoch': 1.49}\n","{'loss': 0.0005, 'learning_rate': 1.537769784172662e-06, 'epoch': 1.5}\n","{'loss': 0.0003, 'learning_rate': 1.5152877697841729e-06, 'epoch': 1.51}\n","{'loss': 0.0004, 'learning_rate': 1.4928057553956835e-06, 'epoch': 1.51}\n","{'loss': 0.0001, 'learning_rate': 1.4703237410071945e-06, 'epoch': 1.52}\n","{'loss': 0.0002, 'learning_rate': 1.447841726618705e-06, 'epoch': 1.53}\n","{'loss': 0.0, 'learning_rate': 1.425359712230216e-06, 'epoch': 1.53}\n","{'loss': 0.1376, 'learning_rate': 1.4028776978417265e-06, 'epoch': 1.54}\n","{'loss': 0.001, 'learning_rate': 1.3803956834532374e-06, 'epoch': 1.55}\n","{'loss': 0.0002, 'learning_rate': 1.3579136690647481e-06, 'epoch': 1.56}\n","{'loss': 0.0005, 'learning_rate': 1.335431654676259e-06, 'epoch': 1.56}\n","{'loss': 0.0, 'learning_rate': 1.3129496402877697e-06, 'epoch': 1.57}\n","{'loss': 0.0, 'learning_rate': 1.2904676258992806e-06, 'epoch': 1.58}\n","{'loss': 0.0001, 'learning_rate': 1.2679856115107913e-06, 'epoch': 1.59}\n","{'loss': 0.0006, 'learning_rate': 1.2455035971223022e-06, 'epoch': 1.59}\n","{'loss': 0.0007, 'learning_rate': 1.2230215827338131e-06, 'epoch': 1.6}\n","{'loss': 0.0003, 'learning_rate': 1.2005395683453238e-06, 'epoch': 1.61}\n","{'loss': 0.117, 'learning_rate': 1.1780575539568347e-06, 'epoch': 1.62}\n","{'loss': 0.0002, 'learning_rate': 1.1555755395683454e-06, 'epoch': 1.62}\n","{'loss': 0.0001, 'learning_rate': 1.1330935251798561e-06, 'epoch': 1.63}\n","{'loss': 0.0147, 'learning_rate': 1.110611510791367e-06, 'epoch': 1.64}\n","{'loss': 0.0003, 'learning_rate': 1.0881294964028777e-06, 'epoch': 1.64}\n","{'loss': 0.0001, 'learning_rate': 1.0656474820143886e-06, 'epoch': 1.65}\n","{'loss': 0.0009, 'learning_rate': 1.0431654676258993e-06, 'epoch': 1.66}\n","{'loss': 0.0218, 'learning_rate': 1.0206834532374102e-06, 'epoch': 1.67}\n","{'loss': 0.0, 'learning_rate': 9.98201438848921e-07, 'epoch': 1.67}\n","{'loss': 0.0002, 'learning_rate': 9.757194244604318e-07, 'epoch': 1.68}\n","{'loss': 0.0006, 'learning_rate': 9.532374100719425e-07, 'epoch': 1.69}\n","{'loss': 0.0006, 'learning_rate': 9.307553956834533e-07, 'epoch': 1.7}\n","{'loss': 0.0006, 'learning_rate': 9.082733812949641e-07, 'epoch': 1.7}\n","{'loss': 0.0018, 'learning_rate': 8.857913669064749e-07, 'epoch': 1.71}\n","{'loss': 0.0011, 'learning_rate': 8.633093525179857e-07, 'epoch': 1.72}\n","{'loss': 0.0001, 'learning_rate': 8.408273381294965e-07, 'epoch': 1.73}\n","{'loss': 0.0012, 'learning_rate': 8.183453237410073e-07, 'epoch': 1.73}\n","{'loss': 0.0, 'learning_rate': 7.958633093525181e-07, 'epoch': 1.74}\n","{'loss': 0.0006, 'learning_rate': 7.733812949640289e-07, 'epoch': 1.75}\n","{'loss': 0.0, 'learning_rate': 7.508992805755396e-07, 'epoch': 1.75}\n","{'loss': 0.0001, 'learning_rate': 7.284172661870504e-07, 'epoch': 1.76}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d969efe05eb246a9accc2a03775cb9a8","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1362 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.031049499288201332, 'eval_recall': 0.9175, 'eval_precision': 0.7883995703544576, 'eval_f1': 0.9117576799961778, 'eval_runtime': 42.323, 'eval_samples_per_second': 32.181, 'eval_steps_per_second': 32.181, 'epoch': 1.76}\n","{'loss': 0.0008, 'learning_rate': 7.059352517985612e-07, 'epoch': 1.77}\n","{'loss': 0.0009, 'learning_rate': 6.83453237410072e-07, 'epoch': 1.78}\n","{'loss': 0.0001, 'learning_rate': 6.609712230215828e-07, 'epoch': 1.78}\n","{'loss': 0.0006, 'learning_rate': 6.384892086330936e-07, 'epoch': 1.79}\n","{'loss': 0.0043, 'learning_rate': 6.160071942446043e-07, 'epoch': 1.8}\n","{'loss': 0.0002, 'learning_rate': 5.935251798561151e-07, 'epoch': 1.81}\n","{'loss': 0.0, 'learning_rate': 5.710431654676259e-07, 'epoch': 1.81}\n","{'loss': 0.0, 'learning_rate': 5.485611510791367e-07, 'epoch': 1.82}\n","{'loss': 0.0006, 'learning_rate': 5.260791366906475e-07, 'epoch': 1.83}\n","{'loss': 0.0006, 'learning_rate': 5.035971223021583e-07, 'epoch': 1.84}\n","{'loss': 0.0023, 'learning_rate': 4.811151079136691e-07, 'epoch': 1.84}\n","{'loss': 0.031, 'learning_rate': 4.586330935251799e-07, 'epoch': 1.85}\n","{'loss': 0.0014, 'learning_rate': 4.361510791366907e-07, 'epoch': 1.86}\n","{'loss': 0.0006, 'learning_rate': 4.136690647482015e-07, 'epoch': 1.86}\n","{'loss': 0.0008, 'learning_rate': 3.911870503597123e-07, 'epoch': 1.87}\n","{'loss': 0.026, 'learning_rate': 3.68705035971223e-07, 'epoch': 1.88}\n","{'loss': 0.0009, 'learning_rate': 3.462230215827338e-07, 'epoch': 1.89}\n","{'loss': 0.0001, 'learning_rate': 3.237410071942446e-07, 'epoch': 1.89}\n","{'loss': 0.0063, 'learning_rate': 3.012589928057554e-07, 'epoch': 1.9}\n","{'loss': 0.0007, 'learning_rate': 2.787769784172662e-07, 'epoch': 1.91}\n","{'loss': 0.0005, 'learning_rate': 2.56294964028777e-07, 'epoch': 1.92}\n","{'loss': 0.0002, 'learning_rate': 2.338129496402878e-07, 'epoch': 1.92}\n","{'loss': 0.0347, 'learning_rate': 2.1133093525179857e-07, 'epoch': 1.93}\n","{'loss': 0.0011, 'learning_rate': 1.8884892086330937e-07, 'epoch': 1.94}\n","{'loss': 0.0001, 'learning_rate': 1.6636690647482017e-07, 'epoch': 1.95}\n","{'loss': 0.0826, 'learning_rate': 1.4388489208633095e-07, 'epoch': 1.95}\n","{'loss': 0.0115, 'learning_rate': 1.2140287769784175e-07, 'epoch': 1.96}\n","{'loss': 0.0013, 'learning_rate': 9.892086330935252e-08, 'epoch': 1.97}\n","{'loss': 0.0006, 'learning_rate': 7.643884892086332e-08, 'epoch': 1.98}\n","{'loss': 0.0012, 'learning_rate': 5.395683453237411e-08, 'epoch': 1.98}\n","{'loss': 0.0009, 'learning_rate': 3.1474820143884896e-08, 'epoch': 1.99}\n","{'loss': 0.0023, 'learning_rate': 8.992805755395684e-09, 'epoch': 2.0}\n","{'train_runtime': 1134.7288, 'train_samples_per_second': 9.597, 'train_steps_per_second': 2.401, 'train_loss': 0.011447668101200993, 'epoch': 2.0}\n","Shutting down background jobs, please wait a moment...\n","Done!\n","Waiting for the remaining 9 operations to synchronize with Neptune. Do not kill this process.\n","All 9 operations synced, thanks for waiting!\n","Explore the metadata in the Neptune app:\n","https://app.neptune.ai/bernd.heidemann/PII/e/PII-221/metadata\n"]},{"data":{"text/plain":["TrainOutput(global_step=2724, training_loss=0.011447668101200993, metrics={'train_runtime': 1134.7288, 'train_samples_per_second': 9.597, 'train_steps_per_second': 2.401, 'train_loss': 0.011447668101200993, 'epoch': 2.0})"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["unfreeze(model)\n","trainer=get_trainer(model, train_ds, valid_ds, learnrate_multiplier=parameter[\"lr_scale_unfreeze\"])\n","trainer.train()"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","c:\\Users\\Bernd\\anaconda3\\envs\\mytorch\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:470: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n","  warnings.warn(\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"name":"stdout","output_type":"stream","text":["    row_id  document  token           label\n","0        0         7      9  B-NAME_STUDENT\n","1        1         7     10  I-NAME_STUDENT\n","2        2         7    482  B-NAME_STUDENT\n","3        3         7    483  I-NAME_STUDENT\n","4        4         7    741  B-NAME_STUDENT\n","5        5         7    742  I-NAME_STUDENT\n","6        6        10      0  B-NAME_STUDENT\n","7        7        10      1  I-NAME_STUDENT\n","8        8        10    464  B-NAME_STUDENT\n","9        9        10    465  I-NAME_STUDENT\n","10      10        16      4  B-NAME_STUDENT\n","11      11        16      5  I-NAME_STUDENT\n","12      12        20      5  B-NAME_STUDENT\n","13      13        20      6  I-NAME_STUDENT\n","14      14        56     12  B-NAME_STUDENT\n","15      15        56     13  I-NAME_STUDENT\n","16      16        86      6  B-NAME_STUDENT\n","17      17        86      7  I-NAME_STUDENT\n","18      18        93      0  B-NAME_STUDENT\n","19      19        93      1  I-NAME_STUDENT\n","20      20       104      7  B-NAME_STUDENT\n","21      21       104      8  B-NAME_STUDENT\n","22      22       104      9  I-NAME_STUDENT\n","23      23       112      5  B-NAME_STUDENT\n","24      24       112      6  I-NAME_STUDENT\n","25      25       123     32  B-NAME_STUDENT\n","26      26       123     33  I-NAME_STUDENT\n"]}],"source":["create_submission(model, f\"submission.csv\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":7500999,"sourceId":66653,"sourceType":"competition"},{"datasetId":4319117,"sourceId":7429898,"sourceType":"datasetVersion"}],"dockerImageVersionId":30636,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
