{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from transformers import AutoTokenizer\n","from pathlib import Path\n","import numpy as np\n","import torch\n","from transformers import AutoModelForTokenClassification\n","from tokenizers import AddedToken\n","from tqdm.notebook import tqdm\n","from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n","import pandas as pd\n","from seqeval.metrics import recall_score, precision_score, f1_score, accuracy_score\n","\n","kaggle=False\n","\n","path=\"/kaggle/input/pii-detection-removal-from-educational-data\" if kaggle else \"data\"\n","train_path = path + \"/train.json\"\n","test_path = path + \"/test.json\"\n","\n","model_path = \"/kaggle/input/huggingfacedebertav3variants/deberta-v3-base\" if kaggle else \"microsoft/deberta-v3-base\"\n","\n","\n","if not kaggle: import neptune\n"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-01-25T21:37:35.578114Z","iopub.status.busy":"2024-01-25T21:37:35.577723Z","iopub.status.idle":"2024-01-25T21:37:35.587366Z","shell.execute_reply":"2024-01-25T21:37:35.5858Z","shell.execute_reply.started":"2024-01-25T21:37:35.578083Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["{'model': 'microsoft/deberta-v3-base', 'max_length': 1024, 'inference_max_length': 2000, 'batch_size': 9, 'inference_batch_size': 3, 'lr': 0.001, 'lr_scale_unfreeze': 0.1, 'filter_no_pii_percent_allow': 0.2, 'notebook': '20_deberta base_1024len.ipynb', 'CROSS_ENTROPY_WEIGHT_MULTI': 400, 'epochs_before_unfreeze': 4, 'epochs_after_unfreeze': 6, 'repeat_unfreeze_train_n_times': 2, 'validate_every_n_epochs': 2, 'train_test_split': 0.2, 'num_proc': 16}\n"]}],"source":["cross_entropy_weight_multi = 400\n","\n","CROSS_ENTROPY_WEIGHTS = [cross_entropy_weight_multi]*12\n","CROSS_ENTROPY_WEIGHTS.append(1)\n","\n","parameter= {\n","    \"model\": model_path,\n","    \"max_length\": 1024,\n","    \"inference_max_length\": 2000,\n","    \"batch_size\": 9,\n","    \"inference_batch_size\": 3,\n","    \"lr\": 1e-3,\n","    \"lr_scale_unfreeze\": 0.1,\n","    \"filter_no_pii_percent_allow\": 0.2,\n","    \"notebook\": \"20_deberta base_1024len.ipynb\",\n","    \"CROSS_ENTROPY_WEIGHT_MULTI\": cross_entropy_weight_multi,\n","    \"epochs_before_unfreeze\": 4,\n","    \"epochs_after_unfreeze\": 6,\n","    \"repeat_unfreeze_train_n_times\": 2,\n","    \"validate_every_n_epochs\": 2,\n","    \"train_test_split\": 0.2,\n","    \"num_proc\": 16\n","}\n","\n","print(parameter)"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["target = [\n","    'B-EMAIL', 'B-ID_NUM', 'B-NAME_STUDENT', 'B-PHONE_NUM', \n","    'B-STREET_ADDRESS', 'B-URL_PERSONAL', 'B-USERNAME', 'I-ID_NUM', \n","    'I-NAME_STUDENT', 'I-PHONE_NUM', 'I-STREET_ADDRESS', 'I-URL_PERSONAL'\n","]"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-01-25T21:37:35.606208Z","iopub.status.busy":"2024-01-25T21:37:35.605889Z","iopub.status.idle":"2024-01-25T21:37:35.62164Z","shell.execute_reply":"2024-01-25T21:37:35.620746Z","shell.execute_reply.started":"2024-01-25T21:37:35.606175Z"},"trusted":true},"outputs":[],"source":["from itertools import chain\n","import json\n","\n","data = json.load(open(train_path))\n","all_labels = sorted(list(set(chain(*[x[\"labels\"] for x in data]))))\n","label2id = {l: i for i,l in enumerate(all_labels)}\n","id2label = {v:k for k,v in label2id.items()}"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["import random\n","\n","def tokenize(example, tokenizer, label2id, max_length, all_labels_list):\n","    text = []\n","    import numpy as np\n","\n","    # these are at the character level\n","    labels = []\n","    targets = []\n","\n","    for t, l, ws in zip(example[\"tokens\"], example[\"labels\"], example[\"trailing_whitespace\"]):\n","\n","        text.append(t)\n","        labels.extend([l]*len(t))\n","        \n","        if l in all_labels_list:\n","            targets.append(1)\n","        else:\n","            targets.append(0)\n","        # if there is trailing whitespace\n","        if ws:\n","            text.append(\" \")\n","            labels.append(\"O\")\n","\n","    tokenized = tokenizer(\"\".join(text), return_offsets_mapping=True, truncation=True, max_length=max_length, padding=\"max_length\")\n","    \n","    target_num = sum(targets)\n","    labels = np.array(labels)\n","\n","    text = \"\".join(text)\n","    token_labels = []\n","\n","    for start_idx, end_idx in tokenized.offset_mapping:\n","\n","        # CLS token\n","        if start_idx == 0 and end_idx == 0: \n","            token_labels.append(label2id[\"O\"])\n","            continue\n","\n","        # case when token starts with whitespace\n","        if text[start_idx].isspace():\n","            start_idx += 1\n","\n","        try:\n","            token_labels.append(label2id[labels[start_idx]])\n","        except:\n","            token_labels.append(label2id[\"O\"])\n","\n","    length = len(tokenized.input_ids)\n","\n","    return {\n","        **tokenized,\n","        \"labels\": token_labels,\n","        \"length\": length,\n","        \"target_num\": target_num,\n","        \"group\": 1 if target_num>0 else 0\n","    }\n","\n","# https://www.kaggle.com/competitions/pii-detection-removal-from-educational-data/discussion/468844\n","def filter_no_pii(example, percent_allow=parameter[\"filter_no_pii_percent_allow\"]):\n","    # Return True if there is PII\n","    # Or 20% of the time if there isn't\n","    has_pii = set(\"O\") != set(example[\"labels\"])\n","    return has_pii or (random.random() < percent_allow)"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["dict_keys(['document', 'full_text', 'tokens', 'trailing_whitespace', 'labels'])\n"]},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","c:\\Users\\Bernd\\anaconda3\\envs\\mytorch\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:470: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n","  warnings.warn(\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2f3ac77dafc04fcc8024a63a500a6479","version_major":2,"version_minor":0},"text/plain":["Map (num_proc=16):   0%|          | 0/6807 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from datasets import Dataset\n","class PiiDataset(torch.utils.data.Dataset):\n","    def __init__(self, ds):\n","        self.dataset = ds\n","        \n","    def __getitem__(self, idx):\n","        vals=self.dataset[idx]\n","        input_ids = torch.tensor(vals[\"input_ids\"])\n","        attention_mask = torch.tensor(vals[\"attention_mask\"])\n","        labels = torch.tensor(vals[\"labels\"], dtype=torch.long)\n","        return input_ids, attention_mask, labels\n","    \n","    def __len__(self):\n","        return len(self.dataset)\n","\n","data = json.load(open(train_path))\n","print(data[0].keys())\n","ds = Dataset.from_dict({\n","    \"full_text\": [x[\"full_text\"] for x in data],\n","    \"document\": [str(x[\"document\"]) for x in data],\n","    \"tokens\": [x[\"tokens\"] for x in data],\n","    \"trailing_whitespace\": [x[\"trailing_whitespace\"] for x in data],\n","    \"labels\": [x[\"labels\"] for x in data],\n","})\n","    \n","tokenizer = AutoTokenizer.from_pretrained(parameter[\"model\"])\n","ds = ds.map(tokenize, fn_kwargs={\"tokenizer\": tokenizer, \"label2id\": label2id, \"max_length\": parameter[\"max_length\"], \"all_labels_list\": target}, num_proc=parameter[\"num_proc\"])\n","\n","my_dataset=PiiDataset(ds)\n","\n"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([8, 1024])\n","torch.Size([8, 1024])\n","torch.Size([8, 1024])\n"]}],"source":["loader=torch.utils.data.DataLoader(my_dataset, batch_size=8, shuffle=True)\n","\n","for id, attention_mask, labels in loader:\n","    print(id.shape)\n","    print(attention_mask.shape)\n","    print(labels.shape)\n","    break"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["still learning classifier.weight parameter_size: torch.Size([13, 768])\n","still learning classifier.bias parameter_size: torch.Size([13])\n","torch.Size([8, 1024])\n","torch.Size([8, 1024])\n","torch.Size([8, 1024])\n","tensor([[12, 12, 12,  ..., 12, 12, 12],\n","        [12, 12, 12,  ..., 12, 12, 12],\n","        [12, 12, 12,  ..., 12, 12, 12],\n","        ...,\n","        [12, 12, 12,  ..., 12, 12, 12],\n","        [12, 12, 12,  ..., 12, 12, 12],\n","        [12, 12, 12,  ..., 12, 12, 12]])\n","torch.Size([8, 1024, 13])\n"]}],"source":["device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","class MyModel(torch.nn.Module):\n","    def __init__(self, model_name, num_labels, dropout_p=0.4):\n","        super().__init__()\n","        self.model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=num_labels)\n","        self.softmax=torch.nn.Softmax(dim=-1)\n","        self.freeze()\n","\n","    def freeze(self):\n","        for param in self.model.parameters():\n","            param.requires_grad = False\n","        for param in self.model.classifier.parameters():\n","            param.requires_grad = True\n","        for name, param in self.model.named_parameters():\n","            if param.requires_grad==True:\n","                print(\"still learning\", name, \"parameter_size:\", param.size())\n","\n","    def unfreeze(self):\n","        for param in self.model.parameters():\n","            param.requires_grad = True\n","        \n","    def forward(self, input_ids, attention_mask, labels=None):\n","        if labels is not None:\n","            out=self.model(input_ids, attention_mask=attention_mask, labels=labels)['logits']\n","        else:\n","            out=self.model(input_ids, attention_mask=attention_mask)['logits']\n","        out=self.softmax(out)\n","        return out\n","\n","model = MyModel(parameter['model'], len(label2id))\n","\n","model= model.to(device)\n","for id, attention_mask, labels in loader:\n","    print(id.shape)\n","    print(attention_mask.shape)\n","    print(labels.shape)\n","    print(labels)\n","    id = id.to(device)\n","    attention_mask = attention_mask.to(device)\n","    labels = labels.to(device)\n","    print(model(id, attention_mask, labels).shape)\n","    break\n","\n","#free gpu memory\n","del model\n","torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["from torch.cuda.amp import GradScaler, autocast\n","class Learner():\n","    def __init__(self, model, train_dataloader, valid_dataloader, parameter=None):\n","        self.model=model\n","        #self.loss_fn=torch.nn.CrossEntropyLoss()\n","        self.loss_fn=torch.nn.CrossEntropyLoss(ignore_index=-100, weight=torch.tensor(CROSS_ENTROPY_WEIGHTS, dtype=torch.float32).to(device))\n","        self.device=torch.device(\"cpu\")\n","        if torch.cuda.is_available():\n","            self.device=torch.device(\"cuda\")\n","        self.model.to(self.device)\n","        self.parameter = parameter\n","\n","        if not kaggle:\n","            self.run = neptune.init_run(\n","                project=\"bernd.heidemann/PII\",\n","                api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIzNjBlYzVkNi0zZTUwLTQ1ODYtODhlNC02NDUxNDg0MDdjNzUifQ==\",\n","            )  # your credentials\n","            self.run[\"parameters\"] = {\n","                **self.parameter\n","            }\n","\n","        self.train_dataloader = train_dataloader\n","        self.valid_dataloader = valid_dataloader\n","        self.non_pii_label=label2id[\"O\"]\n","        self.all_labels = target + [\"O\"]\n","\n","    def fit(self, lr=0.1, epochs=10):\n","        scaler = GradScaler()\n","        optimizer=torch.optim.AdamW(self.model.parameters(), lr=lr)\n","        T_0 = epochs//3          # Number of epochs before the first restart\n","        T_mult = 2        # Factor by which T_0 is multiplied after each restart\n","        eta_min = lr*0.01   # Minimum learning rate at restarts\n","\n","        scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=T_0, T_mult=T_mult, eta_min=eta_min)\n","        bar = tqdm(total=len(self.train_dataloader) * epochs, desc=\"Training\")\n","        bar.set_description(\"Epoch 0/{}\".format(epochs))\n","        for epoch in range(epochs):\n","            self.model.train()     \n","            for ids, att_mask, labels in self.train_dataloader:\n","                optimizer.zero_grad()\n","                with autocast(dtype=torch.float16):\n","                    ids=ids.to(self.device)\n","                    labels=labels.to(self.device)\n","                    att_mask=att_mask.to(self.device)\n","                    pred=self.model(ids, att_mask, labels)\n","                    pred = pred.permute(0, 2, 1)\n","                    loss=self.loss_fn(pred, labels)\n","                    if not kaggle:\n","                        self.run[\"train_loss\"].log(loss.item())\n","                    bar.update(1)\n","                scaler.scale(loss).backward()\n","                scaler.step(optimizer)\n","                scaler.update()\n","               \n","            if not kaggle:\n","                self.run[\"learnrate\"].log(optimizer.param_groups[0][\"lr\"])\n","            scheduler.step()\n","            self.model.eval()\n","            # log current state to neptune\n","            if self.valid_dataloader is not None and len(self.valid_dataloader)>0 and (epoch+1) % self.parameter[\"validate_every_n_epochs\"] == 0:\n","                metrics=self.get_accuracy()\n","                if not kaggle:\n","                    self.run[\"valid_accuracy\"].log(metrics[\"accuracy\"])\n","                    self.run[\"valid_loss\"].log(metrics[\"loss\"])\n","                    self.run[\"valid_f1\"].log(metrics[\"f1\"])\n","                    self.run[\"valid_f5\"].log(metrics[\"f5\"])\n","                    self.run[\"valid_precision\"].log(metrics[\"precision\"])\n","                    self.run[\"valid_recall\"].log(metrics[\"recall\"])\n","                bar.set_description(\"Epoch {}/{} validAccuracy: {:.2f} validLoss: {:.2f} valid_f5: {:.2f}\".format(epoch+1, epochs, metrics[\"accuracy\"], metrics[\"loss\"], metrics[\"f5\"]))\n","\n","    def get_accuracy(self):\n","        self.model.eval()\n","        with torch.no_grad():\n","            losses=[]\n","            batch_metrics=[]\n","            for ids, att_mask, labels in self.valid_dataloader:\n","                ids=ids.to(self.device)\n","                labels=labels.to(self.device)\n","                att_mask=att_mask.to(self.device)\n","                pred=self.model(ids, att_mask, labels)\n","                f_beta_score_results = self.compute_metrics(labels, pred)\n","                batch_metrics.append(f_beta_score_results)\n","                pred = pred.permute(0, 2, 1)\n","                loss=self.loss_fn(pred, labels)\n","                losses.append(loss.item())\n","            # calc mean of the dict entries in batch_metrics\n","            f1_scores = np.mean([x[\"f1\"] for x in batch_metrics])\n","            recall_scores = np.mean([x[\"recall\"] for x in batch_metrics])\n","            precision_scores = np.mean([x[\"precision\"] for x in batch_metrics])\n","            accuracy_scores = np.mean([x[\"accuracy\"] for x in batch_metrics])\n","            f5 = np.mean([x[\"f5\"] for x in batch_metrics])\n","            return {\n","                \"accuracy\": accuracy_scores,\n","                \"loss\": np.mean(losses),\n","                \"f1\": f1_scores,\n","                \"recall\": recall_scores,\n","                \"precision\": precision_scores,\n","                \"f5\": f5\n","            }\n","        \n","    def compute_metrics(self, labels, predictions):\n","        predictions = torch.argmax(predictions, axis=2)\n","        true_predictions = []\n","        true_labels = []\n","        for pred, label in zip(predictions, labels):\n","            true_pred = []\n","            true_label = []\n","            for p, l in zip(pred, label):\n","                if l != -100:  ## skip zero\n","                    true_pred.append(self.all_labels[p]) # Add label\n","                    true_label.append(self.all_labels[l]) # Add label\n","            true_predictions.append(true_pred)\n","            true_labels.append(true_label)\n","        # Compute recall, precision and f1 score\n","        recall = recall_score(true_labels, true_predictions, zero_division=1e-7)\n","        precision = precision_score(true_labels, true_predictions, zero_division=1e-7)\n","        accuracy = accuracy_score(true_labels, true_predictions)\n","        f1 = f1_score(true_labels, true_predictions)\n","        # calculate f1 score from scratch, beta is 5\n","        f5 = (1 + 5**2) * (precision * recall) / ((5**2 * precision) + recall)\n","\n","\n","        result = {'f1': f1,\n","                    'recall': recall,\n","                    'precision': precision,\n","                    'accuracy': accuracy,\n","                    'f5': f5}\n","        return result\n","        \n","\n","    "]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["still learning classifier.weight parameter_size: torch.Size([13, 768])\n","still learning classifier.bias parameter_size: torch.Size([13])\n"]},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"name":"stdout","output_type":"stream","text":["      row_id  document  token             label\n","0          0         7      0  I-STREET_ADDRESS\n","1          1         7      1  I-STREET_ADDRESS\n","2          2         7      2  I-STREET_ADDRESS\n","3          3         7      3  I-STREET_ADDRESS\n","4          4         7      4  B-STREET_ADDRESS\n","...      ...       ...    ...               ...\n","7171    7171       123   1691  I-STREET_ADDRESS\n","7172    7172       123   1691           B-EMAIL\n","7173    7173       123   1691    I-URL_PERSONAL\n","7174    7174       123   1691       B-PHONE_NUM\n","7175    7175       123   1692  B-STREET_ADDRESS\n","\n","[7176 rows x 4 columns]\n"]}],"source":["def tokenize_inference(example, tokenizer, max_length):\n","        text = []\n","        for t,  ws in zip(example[\"tokens\"], example[\"trailing_whitespace\"]):\n","            text.append(t)\n","            if ws:\n","                text.append(\" \")\n","        tokenized = tokenizer(\"\".join(text), return_offsets_mapping=True, truncation=True, max_length=max_length, padding=\"max_length\")\n","        text = \"\".join(text)\n","        length = len(tokenized.input_ids)\n","        return {\n","            **tokenized,\n","            \"length\": length,\n","        }\n","        \n","class TestTokenizer():\n","    def __init__(self, tokenizer):\n","        self.tokenizer = tokenizer\n","    \n","    def preprocess(self, example):\n","        # Preprocess the tokens and labels by adding trailing whitespace and labels\n","        tokens = []\n","        tokens_without_ws = []\n","        token_map = [] # Use the index as labels\n","        index = 0\n","        for token, t_ws in zip(example[\"tokens\"], example[\"trailing_whitespace\"]):\n","            tokens_without_ws.append(token)\n","            tokens.append(token)\n","            token_map.extend([index] * len(token))\n","            # Added trailing whitespace and label if true and \n","            if t_ws:\n","                tokens.append(\" \")\n","                token_map.append(-1)\n","            index += 1\n","        return tokens, token_map, tokens_without_ws\n","    \n","    def tokenize(self, example):\n","        tokens, token_map, tokens_without_ws = self.preprocess(example)\n","        text = \"\".join(tokens)\n","        tokenized = self.tokenizer(text, return_offsets_mapping=True, padding=\"max_length\",\n","                                   truncation=True, max_length=parameter[\"inference_max_length\"])\n","        return {**tokenized, \"token_map\": token_map, \"tokens\": tokens, \"tokens_without_ws\": tokens_without_ws} \n","\n","class PiiDatasetInference(torch.utils.data.Dataset):\n","        def __init__(self, dataset, tokenizer):\n","            self.dataset = dataset\n","            self.tokenizer=TestTokenizer(tokenizer)\n","            \n","        def __getitem__(self, idx):\n","            vals=self.tokenizer.tokenize(self.dataset[idx])\n","            input_ids = torch.tensor(vals[\"input_ids\"])\n","            attention_mask = torch.tensor(vals[\"attention_mask\"])\n","            document_id = self.dataset[idx][\"document\"]\n","            return input_ids, attention_mask, document_id, vals\n","        \n","        def __len__(self):\n","            return len(self.dataset)\n","\n","def inference(model):\n","    data = json.load(open(train_path))\n","    from itertools import chain\n","    all_labels = sorted(list(set(chain(*[x[\"labels\"] for x in data]))))\n","    label2id = {l: i for i,l in enumerate(all_labels)}\n","    id2label = {v:k for k,v in label2id.items()}\n","\n","    tokenizer = AutoTokenizer.from_pretrained(parameter[\"model\"])\n","    data = json.load(open(test_path))\n","    my_dataset=PiiDatasetInference(data, tokenizer)\n","    loader=torch.utils.data.DataLoader(my_dataset, batch_size=parameter['batch_size'])\n","    for id, attention_mask, document_id, vals  in loader:\n","        id = id.to(device)\n","        print(id.shape)\n","        attention_mask = attention_mask.to(device)\n","        preds=model(id, attention_mask).argmax(dim=2)\n","\n","        for pred, id in zip(preds.flatten(), id.flatten()):\n","            if pred != 12:\n","                print(f\"TOKEN:{tokenizer.decode(id)}  --- pred:{id2label[pred.item()]}\")\n","        print(\"next\")\n","\n","# Convert preds to a list of dictionaries\n","def to_test_submission(preds=None, dataset=None, document_ids=None, id2label=None):\n","    triplets = []\n","    row_id = 0\n","    results = []\n","    \n","    for i in range(len(preds)):\n","        input_ids, attention_mask, document_id, vals = dataset[i]\n","        token_map=vals[\"token_map\"]\n","        offsets=vals[\"offset_mapping\"]\n","        tokens=vals[\"tokens_without_ws\"]\n","        #print(\"tokens\", tokens)\n","        pred=preds[i]\n","        original_text = tokenizer.decode(input_ids)[6:] # skip CLS token\n","        #print(\"original_text\", original_text)\n","        #print(\"token_map\", token_map)\n","        #print(\"offsets\", offsets)   \n","        #print(\"pred\", pred)\n","\n","        for token_pred, input_id, (start_idx, end_idx) in zip(pred, input_ids, offsets):\n","            #print(\"\\nnow doing \", start_idx,  end_idx, token_pred)\n","            if start_idx == 0 and end_idx == 0: # Skip 0 offset\n","                continue\n","            # Skip spaces \n","            while start_idx < len(token_map):\n","                #print(\"loop, start_idx now\", start_idx) \n","                #print(\" tokens[token_map[start_idx]]: \", tokens[token_map[start_idx]] if not tokens[token_map[start_idx]].isspace() else \"WHITESPACE\")          \n","                if token_map[start_idx] == -1: # Skip unknown tokens               \n","                    start_idx += 1\n","                elif tokens[token_map[start_idx]].isspace(): # Skip white space\n","                    start_idx += 1\n","                else:\n","                    break\n","            # Ensure start index < length\n","            if start_idx < len(token_map):\n","                token_id = token_map[start_idx]\n","                #print(\"token_id\", token_id)\n","                #token_id= input_id.item()\n","                label_pred = id2label[token_pred.item()]\n","                #print(\"label_pred\", label_pred)\n","                # ignore \"O\" and whitespace preds\n","                if label_pred != \"O\" and token_id != -1:\n","                    #print(\"is PII\", token_id, label_pred)\n","                    token_str = tokens[token_id]\n","                    triplet = (label_pred, token_id, token_str)\n","                    if triplet not in triplets:\n","                        results.append({\n","                            \"row_id\": row_id, \n","                            \"document\": document_id,\n","                            \"token\": token_id, \n","                            \"label\": label_pred,\n","                            \"token_str\": token_str\n","                        })\n","                        triplets.append(triplet)\n","                        row_id += 1\n","\n","    # Create a dataframe \n","    return results\n","\n","def create_submission(model, filename=\"submission.csv\"):\n","    data = json.load(open(train_path))\n","    from itertools import chain\n","    all_labels = sorted(list(set(chain(*[x[\"labels\"] for x in data]))))\n","    label2id = {l: i for i,l in enumerate(all_labels)}\n","    id2label = {v:k for k,v in label2id.items()}\n","\n","    data=json.load(open(test_path))\n","    tokenizer = AutoTokenizer.from_pretrained(parameter[\"model\"])\n","    my_dataset=PiiDatasetInference(data, tokenizer)\n","    loader=torch.utils.data.DataLoader(my_dataset, batch_size=1, shuffle=False)\n","\n","    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model.eval()\n","    \n","\n","    # stack all predictions into tensor\n","    all_preds = []\n","\n","    for id, attention_mask, document_ids, vals in loader:\n","        id=id.to(device)\n","        attention_mask=attention_mask.to(device)\n","        preds=model(id, attention_mask).argmax(dim=2)\n","        all_preds.append(preds)\n","        #for pred, id in zip(preds.flatten(), id.flatten()):\n","        #    if pred != 12:\n","                #print(f\"Document: {document_id.item()} TOKEN:{tokenizer.decode(id)}  --- pred:{id2label[pred.item()]}\")\n","        #        output[row_id]={\"document\":document_id.item(), \"token\":id.item(), \"label\":id2label[pred.item()]}\n","        #        row_id+=1\n","        #for pred, id in zip(preds.flatten(), id.flatten()):\n","        #    if pred != 12:\n","        #        print(f\"TOKEN:{tokenizer.decode(id)}  --- pred:{id2label[pred.item()]}\")\n","    \n","   \n","    all_preds = torch.cat(all_preds, dim=0)\n","    \n","    results = to_test_submission(preds=all_preds, dataset=my_dataset, document_ids=document_ids, id2label=id2label)\n","    if len(results) == 0:\n","        print(\"Error in create_submission(): No predictions made, probably because the model is not learning. Check the model and the data.\")\n","        return\n","    df = pd.DataFrame(results)\n","    df=df[[\"row_id\", \"document\", \"token\", \"label\"]]\n","    print(df)\n","    df.to_csv(filename, index=False)\n","\n","create_submission(MyModel(parameter['model'], len(label2id)).to(device), \"submission_just_dumb.csv\")\n","#create_submission(learner.model, \"submission.csv\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","c:\\Users\\Bernd\\anaconda3\\envs\\mytorch\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:470: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n","  warnings.warn(\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"202f6ee8327049e4a859126f1610131d","version_major":2,"version_minor":0},"text/plain":["Map (num_proc=16):   0%|          | 0/6807 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"08a1e94ae7b740c69d7cd012eaff5f5f","version_major":2,"version_minor":0},"text/plain":["Filter (num_proc=16):   0%|          | 0/6807 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["train_len 5445\n","valid_len 1362\n"]}],"source":["data = json.load(open(train_path))\n","ds = Dataset.from_dict({\n","    \"full_text\": [x[\"full_text\"] for x in data],\n","    \"document\": [str(x[\"document\"]) for x in data],\n","    \"tokens\": [x[\"tokens\"] for x in data],\n","    \"trailing_whitespace\": [x[\"trailing_whitespace\"] for x in data],\n","    \"labels\": [x[\"labels\"] for x in data],\n","})\n","    \n","tokenizer = AutoTokenizer.from_pretrained(parameter[\"model\"])\n","ds = ds.map(tokenize, fn_kwargs={\"tokenizer\": tokenizer, \"label2id\": label2id, \"max_length\": parameter[\"max_length\"], \"all_labels_list\": target}, num_proc=parameter[\"num_proc\"])\n","ds=ds.filter(filter_no_pii, num_proc=parameter[\"num_proc\"])\n","\n","\n","data_len=len(ds)\n","train_len=int(len(ds)*(1-parameter[\"train_test_split\"]))\n","valid_len=len(ds)-train_len\n","train_data_idx=np.random.choice(data_len, train_len, replace=False)\n","valid_data_idx=np.array(list(set(range(data_len))-set(train_data_idx)))\n","print(\"train_len\", train_len)\n","print(\"valid_len\", valid_len)\n","\n","# split ds in train and valid\n","train_ds=ds.select(train_data_idx)\n","valid_ds=ds.select(valid_data_idx)\n","\n"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","c:\\Users\\Bernd\\anaconda3\\envs\\mytorch\\lib\\site-packages\\neptune\\common\\warnings.py:71: NeptuneWarning: The following monitoring options are disabled by default in interactive sessions: 'capture_stdout', 'capture_stderr', 'capture_traceback', and 'capture_hardware_metrics'. To enable them, set each parameter to 'True' when initializing the run. The monitoring will continue until you call run.stop() or the kernel stops. Also note: Your source files can only be tracked if you pass the path(s) to the 'source_code' argument. For help, see the Neptune docs: https://docs.neptune.ai/logging/source_code/\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["still learning classifier.weight parameter_size: torch.Size([13, 768])\n","still learning classifier.bias parameter_size: torch.Size([13])\n","https://app.neptune.ai/bernd.heidemann/PII/e/PII-185\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\Bernd\\AppData\\Local\\Temp\\ipykernel_149980\\157558940.py:120: RuntimeWarning: invalid value encountered in scalar divide\n","  f5 = (1 + 5**2) * (precision * recall) / ((5**2 * precision) + recall)\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[12], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m my_model\u001b[38;5;241m=\u001b[39mMyModel(parameter[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;28mlen\u001b[39m(label2id))\n\u001b[0;32m     14\u001b[0m learner\u001b[38;5;241m=\u001b[39mLearner(my_model, train_dataloader, valid_dataloader, parameter\u001b[38;5;241m=\u001b[39mparameter)\n\u001b[1;32m---> 15\u001b[0m \u001b[43mlearner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_accuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m parameter[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs_before_unfreeze\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     17\u001b[0m     learner\u001b[38;5;241m.\u001b[39mfit(lr\u001b[38;5;241m=\u001b[39mparameter[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m], epochs\u001b[38;5;241m=\u001b[39mparameter[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs_before_unfreeze\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n","Cell \u001b[1;32mIn[9], line 81\u001b[0m, in \u001b[0;36mLearner.get_accuracy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m att_mask\u001b[38;5;241m=\u001b[39matt_mask\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     80\u001b[0m pred\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(ids, att_mask, labels)\n\u001b[1;32m---> 81\u001b[0m f_beta_score_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m batch_metrics\u001b[38;5;241m.\u001b[39mappend(f_beta_score_results)\n\u001b[0;32m     83\u001b[0m pred \u001b[38;5;241m=\u001b[39m pred\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n","Cell \u001b[1;32mIn[9], line 110\u001b[0m, in \u001b[0;36mLearner.compute_metrics\u001b[1;34m(self, labels, predictions)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p, l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(pred, label):\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m l \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m:  \u001b[38;5;66;03m## skip zero\u001b[39;00m\n\u001b[1;32m--> 110\u001b[0m         \u001b[43mtrue_pred\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_labels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mp\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Add label\u001b[39;00m\n\u001b[0;32m    111\u001b[0m         true_label\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_labels[l]) \u001b[38;5;66;03m# Add label\u001b[39;00m\n\u001b[0;32m    112\u001b[0m true_predictions\u001b[38;5;241m.\u001b[39mappend(true_pred)\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# set environment variables: TOKENIZERS_PARALLELISM=false\n","import os\n","os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n","\n","tokenizer = AutoTokenizer.from_pretrained(parameter[\"model\"])\n","tokenizer.add_tokens(AddedToken(\"\\n\", normalized=False))\n","\n","train_dataset = PiiDataset(train_ds)\n","valid_dataset = PiiDataset(valid_ds)\n","train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=parameter['batch_size'], shuffle=True)\n","valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=parameter['inference_batch_size'], shuffle=False)\n","my_model=MyModel(parameter['model'], len(label2id))\n","\n","learner=Learner(my_model, train_dataloader, valid_dataloader, parameter=parameter)\n","learner.get_accuracy()\n","if parameter[\"epochs_before_unfreeze\"] > 0:\n","    learner.fit(lr=parameter['lr'], epochs=parameter[\"epochs_before_unfreeze\"])\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["learner.model.unfreeze()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c005f3721f9b4cc5affed64d55712ecc","version_major":2,"version_minor":0},"text/plain":["Training:   0%|          | 0/1164 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["for i in range(parameter[\"repeat_unfreeze_train_n_times\"]):\n","    learner.fit(lr=parameter['lr']*parameter[\"lr_scale_unfreeze\"], epochs=parameter[\"epochs_after_unfreeze\"])\n","    create_submission(learner.model, f\"submission_{i}.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","c:\\Users\\Bernd\\anaconda3\\envs\\mytorch\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:470: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n","  warnings.warn(\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"name":"stdout","output_type":"stream","text":["    row_id  document  token           label\n","0        0         7      6  B-NAME_STUDENT\n","1        1         7      9  B-NAME_STUDENT\n","2        2         7     10  I-NAME_STUDENT\n","3        3         7     52  B-NAME_STUDENT\n","4        4         7     53  I-NAME_STUDENT\n","5        5         7     55  B-NAME_STUDENT\n","6        6         7     56  I-NAME_STUDENT\n","7        7         7    479  B-NAME_STUDENT\n","8        8         7    482  B-NAME_STUDENT\n","9        9         7    483  I-NAME_STUDENT\n","10      10        10      0  B-NAME_STUDENT\n","11      11        10      1  I-NAME_STUDENT\n","12      12        10    464  B-NAME_STUDENT\n","13      13        10    465  I-NAME_STUDENT\n","14      14        16      4  B-NAME_STUDENT\n","15      15        16      5  I-NAME_STUDENT\n","16      16        20      5  B-NAME_STUDENT\n","17      17        20      6  I-NAME_STUDENT\n","18      18        20    328  B-NAME_STUDENT\n","19      19        20    330  B-NAME_STUDENT\n","20      20        56     12  B-NAME_STUDENT\n","21      21        56     13  I-NAME_STUDENT\n","22      22        86      6  B-NAME_STUDENT\n","23      23        86      7  I-NAME_STUDENT\n","24      24        93      0  B-NAME_STUDENT\n","25      25        93      1  I-NAME_STUDENT\n","26      26       104      7  B-NAME_STUDENT\n","27      27       104      8  B-NAME_STUDENT\n","28      28       104      9  I-NAME_STUDENT\n","29      29       112      5  B-NAME_STUDENT\n","30      30       112      6  I-NAME_STUDENT\n","31      31       123     32  B-NAME_STUDENT\n","32      32       123     33  I-NAME_STUDENT\n","33      33       123    223  B-NAME_STUDENT\n","34      34       123    224  I-NAME_STUDENT\n","35      35       123    234  I-NAME_STUDENT\n","36      36       123    490  B-NAME_STUDENT\n","37      37       123    491  I-NAME_STUDENT\n"]}],"source":["create_submission(learner.model, f\"submission.csv\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":7500999,"sourceId":66653,"sourceType":"competition"},{"datasetId":4319117,"sourceId":7429898,"sourceType":"datasetVersion"}],"dockerImageVersionId":30636,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
