{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TrainingArguments, Trainer, AutoModelForTokenClassification\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "from tokenizers import AddedToken\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "kaggle=False\n",
    "\n",
    "path=\"/kaggle/input/pii-detection-removal-from-educational-data\" if kaggle else \"data\"\n",
    "test_path = path + \"/test.json\"\n",
    "train_path = path + \"/train.json\"\n",
    "tokenizer_path = \"/kaggle/input/huggingfacedebertav3variants/deberta-v3-base\" if kaggle else \"microsoft/deberta-v3-base\"\n",
    "model_path = \"/kaggle/input/huggingfacedebertav3variants/deberta-v3-base\" if kaggle else \"final_model\"\n",
    "\n",
    "parameter= {\n",
    "    \"model\": model_path,\n",
    "    \"inference_max_length\": 2500,\n",
    "    \"inference_batch_size\": 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_inference(example, tokenizer, max_length):\n",
    "        text = []\n",
    "        for t,  ws in zip(example[\"tokens\"], example[\"trailing_whitespace\"]):\n",
    "            text.append(t)\n",
    "            if ws:\n",
    "                text.append(\" \")\n",
    "        tokenized = tokenizer(\"\".join(text), return_offsets_mapping=True, truncation=True, max_length=max_length, padding=\"max_length\")\n",
    "        text = \"\".join(text)\n",
    "        length = len(tokenized.input_ids)\n",
    "        return {\n",
    "            **tokenized,\n",
    "            \"length\": length,\n",
    "        }\n",
    "        \n",
    "class TestTokenizer():\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def preprocess(self, example):\n",
    "        # Preprocess the tokens and labels by adding trailing whitespace and labels\n",
    "        tokens = []\n",
    "        tokens_without_ws = []\n",
    "        token_map = [] # Use the index as labels\n",
    "        index = 0\n",
    "        for token, t_ws in zip(example[\"tokens\"], example[\"trailing_whitespace\"]):\n",
    "            tokens_without_ws.append(token)\n",
    "            tokens.append(token)\n",
    "            token_map.extend([index] * len(token))\n",
    "            # Added trailing whitespace and label if true and \n",
    "            if t_ws:\n",
    "                tokens.append(\" \")\n",
    "                token_map.append(-1)\n",
    "            index += 1\n",
    "        return tokens, token_map, tokens_without_ws\n",
    "    \n",
    "    def tokenize(self, example):\n",
    "        tokens, token_map, tokens_without_ws = self.preprocess(example)\n",
    "        text = \"\".join(tokens)\n",
    "        tokenized = self.tokenizer(text, return_offsets_mapping=True, padding=\"max_length\",\n",
    "                                   truncation=True, max_length=parameter[\"inference_max_length\"])\n",
    "        return {**tokenized, \"token_map\": token_map, \"tokens\": tokens, \"tokens_without_ws\": tokens_without_ws} \n",
    "\n",
    "class PiiDatasetInference(torch.utils.data.Dataset):\n",
    "        def __init__(self, dataset, tokenizer):\n",
    "            self.dataset = dataset\n",
    "            self.tokenizer=TestTokenizer(tokenizer)\n",
    "            \n",
    "        def __getitem__(self, idx):\n",
    "            vals=self.tokenizer.tokenize(self.dataset[idx])\n",
    "            input_ids = torch.tensor(vals[\"input_ids\"])\n",
    "            attention_mask = torch.tensor(vals[\"attention_mask\"])\n",
    "            document_id = self.dataset[idx][\"document\"]\n",
    "            return input_ids, attention_mask, document_id, vals\n",
    "        \n",
    "        def __len__(self):\n",
    "            return len(self.dataset)\n",
    "\n",
    "# Convert preds to a list of dictionaries\n",
    "def to_test_submission(preds=None, dataset=None, document_ids=None, id2label=None):\n",
    "    pairs = []\n",
    "    row_id = 0\n",
    "    results = []\n",
    "    \n",
    "    for i in range(len(preds)):\n",
    "        input_ids, attention_mask, document_id, vals = dataset[i]\n",
    "        token_map=vals[\"token_map\"]\n",
    "        offsets=vals[\"offset_mapping\"]\n",
    "        tokens=vals[\"tokens_without_ws\"]\n",
    "        #print(\"tokens\", tokens)\n",
    "        pred=preds[i]\n",
    "        #print(\"original_text\", original_text)\n",
    "        #print(\"token_map\", token_map)\n",
    "        #print(\"offsets\", offsets)   \n",
    "        #print(\"pred\", pred)\n",
    "\n",
    "\n",
    "        for token_pred, input_id, (start_idx, end_idx) in zip(pred, input_ids, offsets):\n",
    "            #print(\"\\nnow doing \", start_idx,  end_idx, token_pred)\n",
    "            if start_idx == 0 and end_idx == 0: # Skip 0 offset\n",
    "                continue\n",
    "            # Skip spaces \n",
    "            while start_idx < len(token_map):\n",
    "                #print(\"loop, start_idx now\", start_idx) \n",
    "                #print(\" tokens[token_map[start_idx]]: \", tokens[token_map[start_idx]] if not tokens[token_map[start_idx]].isspace() else \"WHITESPACE\")          \n",
    "                if token_map[start_idx] == -1: # Skip unknown tokens               \n",
    "                    start_idx += 1\n",
    "                elif tokens[token_map[start_idx]].isspace(): # Skip white space\n",
    "                    start_idx += 1\n",
    "                else:\n",
    "                    break\n",
    "            # Ensure start index < length\n",
    "            if start_idx < len(token_map):\n",
    "                token_id = token_map[start_idx]\n",
    "                #print(\"token_id\", token_id)\n",
    "                #token_id= input_id.item()\n",
    "                label_pred = id2label[token_pred.item()]\n",
    "                #print(\"label_pred\", label_pred)\n",
    "                # ignore \"O\" and whitespace preds\n",
    "                if label_pred != \"O\" and token_id != -1:\n",
    "                    #print(\"is PII\", token_id, label_pred)\n",
    "                    token_str = tokens[token_id]\n",
    "                    pair=(document_id, token_id)\n",
    "                    if pair not in pairs:\n",
    "                        results.append({\n",
    "                            \"row_id\": row_id, \n",
    "                            \"document\": document_id,\n",
    "                            \"token\": token_id, \n",
    "                            \"label\": label_pred,\n",
    "                            \"token_str\": token_str\n",
    "                        })\n",
    "                        pairs.append(pair)\n",
    "                        row_id += 1\n",
    "\n",
    "    # Create a dataframe \n",
    "    return results\n",
    "\n",
    "def create_submission(model, filename=\"submission.csv\"):\n",
    "    data = json.load(open(train_path))\n",
    "    from itertools import chain\n",
    "    all_labels = sorted(list(set(chain(*[x[\"labels\"] for x in data]))))\n",
    "    label2id = {l: i for i,l in enumerate(all_labels)}\n",
    "    id2label = {v:k for k,v in label2id.items()}\n",
    "\n",
    "    data=json.load(open(test_path))\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "    my_dataset=PiiDatasetInference(data, tokenizer)\n",
    "    loader=torch.utils.data.DataLoader(my_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.eval()\n",
    "    \n",
    "    # stack all predictions into tensor\n",
    "    all_preds = []\n",
    "\n",
    "    for id, attention_mask, document_ids, vals in loader:\n",
    "        id=id.to(device)\n",
    "        attention_mask=attention_mask.to(device)\n",
    "        preds=model(id, attention_mask).get('logits').argmax(dim=2)\n",
    "        all_preds.append(preds)\n",
    "        #for pred, id in zip(preds.flatten(), id.flatten()):\n",
    "        #    if pred != 12:\n",
    "                #print(f\"Document: {document_id.item()} TOKEN:{tokenizer.decode(id)}  --- pred:{id2label[pred.item()]}\")\n",
    "        #        output[row_id]={\"document\":document_id.item(), \"token\":id.item(), \"label\":id2label[pred.item()]}\n",
    "        #        row_id+=1\n",
    "        #for pred, id in zip(preds.flatten(), id.flatten()):\n",
    "        #    if pred != 12:\n",
    "        #        print(f\"TOKEN:{tokenizer.decode(id)}  --- pred:{id2label[pred.item()]}\")\n",
    "    \n",
    "   \n",
    "    all_preds = torch.cat(all_preds, dim=0)\n",
    "    \n",
    "    results = to_test_submission(preds=all_preds, dataset=my_dataset, document_ids=document_ids, id2label=id2label)\n",
    "    if len(results) == 0:\n",
    "        print(\"Error in create_submission(): No predictions made, probably because the model is not learning. Check the model and the data.\")\n",
    "        return\n",
    "    df = pd.DataFrame(results)\n",
    "    df=df[[\"row_id\", \"document\", \"token\", \"label\"]]\n",
    "    print(df)\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "#create_submission(MyModel(parameter['model'], len(label2id)).to(device), \"submission_just_dumb.csv\")\n",
    "# create_submission(model, \"submission.csv\")\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "import json\n",
    "\n",
    "data = json.load(open(train_path))\n",
    "all_labels = sorted(list(set(chain(*[x[\"labels\"] for x in data]))))\n",
    "label2id = {l: i for i,l in enumerate(all_labels)}\n",
    "id2label = {v:k for k,v in label2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "c:\\Users\\Bernd\\anaconda3\\envs\\mytorch\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:470: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "\n",
    "tokenizer=AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "collator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=16)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    parameter[\"model\"],\n",
    "    num_labels=len(all_labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    row_id  document  token           label\n",
      "0        0         7      9  B-NAME_STUDENT\n",
      "1        1         7     10  I-NAME_STUDENT\n",
      "2        2         7    482  B-NAME_STUDENT\n",
      "3        3         7    483  I-NAME_STUDENT\n",
      "4        4         7    741  B-NAME_STUDENT\n",
      "5        5         7    742  I-NAME_STUDENT\n",
      "6        6        10      0  B-NAME_STUDENT\n",
      "7        7        10      1  I-NAME_STUDENT\n",
      "8        8        10    464  B-NAME_STUDENT\n",
      "9        9        10    465  I-NAME_STUDENT\n",
      "10      10        16      4  B-NAME_STUDENT\n",
      "11      11        16      5  I-NAME_STUDENT\n",
      "12      12        20      5  B-NAME_STUDENT\n",
      "13      13        20      6  I-NAME_STUDENT\n",
      "14      14        20      8  I-NAME_STUDENT\n",
      "15      15        56     12  B-NAME_STUDENT\n",
      "16      16        56     13  I-NAME_STUDENT\n",
      "17      17        86      6  B-NAME_STUDENT\n",
      "18      18        86      7  I-NAME_STUDENT\n",
      "19      19        93      0  B-NAME_STUDENT\n",
      "20      20        93      1  I-NAME_STUDENT\n",
      "21      21       104      7  B-NAME_STUDENT\n",
      "22      22       104      8  B-NAME_STUDENT\n",
      "23      23       104      9  I-NAME_STUDENT\n",
      "24      24       112      5  B-NAME_STUDENT\n",
      "25      25       112      6  I-NAME_STUDENT\n",
      "26      26       123     32  B-NAME_STUDENT\n",
      "27      27       123     33  I-NAME_STUDENT\n",
      "28      28       123   1509  B-URL_PERSONAL\n",
      "29      29       123   1575  B-URL_PERSONAL\n"
     ]
    }
   ],
   "source": [
    "create_submission(model, \"submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
