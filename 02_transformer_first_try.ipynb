{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-25T21:37:35.578114Z","iopub.status.busy":"2024-01-25T21:37:35.577723Z","iopub.status.idle":"2024-01-25T21:37:35.587366Z","shell.execute_reply":"2024-01-25T21:37:35.5858Z","shell.execute_reply.started":"2024-01-25T21:37:35.578083Z"},"trusted":true},"outputs":[],"source":["TRAINING_MODEL_PATH = \"microsoft/deberta-v3-base\"\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-25T21:37:35.590432Z","iopub.status.busy":"2024-01-25T21:37:35.589758Z","iopub.status.idle":"2024-01-25T21:37:35.604681Z","shell.execute_reply":"2024-01-25T21:37:35.603633Z","shell.execute_reply.started":"2024-01-25T21:37:35.590393Z"},"trusted":true},"outputs":[],"source":["\n","import json\n","import numpy as np\n","\n","data = json.load(open(\"data/train.json\"))\n","\n","print(len(data))\n","print(data[0].keys())\n","\n","x = data[0]\n","\n","print(x[\"tokens\"][:10])\n","print(x[\"labels\"][:10])\n","print(x[\"trailing_whitespace\"][:10])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-25T21:37:35.606208Z","iopub.status.busy":"2024-01-25T21:37:35.605889Z","iopub.status.idle":"2024-01-25T21:37:35.62164Z","shell.execute_reply":"2024-01-25T21:37:35.620746Z","shell.execute_reply.started":"2024-01-25T21:37:35.606175Z"},"trusted":true},"outputs":[],"source":["\n","from itertools import chain\n","\n","all_labels = sorted(list(set(chain(*[x[\"labels\"] for x in data]))))\n","label2id = {l: i for i,l in enumerate(all_labels)}\n","id2label = {v:k for k,v in label2id.items()}\n","\n","id2label"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from datasets import Dataset\n","\n","ds = Dataset.from_dict({\n","    \"full_text\": [x[\"full_text\"] for x in data],\n","    \"document\": [x[\"document\"] for x in data],\n","    \"tokens\": [x[\"tokens\"] for x in data],\n","    \"trailing_whitespace\": [x[\"trailing_whitespace\"] for x in data],\n","    \"provided_labels\": [x[\"labels\"] for x in data],\n","})"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["TRAINING_MAX_LENGTH = 128"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import random\n","\n","\n","def tokenize(example, tokenizer, label2id, max_length):\n","    text = []\n","    labels = []\n","    \n","    for t, l, ws in zip(example[\"tokens\"], example[\"provided_labels\"], example[\"trailing_whitespace\"]):\n","        \n","        text.append(t)\n","        labels.extend([l]*len(t))\n","        if ws:\n","            text.append(\" \")\n","            labels.append(\"O\")\n","    \n","    \n","    tokenized = tokenizer(\"\".join(text), return_offsets_mapping=True, max_length=max_length, truncation=True, padding=\"max_length\")\n","    \n","    labels = np.array(labels)\n","    \n","    text = \"\".join(text)\n","    token_labels = []\n","    \n","    for start_idx, end_idx in tokenized.offset_mapping:\n","        \n","        # CLS token\n","        if start_idx == 0 and end_idx == 0: \n","            token_labels.append(label2id[\"O\"])\n","            continue\n","        \n","        # case when token starts with whitespace\n","        if text[start_idx].isspace():\n","            start_idx += 1\n","        \n","        while start_idx >= len(labels):\n","            start_idx -= 1\n","            \n","        token_labels.append(label2id[labels[start_idx]])\n","        \n","    length = len(tokenized.input_ids)\n","        \n","    return {\n","        **tokenized,\n","        \"labels\": token_labels,\n","        \"length\": length\n","    }\n","\n","# https://www.kaggle.com/competitions/pii-detection-removal-from-educational-data/discussion/468844\n","def filter_no_pii(example, percent_allow=0.2):\n","    # Return True if there is PII\n","    # Or 20% of the time if there isn't\n","    has_pii = set(\"O\") != set(example[\"provided_labels\"])\n","    return has_pii or (random.random() < percent_allow)\n","    \n","def compute_metrics(p, metric, all_labels):\n","    predictions, labels = p\n","    predictions = np.argmax(predictions, axis=2)\n","\n","    # Remove ignored index (special tokens)\n","    true_predictions = [\n","        [all_labels[p] for (p, l) in zip(prediction, label) if l != -100]\n","        for prediction, label in zip(predictions, labels)\n","    ]\n","    true_labels = [\n","        [all_labels[l] for (p, l) in zip(prediction, label) if l != -100]\n","        for prediction, label in zip(predictions, labels)\n","    ]\n","\n","    results = metric.compute(predictions=true_predictions, references=true_labels)\n","\n","    # Unpack nested dictionaries\n","    final_results = {}\n","    for key, value in results.items():\n","        if isinstance(value, dict):\n","            for n, v in value.items():\n","                final_results[f\"{key}_{n}\"] = v\n","        else:\n","            final_results[key] = value\n","    return final_results   \n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","import json\n","\n","class PiiDataset(torch.utils.data.Dataset):\n","    def __init__(self, dataset, tokenizer, label2id, max_length):\n","        self.dataset = dataset\n","        self.tokenizer = tokenizer\n","        self.label2id = label2id\n","        self.max_length = max_length\n","        \n","    def __getitem__(self, idx):\n","        vals=tokenize(self.dataset[idx], self.tokenizer, self.label2id, self.max_length)\n","\n","        input_ids = torch.tensor(vals[\"input_ids\"])\n","        attention_mask = torch.tensor(vals[\"attention_mask\"])\n","        labels = torch.tensor(vals[\"labels\"])\n","\n","        return input_ids, attention_mask, labels\n","    \n","    def __len__(self):\n","        return len(self.dataset)\n","    \n","\n","data = json.load(open(\"data/train.json\"))\n","\n","ds = Dataset.from_dict({\n","    \"full_text\": [x[\"full_text\"] for x in data],\n","    \"document\": [x[\"document\"] for x in data],\n","    \"tokens\": [x[\"tokens\"] for x in data],\n","    \"trailing_whitespace\": [x[\"trailing_whitespace\"] for x in data],\n","    \"provided_labels\": [x[\"labels\"] for x in data],\n","})\n","\n","from transformers import AutoTokenizer\n","\n","tokenizer = AutoTokenizer.from_pretrained(TRAINING_MODEL_PATH)\n","\n","my_dataset=PiiDataset(ds, tokenizer, label2id, TRAINING_MAX_LENGTH)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","loader=torch.utils.data.DataLoader(my_dataset, batch_size=64, shuffle=True)\n","\n","for id, attention_mask, labels in loader:\n","    print(id.shape)\n","    print(attention_mask.shape)\n","    print(labels.shape)\n","    break"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["TRAINING_MODEL_PATH"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from transformers import AutoModelForTokenClassification\n","\n","device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","class MyModel(torch.nn.Module):\n","    def __init__(self, model_name, num_labels):\n","        super().__init__()\n","        self.model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=num_labels)\n","        \n","    def forward(self, input_ids, attention_mask, labels):\n","        return self.model(input_ids, attention_mask=attention_mask, labels=labels)['logits']\n","    \n","model = MyModel(TRAINING_MODEL_PATH, len(label2id))\n","model= model.to(device)\n","for id, attention_mask, labels in loader:\n","    print(id.shape)\n","    print(attention_mask.shape)\n","    print(labels.shape)\n","    id = id.to(device)\n","    attention_mask = attention_mask.to(device)\n","    labels = labels.to(device)\n","    print(model(id, attention_mask, labels).shape)\n","    break\n","\n","#free gpu memory\n","del model\n","torch.cuda.empty_cache()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","import neptune\n","from tqdm.notebook import tqdm\n","\n","class Learner():\n","    def __init__(self, model, train_dataloader, valid_dataloader, batch_size=32):\n","        self.model=model\n","        self.loss_fn=torch.nn.CrossEntropyLoss()\n","        self.device=torch.device(\"cpu\")\n","        if torch.cuda.is_available():\n","            self.device=torch.device(\"cuda\")\n","        #elif torch.backends.mps.is_available():\n","        #    self.device=torch.device(\"mps\")\n","\n","        self.model.to(self.device)\n","        self.run = neptune.init_run(\n","            project=\"bernd.heidemann/PII\",\n","            api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIzNjBlYzVkNi0zZTUwLTQ1ODYtODhlNC02NDUxNDg0MDdjNzUifQ==\",\n","        )  # your credentials\n","        self.batch_size=batch_size\n","        self.train_dataloader = train_dataloader\n","        self.valid_dataloader = valid_dataloader\n","\n","    def fit(self, lr=0.001, epochs=10):\n","        self.run[\"parameters\"] = {\n","            \"lr\": lr,\n","            \"epochs\": epochs,\n","            \"batch_size\": self.batch_size,\n","            \"model\": TRAINING_MODEL_PATH,\n","            \"loss\": \"CrossEntropyLoss\",\n","            \"MAX_LENGTH\": TRAINING_MAX_LENGTH\n","        }\n","        optimizer=torch.optim.AdamW(self.model.parameters(), lr=lr)\n","        scheduler=scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n","        bar = tqdm(total=len(self.train_dataloader) * epochs, desc=\"Training\")\n","        bar.set_description(\"Epoch 0/{}\".format(epochs))\n","\n","        for epoch in range(epochs):\n","            self.model.train()            \n","            for ids, att_mask, labels in self.train_dataloader:\n","                \n","                ids=ids.to(self.device)\n","                labels=labels.to(self.device)\n","                att_mask=att_mask.to(self.device)\n","                pred=self.model(ids, att_mask, labels)\n","                loss=self.loss_fn(pred, labels)\n","                self.run[\"train_loss\"].log(loss.item())\n","                loss.backward()\n","                optimizer.step()\n","                optimizer.zero_grad()\n","                bar.update(1)\n","            scheduler.step()\n","            self.model.eval()\n","            # log current state to neptune\n","            if self.valid_dataloader is not None:\n","                metrics=self.get_accuracy()\n","                self.run[\"valid_accuracy\"].log(metrics[\"accuracy\"])\n","                self.run[\"valid_loss\"].log(metrics[\"loss\"])\n","                bar.set_description(\"Epoch {}/{} validAccuracy: {:.2f} validLoss: {:.2f}\".format(epoch+1, epochs, metrics[\"accuracy\"], metrics[\"loss\"]))\n","            \n","                \n","    def get_accuracy(self):\n","        self.model.eval()\n","        with torch.no_grad():\n","            correct=0\n","            losses=[]\n","            for xb, yb, att_mask in self.valid_dataloader:\n","                xb=xb.to(self.device)\n","                yb=yb.to(self.device)\n","                att_mask=att_mask.to(self.device)\n","                pred=self.model(xb, attention_mask=att_mask)\n","                loss=self.loss_fn(pred, yb)\n","                losses.append(loss.item())\n","                pred=torch.argmax(pred, dim=1)\n","                correct+=torch.sum(pred==yb).item()\n","            return {\n","                \"accuracy\": correct/len(self.valid_dataloader.dataset),\n","                \"loss\": np.mean(losses)\n","            }\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["data = json.load(open(\"data/train.json\"))\n","\n","ds = Dataset.from_dict({\n","    \"full_text\": [x[\"full_text\"] for x in data],\n","    \"document\": [x[\"document\"] for x in data],\n","    \"tokens\": [x[\"tokens\"] for x in data],\n","    \"trailing_whitespace\": [x[\"trailing_whitespace\"] for x in data],\n","    \"provided_labels\": [x[\"labels\"] for x in data],\n","})\n","\n","# split into train and validation\n","\n","from sklearn.model_selection import train_test_split\n","\n","train_ds, valid_ds = train_test_split(ds, test_size=0.2, random_state=42)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_dataset = Dataset(train_ds)\n","valid_dataset = Dataset(valid_ds)\n","train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n","valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=32, shuffle=False)\n","my_model=MyModel()\n","\n","learner=Learner(my_model, train_dataloader, valid_dataloader, batch_size=128)\n","learner.fit(lr=0.0001, epochs=2)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-25T21:37:35.708858Z","iopub.status.busy":"2024-01-25T21:37:35.708576Z","iopub.status.idle":"2024-01-25T21:37:35.721935Z","shell.execute_reply":"2024-01-25T21:37:35.720959Z","shell.execute_reply.started":"2024-01-25T21:37:35.708834Z"},"trusted":true},"outputs":[],"source":["device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","import os\n","import json\n","import argparse\n","from itertools import chain\n","from functools import partial\n","\n","from transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments, DataCollatorForTokenClassification\n","from tokenizers import AddedToken\n","import evaluate\n","from datasets import Dataset\n","import numpy as np\n","\n","os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n","\n","\n","# lots of newlines in the text\n","# adding this should be helpful\n","tokenizer.add_tokens(AddedToken(\"\\n\", normalized=False))\n","\n","ds = ds.map(lambda x: tokenize(x, tokenizer, label2id, TRAINING_MAX_LENGTH), num_proc=12)\n","ds = ds.filter(\n","    filter_no_pii,\n","    num_proc=12,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["x"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","sample = x\n","\n","\n","sample_tokenized=tokenize(sample, tokenizer, label2id, max_length=TRAINING_MAX_LENGTH)\n","\n","# create text of sample_tokenized\n","\n","text = tokenizer.decode(sample_tokenized[\"input_ids\"])\n","\n","print(\"original: [CLS] \" + sample[\"full_text\"].replace(\"\\n\", \" \"))\n","print(\"tokenize: \" + text)\n","print(\"labels: \" + \" \".join([id2label[x] for x in sample_tokenized[\"labels\"]]))"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":7500999,"sourceId":66653,"sourceType":"competition"},{"datasetId":4319117,"sourceId":7429898,"sourceType":"datasetVersion"}],"dockerImageVersionId":30636,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
