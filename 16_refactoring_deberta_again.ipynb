{"cells":[{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["from transformers import AutoTokenizer\n","from pathlib import Path\n","import numpy as np\n","import torch\n","from transformers import AutoModelForTokenClassification\n","from tokenizers import AddedToken\n","from tqdm.notebook import tqdm\n","from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n","import pandas as pd\n","\n","kaggle=False\n","\n","path=\"/kaggle/input/pii-detection-removal-from-educational-data\" if kaggle else \"data\"\n","train_path = path + \"/train.json\"\n","test_path = path + \"/test.json\"\n","\n","model_path = \"/kaggle/input/huggingfacedebertav3variants/deberta-v3-large\" if kaggle else \"microsoft/deberta-v3-large\"\n","\n","\n","if not kaggle: import neptune\n"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-01-25T21:37:35.578114Z","iopub.status.busy":"2024-01-25T21:37:35.577723Z","iopub.status.idle":"2024-01-25T21:37:35.587366Z","shell.execute_reply":"2024-01-25T21:37:35.5858Z","shell.execute_reply.started":"2024-01-25T21:37:35.578083Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["{'model': 'microsoft/deberta-v3-large', 'max_length': 512, 'batch_size': 5, 'lr': 0.001, 'filter_no_pii_percent_allow': 0.1, 'notebook': '12_unfreeze.ipynb', 'CROSS_ENTROPY_WEIGHT_MULTI': 200}\n"]}],"source":["cross_entropy_weight_multi = 200\n","\n","CROSS_ENTROPY_WEIGHTS = [cross_entropy_weight_multi]*12\n","CROSS_ENTROPY_WEIGHTS.append(1)\n","\n","parameter= {\n","    \"model\": model_path,\n","    \"max_length\": 512,\n","    \"batch_size\": 5,\n","    \"lr\": 1e-3,\n","    \"filter_no_pii_percent_allow\": 0.1,\n","    \"notebook\": \"12_unfreeze.ipynb\",\n","    \"CROSS_ENTROPY_WEIGHT_MULTI\": cross_entropy_weight_multi\n","}\n","\n","print(parameter)"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["target = [\n","    'B-EMAIL', 'B-ID_NUM', 'B-NAME_STUDENT', 'B-PHONE_NUM', \n","    'B-STREET_ADDRESS', 'B-URL_PERSONAL', 'B-USERNAME', 'I-ID_NUM', \n","    'I-NAME_STUDENT', 'I-PHONE_NUM', 'I-STREET_ADDRESS', 'I-URL_PERSONAL'\n","]"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-01-25T21:37:35.606208Z","iopub.status.busy":"2024-01-25T21:37:35.605889Z","iopub.status.idle":"2024-01-25T21:37:35.62164Z","shell.execute_reply":"2024-01-25T21:37:35.620746Z","shell.execute_reply.started":"2024-01-25T21:37:35.606175Z"},"trusted":true},"outputs":[],"source":["from itertools import chain\n","import json\n","\n","data = json.load(open(train_path))\n","all_labels = sorted(list(set(chain(*[x[\"labels\"] for x in data]))))\n","label2id = {l: i for i,l in enumerate(all_labels)}\n","id2label = {v:k for k,v in label2id.items()}"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["import random\n","\n","def tokenize(example, tokenizer, label2id, max_length):\n","    text = []\n","\n","    # these are at the character level\n","    labels = []\n","    targets = []\n","\n","    for t, l, ws in zip(example[\"tokens\"], example[\"labels\"], example[\"trailing_whitespace\"]):\n","\n","        text.append(t)\n","        labels.extend([l]*len(t))\n","        \n","        if l in target:\n","            targets.append(1)\n","        else:\n","            targets.append(0)\n","        # if there is trailing whitespace\n","        if ws:\n","            text.append(\" \")\n","            labels.append(\"O\")\n","\n","    tokenized = tokenizer(\"\".join(text), return_offsets_mapping=True, truncation=True, max_length=max_length, padding=\"max_length\")\n","    \n","    target_num = sum(targets)\n","    labels = np.array(labels)\n","\n","    text = \"\".join(text)\n","    token_labels = []\n","\n","    for start_idx, end_idx in tokenized.offset_mapping:\n","\n","        # CLS token\n","        if start_idx == 0 and end_idx == 0: \n","            token_labels.append(label2id[\"O\"])\n","            continue\n","\n","        # case when token starts with whitespace\n","        if text[start_idx].isspace():\n","            start_idx += 1\n","\n","        try:\n","            token_labels.append(label2id[labels[start_idx]])\n","        except:\n","            token_labels.append(label2id[\"O\"])\n","\n","    length = len(tokenized.input_ids)\n","\n","    return {\n","        **tokenized,\n","        \"labels\": token_labels,\n","        \"length\": length,\n","        \"target_num\": target_num,\n","        \"group\": 1 if target_num>0 else 0\n","    }\n","\n","# https://www.kaggle.com/competitions/pii-detection-removal-from-educational-data/discussion/468844\n","def filter_no_pii(example, percent_allow=parameter[\"filter_no_pii_percent_allow\"]):\n","    # Return True if there is PII\n","    # Or 20% of the time if there isn't\n","    has_pii = set(\"O\") != set(example[\"labels\"])\n","    return has_pii or (random.random() < percent_allow)"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","/opt/homebrew/Caskroom/miniconda/base/envs/torch_ds/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:454: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n","  warnings.warn(\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["class PiiDataset(torch.utils.data.Dataset):\n","    def __init__(self, dataset, tokenizer, label2id, max_length):\n","        self.dataset = dataset\n","        self.tokenizer = tokenizer\n","        self.label2id = label2id\n","        self.max_length = max_length\n","        \n","    def __getitem__(self, idx):\n","        vals=tokenize(self.dataset[idx], self.tokenizer, self.label2id, self.max_length)\n","\n","        input_ids = torch.tensor(vals[\"input_ids\"])\n","        attention_mask = torch.tensor(vals[\"attention_mask\"])\n","        labels = torch.tensor(vals[\"labels\"], dtype=torch.long)\n","        return input_ids, attention_mask, labels\n","    \n","    def __len__(self):\n","        return len(self.dataset)\n","    \n","data = json.load(open(train_path))\n","tokenizer = AutoTokenizer.from_pretrained(parameter[\"model\"])\n","my_dataset=PiiDataset(data, tokenizer, label2id, parameter[\"max_length\"])"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([8, 512])\n","torch.Size([8, 512])\n","torch.Size([8, 512])\n"]}],"source":["loader=torch.utils.data.DataLoader(my_dataset, batch_size=8, shuffle=True)\n","\n","for id, attention_mask, labels in loader:\n","    print(id.shape)\n","    print(attention_mask.shape)\n","    print(labels.shape)\n","    break"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2ForTokenClassification: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight', 'mask_predictions.dense.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'deberta.embeddings.position_embeddings.weight']\n","- This IS expected if you are initializing DebertaV2ForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2ForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["still learning classifier.weight parameter_size: torch.Size([13, 1024])\n","still learning classifier.bias parameter_size: torch.Size([13])\n","torch.Size([8, 512])\n","torch.Size([8, 512])\n","torch.Size([8, 512])\n","tensor([[12, 12, 12,  ..., 12, 12, 12],\n","        [12, 12, 12,  ..., 12, 12, 12],\n","        [12, 12, 12,  ..., 12, 12, 12],\n","        ...,\n","        [12,  2,  8,  ..., 12, 12, 12],\n","        [12, 12, 12,  ..., 12, 12, 12],\n","        [12, 12, 12,  ..., 12, 12, 12]])\n","torch.Size([8, 512, 13])\n"]}],"source":["device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","class MyModel(torch.nn.Module):\n","    def __init__(self, model_name, num_labels, dropout_p=0.4):\n","        super().__init__()\n","        self.model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=num_labels)\n","        self.softmax=torch.nn.Softmax(dim=-1)\n","        self.freeze()\n","\n","    def freeze(self):\n","        for param in self.model.parameters():\n","            param.requires_grad = False\n","        for param in self.model.classifier.parameters():\n","            param.requires_grad = True\n","        for name, param in self.model.named_parameters():\n","            if param.requires_grad==True:\n","                print(\"still learning\", name, \"parameter_size:\", param.size())\n","\n","    def unfreeze(self):\n","        for param in self.model.parameters():\n","            param.requires_grad = True\n","        \n","    def forward(self, input_ids, attention_mask, labels=None):\n","        if labels is not None:\n","            out=self.model(input_ids, attention_mask=attention_mask, labels=labels)['logits']\n","        else:\n","            out=self.model(input_ids, attention_mask=attention_mask)['logits']\n","        out=self.softmax(out)\n","        return out\n","\n","model = MyModel(parameter['model'], len(label2id))\n","\n","model= model.to(device)\n","for id, attention_mask, labels in loader:\n","    print(id.shape)\n","    print(attention_mask.shape)\n","    print(labels.shape)\n","    print(labels)\n","    id = id.to(device)\n","    attention_mask = attention_mask.to(device)\n","    labels = labels.to(device)\n","    print(model(id, attention_mask, labels).shape)\n","    break\n","\n","#free gpu memory\n","del model\n","torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["class Learner():\n","    def __init__(self, model, train_dataloader, valid_dataloader, parameter=None):\n","        self.model=model\n","        #self.loss_fn=torch.nn.CrossEntropyLoss()\n","        self.loss_fn=torch.nn.CrossEntropyLoss(ignore_index=-100, weight=torch.tensor(CROSS_ENTROPY_WEIGHTS, dtype=torch.float32).to(device))\n","        self.device=torch.device(\"cpu\")\n","        if torch.cuda.is_available():\n","            self.device=torch.device(\"cuda\")\n","        self.model.to(self.device)\n","        self.parameter = parameter\n","\n","        if not kaggle:\n","            self.run = neptune.init_run(\n","                project=\"bernd.heidemann/PII\",\n","                api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIzNjBlYzVkNi0zZTUwLTQ1ODYtODhlNC02NDUxNDg0MDdjNzUifQ==\",\n","            )  # your credentials\n","            self.run[\"parameters\"] = {\n","                **self.parameter\n","            }\n","\n","        self.train_dataloader = train_dataloader\n","        self.valid_dataloader = valid_dataloader\n","        self.non_pii_label=label2id[\"O\"]\n","\n","    def fit(self, lr=0.1, epochs=10):\n","        \n","        optimizer=torch.optim.AdamW(self.model.parameters(), lr=lr)\n","        T_0 = epochs//3          # Number of epochs before the first restart\n","        T_mult = 2        # Factor by which T_0 is multiplied after each restart\n","        eta_min = lr*0.01   # Minimum learning rate at restarts\n","\n","        scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=T_0, T_mult=T_mult, eta_min=eta_min)\n","        bar = tqdm(total=len(self.train_dataloader) * epochs, desc=\"Training\")\n","        bar.set_description(\"Epoch 0/{}\".format(epochs))\n","        for epoch in range(epochs):\n","            self.model.train()     \n","            pii_count=0       \n","            for ids, att_mask, labels in self.train_dataloader:\n","                ids=ids.to(self.device)\n","                labels=labels.to(self.device)\n","                att_mask=att_mask.to(self.device)\n","                pred=self.model(ids, att_mask, labels)\n","                metrics=self.f_beta_score_multiclass(labels, pred)\n","                pred = pred.permute(0, 2, 1)\n","                loss=self.loss_fn(pred, labels)\n","                if not kaggle:\n","                    self.run[\"train_f_beta_score\"].log(metrics[\"f_beta\"])\n","                    self.run[\"train_precision\"].log(metrics[\"precision\"])\n","                    self.run[\"train_recall\"].log(metrics[\"recall\"])\n","                    self.run[\"train_true_positives\"].log(metrics[\"true_positives\"])\n","                    self.run[\"train_false_positives\"].log(metrics[\"false_positives\"])\n","                    self.run[\"train_false_negatives\"].log(metrics[\"false_negatives\"])\n","                    self.run[\"train_loss\"].log(loss.item())\n","                if kaggle:\n","                    print(f\"Epoch {epoch+1}/{epochs} loss: {loss.item()} f_beta: {metrics['f_beta']} accuracy: {metrics['accuracy']}\")\n","                loss.backward()\n","                optimizer.step()\n","                optimizer.zero_grad()\n","                bar.update(1)\n","               \n","            if not kaggle:\n","                self.run[\"learnrate\"].log(optimizer.param_groups[0][\"lr\"])\n","            scheduler.step()\n","            self.model.eval()\n","            # log current state to neptune\n","            if self.valid_dataloader is not None:\n","                metrics=self.get_accuracy()\n","                if not kaggle:\n","                    self.run[\"valid_accuracy\"].log(metrics[\"accuracy\"])\n","                    self.run[\"valid_loss\"].log(metrics[\"loss\"])\n","                    self.run[\"valid_f_beta_score\"].log(metrics[\"f_beta_score\"])\n","                    self.run[\"valid_precision\"].log(metrics[\"precision\"])\n","                    self.run[\"valid_recall\"].log(metrics[\"recall\"])\n","                    self.run[\"valid_true_positives\"].log(metrics[\"true_positives\"])\n","                    self.run[\"valid_false_positives\"].log(metrics[\"false_positives\"])\n","                    self.run[\"valid_false_negatives\"].log(metrics[\"false_negatives\"])\n","                    self.run[\"valid_pii_count\"].log(metrics[\"pii_count\"])\n","                bar.set_description(\"Epoch {}/{} validAccuracy: {:.2f} validLoss: {:.2f}\".format(epoch+1, epochs, metrics[\"accuracy\"], metrics[\"loss\"]))\n","\n","    def get_accuracy(self):\n","        self.model.eval()\n","        with torch.no_grad():\n","            correct=0\n","            losses=[]\n","            batch_metrics=[]\n","            pii_count=0       \n","            for ids, att_mask, labels in self.valid_dataloader:\n","                ids=ids.to(self.device)\n","                labels=labels.to(self.device)\n","                att_mask=att_mask.to(self.device)\n","                pred=self.model(ids, att_mask, labels)\n","                f_beta_score_results = self.f_beta_score_multiclass(labels, pred)\n","                batch_metrics.append(f_beta_score_results)\n","                pred = pred.permute(0, 2, 1)\n","                loss=self.loss_fn(pred, labels)\n","                losses.append(loss.item())\n","                pred=torch.argmax(pred, dim=1)\n","                correct+=torch.sum(pred==labels).item()\n","                pii_count_batch = torch.sum((pred != self.non_pii_label)).item()\n","                pii_count += pii_count_batch\n","            # calc mean of the dict entries in batch_metrics\n","            f_beta_scores = np.mean([x[\"f_beta\"] for x in batch_metrics])\n","            precision = np.mean([x[\"precision\"] for x in batch_metrics])\n","            recall = np.mean([x[\"recall\"] for x in batch_metrics])\n","            true_positives = np.mean([x[\"true_positives\"] for x in batch_metrics])\n","            false_positives = np.mean([x[\"false_positives\"] for x in batch_metrics])\n","            false_negatives = np.mean([x[\"false_negatives\"] for x in batch_metrics])\n","\n","            return {\n","                \"accuracy\": correct/len(self.valid_dataloader.dataset),\n","                \"loss\": np.mean(losses),\n","                \"f_beta_score\": f_beta_scores,\n","                \"precision\": precision,\n","                \"recall\": recall,\n","                \"true_positives\": true_positives,\n","                \"false_positives\": false_positives,\n","                \"false_negatives\": false_negatives,\n","                 \"pii_count\": pii_count\n","            }\n","        \n","    def f_beta_score_multiclass(self, y_true, y_pred, beta=5, epsilon=1e-7):\n","\n","        # assert y_pred has values between 0 and 1\n","        assert y_pred.min() >= 0\n","        assert y_pred.max() <= 1\n","\n","        y_true_one_hot = torch.nn.functional.one_hot(y_true, num_classes=y_pred.shape[2])\n","    \n","        # Berechnung von True Positives, False Positives und False Negatives\n","        tp = torch.sum(y_true_one_hot * y_pred, dim=0)\n","        fp = torch.sum((1 - y_true_one_hot) * y_pred, dim=0)\n","        fn = torch.sum(y_true_one_hot * (1 - y_pred), dim=0)\n","\n","        # Summierung über alle Klassen\n","        tp_sum = torch.sum(tp)\n","        fp_sum = torch.sum(fp)\n","        fn_sum = torch.sum(fn)\n","\n","        # Berechnung von Präzision und Recall\n","        precision = tp_sum / (tp_sum + fp_sum + epsilon)\n","        recall = tp_sum / (tp_sum + fn_sum + epsilon)\n","        # Berechnung des F-Beta-Scores\n","        f_beta = (1 + beta**2) * (precision * recall) / ((beta**2 * precision) + recall + epsilon)\n","\n","        return {\n","            \"f_beta\": f_beta.item(),\n","            \"precision\": precision.item(),\n","            \"recall\": recall.item(),\n","            \"true_positives\": tp_sum.item(),\n","            \"false_positives\": fp_sum.item(),\n","            \"false_negatives\": fn_sum.item()\n","        }\n","\n","    "]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2ForTokenClassification: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight', 'mask_predictions.dense.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'deberta.embeddings.position_embeddings.weight']\n","- This IS expected if you are initializing DebertaV2ForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2ForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["still learning classifier.weight parameter_size: torch.Size([13, 1024])\n","still learning classifier.bias parameter_size: torch.Size([13])\n"]},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","/opt/homebrew/Caskroom/miniconda/base/envs/torch_ds/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:454: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n","  warnings.warn(\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"name":"stdout","output_type":"stream","text":["dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'token_map'])\n"]},{"ename":"KeyError","evalue":"'document'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","Cell \u001b[0;32mIn[11], line 160\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28mprint\u001b[39m(df)\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m#df.to_csv(filename, index=False)\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m \u001b[43mcreate_submission\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMyModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparameter\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlabel2id\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msubmission.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[11], line 147\u001b[0m, in \u001b[0;36mcreate_submission\u001b[0;34m(model, filename)\u001b[0m\n\u001b[1;32m    141\u001b[0m     preds\u001b[38;5;241m=\u001b[39mmodel(\u001b[38;5;28mid\u001b[39m, attention_mask)\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;66;03m#for pred, id in zip(preds.flatten(), id.flatten()):\u001b[39;00m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;66;03m#    if pred != 12:\u001b[39;00m\n\u001b[1;32m    144\u001b[0m             \u001b[38;5;66;03m#print(f\"Document: {document_id.item()} TOKEN:{tokenizer.decode(id)}  --- pred:{id2label[pred.item()]}\")\u001b[39;00m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;66;03m#        output[row_id]={\"document\":document_id.item(), \"token\":id.item(), \"label\":id2label[pred.item()]}\u001b[39;00m\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;66;03m#        row_id+=1\u001b[39;00m\n\u001b[0;32m--> 147\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mto_test_submission\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mid2label\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28mprint\u001b[39m(results)\n\u001b[1;32m    150\u001b[0m output \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(output\u001b[38;5;241m.\u001b[39mitems(), key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m item: (item[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdocument\u001b[39m\u001b[38;5;124m'\u001b[39m], item[\u001b[38;5;241m0\u001b[39m]))}\n","Cell \u001b[0;32mIn[11], line 88\u001b[0m, in \u001b[0;36mto_test_submission\u001b[0;34m(preds, tokenized_ds, id2label)\u001b[0m\n\u001b[1;32m     82\u001b[0m row_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     83\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pred, token_map, offsets, tokens, document \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(preds, \n\u001b[1;32m     85\u001b[0m                                                       tokenized_ds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_map\u001b[39m\u001b[38;5;124m\"\u001b[39m], \n\u001b[1;32m     86\u001b[0m                                                       tokenized_ds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moffset_mapping\u001b[39m\u001b[38;5;124m\"\u001b[39m], \n\u001b[1;32m     87\u001b[0m                                                       tokenized_ds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m---> 88\u001b[0m                                                       \u001b[43mtokenized_ds\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdocument\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m):\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m token_pred, (start_idx, end_idx) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(pred, offsets):\n\u001b[1;32m     90\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m start_idx \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m end_idx \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \u001b[38;5;66;03m# Skip 0 offset\u001b[39;00m\n","\u001b[0;31mKeyError\u001b[0m: 'document'"]}],"source":["def tokenize_inference(example, tokenizer, max_length):\n","        text = []\n","        for t,  ws in zip(example[\"tokens\"], example[\"trailing_whitespace\"]):\n","            text.append(t)\n","            if ws:\n","                text.append(\" \")\n","        tokenized = tokenizer(\"\".join(text), return_offsets_mapping=True, truncation=True, max_length=max_length, padding=\"max_length\")\n","        text = \"\".join(text)\n","        length = len(tokenized.input_ids)\n","        return {\n","            **tokenized,\n","            \"length\": length,\n","        }\n","        \n","class TestTokenizer():\n","    def __init__(self, tokenizer):\n","        self.tokenizer = tokenizer\n","    \n","    def preprocess(self, example):\n","        # Preprocess the tokens and labels by adding trailing whitespace and labels\n","        tokens = []\n","        token_map = [] # Use the index as labels\n","        index = 0\n","        for token, t_ws in zip(example[\"tokens\"], example[\"trailing_whitespace\"]):\n","            tokens.append(token)\n","            token_map.extend([index] * len(token))\n","            # Added trailing whitespace and label if true and \n","            if t_ws:\n","                tokens.append(\" \")\n","                token_map.append(-1)\n","            index += 1\n","        return tokens, token_map\n","    \n","    def tokenize(self, example):\n","        tokens, token_map = self.preprocess(example)\n","        text = \"\".join(tokens)\n","        tokenized = self.tokenizer(text, return_offsets_mapping=True,\n","                                   truncation=True, max_length=parameter[\"max_length\"])\n","        return {**tokenized, \"token_map\": token_map} \n","\n","class PiiDatasetInference(torch.utils.data.Dataset):\n","        def __init__(self, dataset, tokenizer, max_length):\n","            self.dataset = dataset\n","            self.tokenizer = tokenizer\n","            self.max_length = max_length\n","            self.tokenizer=TestTokenizer(tokenizer)\n","            \n","        def __getitem__(self, idx):\n","            vals=self.tokenizer.tokenize(self.dataset[idx])\n","            input_ids = torch.tensor(vals[\"input_ids\"])\n","            attention_mask = torch.tensor(vals[\"attention_mask\"])\n","            document_id = self.dataset[idx][\"document\"]\n","            return input_ids, attention_mask, document_id, vals\n","        \n","        def __len__(self):\n","            return len(self.dataset)\n","\n","def inference(model):\n","    data = json.load(open(train_path))\n","    from itertools import chain\n","    all_labels = sorted(list(set(chain(*[x[\"labels\"] for x in data]))))\n","    label2id = {l: i for i,l in enumerate(all_labels)}\n","    id2label = {v:k for k,v in label2id.items()}\n","\n","    tokenizer = AutoTokenizer.from_pretrained(parameter[\"model\"])\n","    data = json.load(open(test_path))\n","    my_dataset=PiiDatasetInference(data, tokenizer, parameter[\"max_length\"])\n","    loader=torch.utils.data.DataLoader(my_dataset, batch_size=parameter['batch_size'], shuffle=True)\n","    for id, attention_mask, _ in loader:\n","        id = id.to(device)\n","        attention_mask = attention_mask.to(device)\n","        preds=model(id, attention_mask).argmax(dim=2)\n","\n","        for pred, id in zip(preds.flatten(), id.flatten()):\n","            if pred != 12:\n","                print(f\"TOKEN:{tokenizer.decode(id)}  --- pred:{id2label[pred.item()]}\")\n","        print(\"next\")\n","\n","# Convert preds to a list of dictionaries\n","def to_test_submission(preds, tokenized_ds, id2label):\n","    triplets = []\n","    row_id = 0\n","    results = []\n","    for pred, token_map, offsets, tokens, document in zip(preds, \n","                                                          tokenized_ds[\"token_map\"], \n","                                                          tokenized_ds[\"offset_mapping\"], \n","                                                          tokenized_ds[\"input_ids\"],\n","                                                          tokenized_ds[\"document\"]):\n","        for token_pred, (start_idx, end_idx) in zip(pred, offsets):\n","            if start_idx == 0 and end_idx == 0: # Skip 0 offset\n","                continue\n","            # Skip spaces \n","            while start_idx < len(token_map):                \n","                if token_map[start_idx] == -1: # Skip unknown tokens               \n","                    start_idx += 1\n","                elif tokens[token_map[start_idx]].isspace(): # Skip white space\n","                    start_idx += 1\n","                else:\n","                    break\n","            # Ensure start index < length\n","            if start_idx < len(token_map):\n","                token_id = token_map[start_idx]\n","                label_pred = id2label[str(token_pred)]\n","                # ignore \"O\" and whitespace preds\n","                if label_pred != \"O\" and token_id != -1:\n","                    token_str = tokens[token_id]\n","                    triplet = (label_pred, token_id, token_str)\n","                    if triplet not in triplets:\n","                        results.append({\n","                            \"row_id\": row_id, \n","                            \"document\": document,\n","                            \"token\": token_id, \n","                            \"label\": label_pred,\n","                            \"token_str\": token_str\n","                        })\n","                        triplets.append(triplet)\n","                        row_id += 1\n","    # Create a dataframe \n","    return results\n","\n","def create_submission(model, filename=\"submission.csv\"):\n","    data = json.load(open(train_path))\n","    from itertools import chain\n","    all_labels = sorted(list(set(chain(*[x[\"labels\"] for x in data]))))\n","    label2id = {l: i for i,l in enumerate(all_labels)}\n","    id2label = {v:k for k,v in label2id.items()}\n","\n","    data=json.load(open(test_path))\n","    tokenizer = AutoTokenizer.from_pretrained(parameter[\"model\"])\n","    my_dataset=PiiDatasetInference(data, tokenizer, parameter[\"max_length\"])\n","    loader=torch.utils.data.DataLoader(my_dataset, batch_size=1, shuffle=True)\n","\n","    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model.eval()\n","    output={}\n","    row_id=0\n","    for id, attention_mask, document_id, vals in loader:\n","        print(vals.keys())\n","        id=id.to(device)\n","        attention_mask=attention_mask.to(device)\n","        preds=model(id, attention_mask).argmax(dim=2)\n","        #for pred, id in zip(preds.flatten(), id.flatten()):\n","        #    if pred != 12:\n","                #print(f\"Document: {document_id.item()} TOKEN:{tokenizer.decode(id)}  --- pred:{id2label[pred.item()]}\")\n","        #        output[row_id]={\"document\":document_id.item(), \"token\":id.item(), \"label\":id2label[pred.item()]}\n","        #        row_id+=1\n","        results = to_test_submission(preds, vals, id2label)\n","        print(results)\n","\n","    output = {k: v for k, v in sorted(output.items(), key=lambda item: (item[1]['document'], item[0]))}\n","\n","    df=pd.DataFrame(output).T\n","    df.reset_index(drop=True, inplace=True)\n","    df['row_id'] = df.index\n","    df = df[ ['row_id'] + [ col for col in df.columns if col != 'row_id' ] ]\n","    print(df)\n","    #df.to_csv(filename, index=False)\n","\n","create_submission(MyModel(parameter['model'], len(label2id)), \"submission.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["data = json.load(open(train_path))\n","data_filterd = list(filter(filter_no_pii, data))\n","data_len=len(data_filterd)\n","train_len=int(len(data_filterd)*0.8)\n","valid_len=len(data_filterd)-train_len\n","train_data_idx=np.random.choice(data_len, train_len, replace=False)\n","valid_data_idx=np.array(list(set(range(data_len))-set(train_data_idx)))\n","train_data=[data_filterd[i] for i in train_data_idx]\n","valid_data=[data_filterd[i] for i in valid_data_idx]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2ForTokenClassification: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.classifier.bias', 'deberta.embeddings.position_embeddings.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.weight']\n","- This IS expected if you are initializing DebertaV2ForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2ForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["still learning classifier.weight parameter_size: torch.Size([13, 1024])\n","still learning classifier.bias parameter_size: torch.Size([13])\n","https://app.neptune.ai/bernd.heidemann/PII/e/PII-139\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b0fe2a2ec8a046f1986411e99a986f56","version_major":2,"version_minor":0},"text/plain":["Training:   0%|          | 0/2940 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[40], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m my_model\u001b[38;5;241m=\u001b[39mMyModel(parameter[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;28mlen\u001b[39m(label2id))\n\u001b[1;32m     14\u001b[0m learner\u001b[38;5;241m=\u001b[39mLearner(my_model, train_dataloader, valid_dataloader, parameter\u001b[38;5;241m=\u001b[39mparameter)\n\u001b[0;32m---> 15\u001b[0m \u001b[43mlearner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameter\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[37], line 67\u001b[0m, in \u001b[0;36mLearner.fit\u001b[0;34m(self, lr, epochs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# log current state to neptune\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalid_dataloader \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 67\u001b[0m     metrics\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_accuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kaggle:\n\u001b[1;32m     69\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalid_accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mlog(metrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n","Cell \u001b[0;32mIn[37], line 91\u001b[0m, in \u001b[0;36mLearner.get_accuracy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     89\u001b[0m labels\u001b[38;5;241m=\u001b[39mlabels\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     90\u001b[0m att_mask\u001b[38;5;241m=\u001b[39matt_mask\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 91\u001b[0m pred\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43matt_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m f_beta_score_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_beta_score_multiclass(labels, pred)\n\u001b[1;32m     93\u001b[0m batch_metrics\u001b[38;5;241m.\u001b[39mappend(f_beta_score_results)\n","File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/torch_ds/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","Cell \u001b[0;32mIn[36], line 24\u001b[0m, in \u001b[0;36mMyModel.forward\u001b[0;34m(self, input_ids, attention_mask, labels)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, attention_mask, labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 24\u001b[0m         out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     26\u001b[0m         out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(input_ids, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m'\u001b[39m]\n","File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/torch_ds/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/torch_ds/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:1417\u001b[0m, in \u001b[0;36mDebertaV2ForTokenClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1411\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1412\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1413\u001b[0m \u001b[38;5;124;03m    Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\u001b[39;00m\n\u001b[1;32m   1414\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1415\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1417\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeberta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1418\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1419\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1420\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1421\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1422\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1423\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1424\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1425\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1426\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1428\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1430\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(sequence_output)\n","File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/torch_ds/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/torch_ds/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:1083\u001b[0m, in \u001b[0;36mDebertaV2Model.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1073\u001b[0m     token_type_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(input_shape, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m   1075\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m   1076\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1077\u001b[0m     token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1080\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[1;32m   1081\u001b[0m )\n\u001b[0;32m-> 1083\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1084\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1085\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1086\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1087\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1088\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1089\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1090\u001b[0m encoded_layers \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   1092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz_steps \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n","File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/torch_ds/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/torch_ds/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:521\u001b[0m, in \u001b[0;36mDebertaV2Encoder.forward\u001b[0;34m(self, hidden_states, attention_mask, output_hidden_states, output_attentions, query_states, relative_pos, return_dict)\u001b[0m\n\u001b[1;32m    512\u001b[0m     output_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    513\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    514\u001b[0m         next_kv,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    518\u001b[0m         rel_embeddings,\n\u001b[1;32m    519\u001b[0m     )\n\u001b[1;32m    520\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 521\u001b[0m     output_states \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnext_kv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[1;32m    531\u001b[0m     output_states, att_m \u001b[38;5;241m=\u001b[39m output_states\n","File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/torch_ds/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/torch_ds/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:362\u001b[0m, in \u001b[0;36mDebertaV2Layer.forward\u001b[0;34m(self, hidden_states, attention_mask, query_states, relative_pos, rel_embeddings, output_attentions)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    355\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    360\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    361\u001b[0m ):\n\u001b[0;32m--> 362\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[1;32m    371\u001b[0m         attention_output, att_matrix \u001b[38;5;241m=\u001b[39m attention_output\n","File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/torch_ds/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/torch_ds/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:293\u001b[0m, in \u001b[0;36mDebertaV2Attention.forward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    286\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    291\u001b[0m     rel_embeddings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    292\u001b[0m ):\n\u001b[0;32m--> 293\u001b[0m     self_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[1;32m    302\u001b[0m         self_output, att_matrix \u001b[38;5;241m=\u001b[39m self_output\n","File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/torch_ds/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/torch_ds/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:725\u001b[0m, in \u001b[0;36mDisentangledSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[1;32m    723\u001b[0m     scale_factor \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    724\u001b[0m scale \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msqrt(torch\u001b[38;5;241m.\u001b[39mtensor(query_layer\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat) \u001b[38;5;241m*\u001b[39m scale_factor)\n\u001b[0;32m--> 725\u001b[0m attention_scores \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m scale\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mquery_layer\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    726\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelative_attention:\n\u001b[1;32m    727\u001b[0m     rel_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_dropout(rel_embeddings)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["\n","# set environment variables: TOKENIZERS_PARALLELISM=false\n","import os\n","os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n","\n","tokenizer = AutoTokenizer.from_pretrained(parameter[\"model\"])\n","tokenizer.add_tokens(AddedToken(\"\\n\", normalized=False))\n","\n","train_dataset = PiiDataset(train_data, tokenizer, label2id, parameter[\"max_length\"])\n","valid_dataset = PiiDataset(valid_data, tokenizer, label2id, parameter[\"max_length\"])\n","train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=parameter['batch_size'], shuffle=True)\n","valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=parameter['batch_size'], shuffle=False)\n","my_model=MyModel(parameter['model'], len(label2id))\n","\n","learner=Learner(my_model, train_dataloader, valid_dataloader, parameter=parameter)\n","learner.fit(lr=parameter['lr'], epochs=12)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["learner.model.unfreeze()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"name":"stdout","output_type":"stream","text":["Document: 10 TOKEN:Diego  --- pred:B-NAME_STUDENT\n","Document: 10 TOKEN:Estrada  --- pred:I-NAME_STUDENT\n","Document: 10 TOKEN:Diego  --- pred:B-NAME_STUDENT\n","Document: 10 TOKEN:Estrada  --- pred:I-NAME_STUDENT\n","Document: 20 TOKEN:Sind  --- pred:B-NAME_STUDENT\n","Document: 20 TOKEN:y  --- pred:B-NAME_STUDENT\n","Document: 20 TOKEN:Sam  --- pred:I-NAME_STUDENT\n","Document: 20 TOKEN:aca  --- pred:I-NAME_STUDENT\n","Document: 20 TOKEN:Gita  --- pred:B-NAME_STUDENT\n","Document: 20 TOKEN:m  --- pred:B-NAME_STUDENT\n","Document: 20 TOKEN:University  --- pred:I-NAME_STUDENT\n","Document: 20 TOKEN:2021  --- pred:B-NAME_STUDENT\n","Document: 20 TOKEN:George  --- pred:B-NAME_STUDENT\n","Document: 20 TOKEN:Geoff  --- pred:B-NAME_STUDENT\n","Document: 104 TOKEN:Storytelling  --- pred:B-NAME_STUDENT\n","Document: 104 TOKEN:Dr  --- pred:B-NAME_STUDENT\n","Document: 104 TOKEN:Saki  --- pred:B-NAME_STUDENT\n","Document: 104 TOKEN:r  --- pred:B-NAME_STUDENT\n","Document: 104 TOKEN:Ahmad  --- pred:I-NAME_STUDENT\n","Document: 56 TOKEN:Nadine  --- pred:B-NAME_STUDENT\n","Document: 56 TOKEN:Born  --- pred:I-NAME_STUDENT\n","Document: 112 TOKEN:Francisco  --- pred:B-NAME_STUDENT\n","Document: 112 TOKEN:Ferreira  --- pred:I-NAME_STUDENT\n","Document: 86 TOKEN:Ela  --- pred:B-NAME_STUDENT\n","Document: 86 TOKEN:dio  --- pred:B-NAME_STUDENT\n","Document: 86 TOKEN:Amaya  --- pred:I-NAME_STUDENT\n","Document: 16 TOKEN:Gilberto  --- pred:B-NAME_STUDENT\n","Document: 16 TOKEN:Gamb  --- pred:I-NAME_STUDENT\n","Document: 16 TOKEN:oa  --- pred:I-NAME_STUDENT\n","Document: 93 TOKEN:Silvia  --- pred:B-NAME_STUDENT\n","Document: 93 TOKEN:Villa  --- pred:I-NAME_STUDENT\n","Document: 93 TOKEN:lobo  --- pred:I-NAME_STUDENT\n","Document: 93 TOKEN:s  --- pred:I-NAME_STUDENT\n","Document: 7 TOKEN:Design  --- pred:B-NAME_STUDENT\n","Document: 7 TOKEN:for  --- pred:B-URL_PERSONAL\n","Document: 7 TOKEN:Av  --- pred:B-NAME_STUDENT\n","Document: 7 TOKEN:ril  --- pred:B-URL_PERSONAL\n","Document: 7 TOKEN:2021  --- pred:B-URL_PERSONAL\n","Document: 7 TOKEN:N  --- pred:B-NAME_STUDENT\n","Document: 7 TOKEN:atha  --- pred:B-NAME_STUDENT\n","Document: 7 TOKEN:lie  --- pred:B-NAME_STUDENT\n","Document: 7 TOKEN:S  --- pred:I-NAME_STUDENT\n","Document: 7 TOKEN:ylla  --- pred:I-NAME_STUDENT\n","Document: 7 TOKEN:Buz  --- pred:B-NAME_STUDENT\n","Document: 7 TOKEN:an  --- pred:I-NAME_STUDENT\n","Document: 7 TOKEN:Buz  --- pred:B-NAME_STUDENT\n","Document: 7 TOKEN:an  --- pred:I-NAME_STUDENT\n","Document: 7 TOKEN:Paris  --- pred:B-NAME_STUDENT\n","Document: 7 TOKEN:for  --- pred:B-URL_PERSONAL\n","Document: 7 TOKEN:Av  --- pred:B-NAME_STUDENT\n","Document: 7 TOKEN:ril  --- pred:B-URL_PERSONAL\n","Document: 7 TOKEN:2021  --- pred:B-URL_PERSONAL\n","Document: 7 TOKEN:N  --- pred:B-NAME_STUDENT\n","Document: 7 TOKEN:atha  --- pred:B-NAME_STUDENT\n","Document: 7 TOKEN:lie  --- pred:B-NAME_STUDENT\n","Document: 7 TOKEN:S  --- pred:I-NAME_STUDENT\n","Document: 7 TOKEN:ylla  --- pred:I-NAME_STUDENT\n","Document: 123 TOKEN:Gandhi  --- pred:B-NAME_STUDENT\n","Document: 123 TOKEN:Stefano  --- pred:B-NAME_STUDENT\n","Document: 123 TOKEN:Lovato  --- pred:I-NAME_STUDENT\n","Document: 123 TOKEN:MD  --- pred:B-NAME_STUDENT\n","Document: 123 TOKEN:Sathya  --- pred:B-NAME_STUDENT\n","Document: 123 TOKEN:b  --- pred:B-NAME_STUDENT\n","Document: 123 TOKEN:ama  --- pred:I-NAME_STUDENT\n","Document: 123 TOKEN:April  --- pred:B-NAME_STUDENT\n","Document: 123 TOKEN:2020  --- pred:B-URL_PERSONAL\n","Document: 123 TOKEN:Gera  --- pred:B-NAME_STUDENT\n","Document: 123 TOKEN:shchenko  --- pred:I-NAME_STUDENT\n","Document: 123 TOKEN:Igor  --- pred:I-NAME_STUDENT\n","Document: 123 TOKEN:Alexander  --- pred:B-NAME_STUDENT\n","Document: 123 TOKEN:Sh  --- pred:I-NAME_STUDENT\n","Document: 123 TOKEN:m  --- pred:B-NAME_STUDENT\n","Document: 123 TOKEN:akov  --- pred:I-NAME_STUDENT\n"]}],"source":["create_submission(learner.model, \"submission.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for i in range(1):\n","    learner.fit(lr=parameter['lr']*0.01, epochs=3)\n","    inference(learner.model)\n","create_submission(learner.model, f\"submission_{i}.csv\")\n"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":7500999,"sourceId":66653,"sourceType":"competition"},{"datasetId":4319117,"sourceId":7429898,"sourceType":"datasetVersion"}],"dockerImageVersionId":30636,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"}},"nbformat":4,"nbformat_minor":4}
